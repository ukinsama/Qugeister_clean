#!/usr/bin/env python3
"""
æ”¹å–„ã•ã‚ŒãŸ3step AI ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ
ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ãŒå¼·åŒ–å­¦ç¿’ã«å¯„ä¸ã™ã‚‹ã‚ˆã†ã«ä¿®æ­£
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import random
from collections import deque
from pathlib import Path

class ImprovedQuantumAI:
    """æ”¹å–„ã•ã‚ŒãŸé‡å­ã‚¤ãƒ³ã‚¹ãƒ‘ã‚¤ã‚¢ãƒ¼ãƒ‰AI (å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ´»ç”¨ç‰ˆ)"""
    
    def __init__(self, config):
        self.config = config
        
        # ğŸ”§ ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’æ´»ç”¨
        # Step 1: å­¦ç¿’æ–¹æ³•
        self.learning_method = config.get('learning_method', 'reinforcement')
        
        # Step 2: ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆ
        self.placement_strategy = config.get('placement', 'standard')
        self.estimator_type = config.get('estimator', 'cqcnn')
        self.reward_type = config.get('reward', 'basic')
        self.qmap_method = config.get('qmap', 'dqn')
        self.action_strategy = config.get('action', 'epsilon')
        
        # Step 3: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ (ã™ã¹ã¦æ´»ç”¨)
        self.learning_rate = config.get('learning_rate', 0.001)
        self.batch_size = config.get('batch_size', 32)
        self.episodes = config.get('episodes', 1000)
        self.epsilon = config.get('epsilon', 0.1)
        self.epsilon_decay = config.get('epsilon_decay', 0.995)  # ğŸ”§ ä¿®æ­£: å®Ÿéš›ã«ä½¿ç”¨
        self.epsilon_min = config.get('epsilon_min', 0.01)
        self.gamma = config.get('gamma', 0.99)  # ğŸ”§ ä¿®æ­£: å®Ÿéš›ã«ä½¿ç”¨
        
        # é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.n_qubits = config.get('n_qubits', 6)  # ğŸ”§ ä¿®æ­£: æœ€é©å€¤ã«èª¿æ•´
        self.n_layers = config.get('n_layers', 2)  # ğŸ”§ ä¿®æ­£: åŠ¹ç‡çš„ãªå±¤æ•°
        self.embedding_type = config.get('embedding_type', 'angle')  # ğŸ”§ ä¿®æ­£: å®Ÿè£…
        self.entanglement = config.get('entanglement', 'linear')  # ğŸ”§ ä¿®æ­£: å®Ÿè£…
        
        # ãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰
        self.model = self._build_model()
        self.target_model = self._build_model()  # ğŸ”§ è¿½åŠ : DQNç”¨ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
        self.update_target_model()
        
        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼
        self.optimizer = optim.Adam(
            self.model.parameters(), 
            lr=self.learning_rate
        )
        
        # çµŒé¨“å†ç”Ÿãƒãƒƒãƒ•ã‚¡
        self.memory = deque(maxlen=10000)
        
        # çµ±è¨ˆæƒ…å ±
        self.training_stats = {
            'episodes': [],
            'rewards': [],
            'losses': [],
            'epsilon_values': []
        }
    
    def _build_model(self):
        """æ”¹å–„ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«æ§‹ç¯‰"""
        return ImprovedQuantumNetwork(
            n_qubits=self.n_qubits,
            n_layers=self.n_layers,
            embedding_type=self.embedding_type,
            entanglement=self.entanglement,
            reward_type=self.reward_type  # ğŸ”§ å ±é…¬ã‚¿ã‚¤ãƒ—ã‚’åæ˜ 
        )
    
    def select_action(self, state):
        """æ”¹å–„ã•ã‚ŒãŸè¡Œå‹•é¸æŠ (å…¨æˆ¦ç•¥å®Ÿè£…)"""
        
        if self.action_strategy == 'epsilon':
            # Îµ-greedyæˆ¦ç•¥
            if random.random() < self.epsilon:
                return random.randint(0, 3)
            else:
                with torch.no_grad():
                    q_values = self.model(state)
                    return q_values.argmax().item()
                    
        elif self.action_strategy == 'boltzmann':
            # ãƒœãƒ«ãƒ„ãƒãƒ³é¸æŠ
            with torch.no_grad():
                q_values = self.model(state)
                temperature = 1.0
                probs = torch.softmax(q_values / temperature, dim=-1)
                return torch.multinomial(probs, 1).item()
                
        elif self.action_strategy == 'ucb':
            # UCBé¸æŠ (ç°¡ç•¥åŒ–ç‰ˆ)
            with torch.no_grad():
                q_values = self.model(state)
                exploration_bonus = torch.randn_like(q_values) * 0.1
                return (q_values + exploration_bonus).argmax().item()
                
        else:  # greedy
            with torch.no_grad():
                q_values = self.model(state)
                return q_values.argmax().item()
    
    def remember(self, state, action, reward, next_state, done):
        """çµŒé¨“ã‚’ä¿å­˜"""
        self.memory.append((state, action, reward, next_state, done))
    
    def replay(self):
        """æ”¹å–„ã•ã‚ŒãŸçµŒé¨“å†ç”Ÿå­¦ç¿’ (gammaæ´»ç”¨)"""
        
        if len(self.memory) < self.batch_size:  # ğŸ”§ batch_sizeæ´»ç”¨
            return
        
        # ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = random.sample(self.memory, self.batch_size)
        states = torch.cat([s for s, _, _, _, _ in batch])
        actions = torch.tensor([a for _, a, _, _, _ in batch])
        rewards = torch.tensor([r for _, _, r, _, _ in batch], dtype=torch.float32)
        next_states = torch.cat([s for _, _, _, s, _ in batch])
        dones = torch.tensor([d for _, _, _, _, d in batch], dtype=torch.float32)
        
        # ç¾åœ¨ã®Qå€¤
        current_q_values = self.model(states)
        current_q_values = current_q_values.gather(1, actions.unsqueeze(1))
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤ (gammaæ´»ç”¨)
        with torch.no_grad():
            if self.qmap_method == 'dqn':
                # Double DQN
                next_q_values = self.target_model(next_states).max(1)[0]
            else:
                # é€šå¸¸ã®Qå­¦ç¿’
                next_q_values = self.model(next_states).max(1)[0]
            
            # ğŸ”§ gammaã‚’å®Ÿéš›ã«ä½¿ç”¨
            target_q_values = rewards + (1 - dones) * self.gamma * next_q_values
        
        # Lossè¨ˆç®—ã¨æ›´æ–°
        loss = nn.MSELoss()(current_q_values.squeeze(), target_q_values)
        
        self.optimizer.zero_grad()
        loss.backward()
        self.optimizer.step()
        
        return loss.item()
    
    def update_epsilon(self):
        """æ¢ç´¢ç‡ã®æ›´æ–° (epsilon_decayæ´»ç”¨)"""
        # ğŸ”§ epsilon_decayã‚’å®Ÿéš›ã«ä½¿ç”¨
        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)
        self.training_stats['epsilon_values'].append(self.epsilon)
    
    def update_target_model(self):
        """ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«ã®æ›´æ–°"""
        if self.qmap_method == 'dqn':
            self.target_model.load_state_dict(self.model.state_dict())
    
    def calculate_reward(self, game_state):
        """æ”¹å–„ã•ã‚ŒãŸå ±é…¬è¨ˆç®— (reward_typeæ´»ç”¨)"""
        
        base_reward = game_state.get('base_reward', 0)
        
        # ğŸ”§ reward_typeã«åŸºã¥ã„ã¦å ±é…¬ã‚’èª¿æ•´
        if self.reward_type == 'aggressive':
            # æ”»æ’ƒçš„: å‰é€²ãƒœãƒ¼ãƒŠã‚¹
            forward_bonus = game_state.get('forward_distance', 0) * 2.0
            capture_bonus = game_state.get('captures', 0) * 5.0
            return base_reward + forward_bonus + capture_bonus
            
        elif self.reward_type == 'defensive':
            # é˜²å¾¡çš„: ç”Ÿå­˜ãƒœãƒ¼ãƒŠã‚¹
            survival_bonus = game_state.get('survival_time', 0) * 1.0
            safety_bonus = game_state.get('safe_pieces', 0) * 2.0
            return base_reward + survival_bonus + safety_bonus
            
        elif self.reward_type == 'escape':
            # è„±å‡ºé‡è¦–: ã‚´ãƒ¼ãƒ«åˆ°é”ãƒœãƒ¼ãƒŠã‚¹
            escape_bonus = game_state.get('escaped_pieces', 0) * 10.0
            distance_bonus = game_state.get('goal_distance', 0) * 0.5
            return base_reward + escape_bonus + distance_bonus
            
        else:  # basic
            return base_reward
    
    def train_episode(self, env):
        """1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®å­¦ç¿’ (å…¨ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ´»ç”¨)"""
        
        state = env.reset()
        total_reward = 0
        losses = []
        
        for step in range(1000):  # æœ€å¤§ã‚¹ãƒ†ãƒƒãƒ—æ•°
            # è¡Œå‹•é¸æŠ
            action = self.select_action(state)
            
            # ç’°å¢ƒã§å®Ÿè¡Œ
            next_state, reward_info, done = env.step(action)
            
            # å ±é…¬è¨ˆç®— (reward_typeæ´»ç”¨)
            reward = self.calculate_reward(reward_info)
            total_reward += reward
            
            # çµŒé¨“ã‚’ä¿å­˜
            self.remember(state, action, reward, next_state, done)
            
            # å­¦ç¿’ (batch_sizeæ´»ç”¨)
            if len(self.memory) >= self.batch_size:
                loss = self.replay()
                if loss is not None:
                    losses.append(loss)
            
            state = next_state
            
            if done:
                break
        
        # Îµæ›´æ–° (epsilon_decayæ´»ç”¨)
        self.update_epsilon()
        
        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒ¢ãƒ‡ãƒ«æ›´æ–°
        if len(self.training_stats['episodes']) % 10 == 0:
            self.update_target_model()
        
        # çµ±è¨ˆè¨˜éŒ²
        self.training_stats['episodes'].append(len(self.training_stats['episodes']))
        self.training_stats['rewards'].append(total_reward)
        self.training_stats['losses'].append(np.mean(losses) if losses else 0)
        
        return total_reward, np.mean(losses) if losses else 0


class ImprovedQuantumNetwork(nn.Module):
    """æ”¹å–„ã•ã‚ŒãŸé‡å­ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯"""
    
    def __init__(self, n_qubits, n_layers, embedding_type, entanglement, reward_type):
        super().__init__()
        
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.embedding_type = embedding_type
        self.entanglement = entanglement
        self.reward_type = reward_type
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ€ãƒ¼
        self.encoder = nn.Sequential(
            nn.Linear(36, 64),
            nn.ReLU(),
            nn.Dropout(0.1),  # ğŸ”§ è¿½åŠ : éå­¦ç¿’é˜²æ­¢
            nn.Linear(64, n_qubits)
        )
        
        # é‡å­å±¤ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
        quantum_layers = []
        for i in range(n_layers):
            if entanglement == 'linear':
                # ç·šå½¢ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
                quantum_layers.append(nn.Linear(n_qubits, n_qubits))
            elif entanglement == 'full':
                # å®Œå…¨ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
                quantum_layers.append(nn.Linear(n_qubits, n_qubits * 2))
                quantum_layers.append(nn.ReLU())
                quantum_layers.append(nn.Linear(n_qubits * 2, n_qubits))
            else:  # circular
                # å††å½¢ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ
                quantum_layers.append(nn.Conv1d(1, 1, 3, padding=1))
                quantum_layers.append(lambda x: x.squeeze(1))
            
            quantum_layers.append(nn.Tanh())
        
        self.quantum_circuit = nn.Sequential(*quantum_layers)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ€ãƒ¼
        self.decoder = nn.Sequential(
            nn.Linear(n_qubits, 32),
            nn.ReLU(),
            nn.Dropout(0.1),  # ğŸ”§ è¿½åŠ : éå­¦ç¿’é˜²æ­¢
            nn.Linear(32, 4)
        )
        
        # å ±é…¬ã‚¿ã‚¤ãƒ—ã«åŸºã¥ããƒã‚¤ã‚¢ã‚¹
        self.strategy_bias = nn.Parameter(self._get_strategy_bias())
    
    def _get_strategy_bias(self):
        """å ±é…¬ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãåˆæœŸãƒã‚¤ã‚¢ã‚¹"""
        if self.reward_type == 'aggressive':
            return torch.tensor([0.1, 0.1, -0.1, -0.1])  # å‰é€²å„ªå…ˆ
        elif self.reward_type == 'defensive':
            return torch.tensor([-0.1, -0.1, 0.1, 0.1])  # å¾Œé€€å„ªå…ˆ
        elif self.reward_type == 'escape':
            return torch.tensor([0.2, 0.0, 0.0, -0.2])  # æ¨ªç§»å‹•å„ªå…ˆ
        else:
            return torch.zeros(4)
    
    def forward(self, x):
        if x.dim() == 1:
            x = x.unsqueeze(0)
        
        # ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
        x = self.encoder(x)
        
        # åŸ‹ã‚è¾¼ã¿ã‚¿ã‚¤ãƒ—ã«åŸºã¥ãå‡¦ç†
        if self.embedding_type == 'angle':
            x = torch.tanh(x) * np.pi  # è§’åº¦åŸ‹ã‚è¾¼ã¿
        elif self.embedding_type == 'amplitude':
            x = torch.sigmoid(x)  # æŒ¯å¹…åŸ‹ã‚è¾¼ã¿
        
        # é‡å­å›è·¯
        if self.entanglement == 'circular' and x.dim() == 2:
            x = x.unsqueeze(1)
            x = self.quantum_circuit(x)
            x = x.squeeze(1)
        else:
            x = self.quantum_circuit(x)
        
        # ãƒ‡ã‚³ãƒ¼ãƒ‰
        x = self.decoder(x)
        
        # æˆ¦ç•¥ãƒã‚¤ã‚¢ã‚¹è¿½åŠ 
        x = x + self.strategy_bias
        
        return x


def get_ai_config():
    """å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ç”¨ã®è¨­å®šã‚’è¿”ã™"""
    return {
        'name': 'improved_3step_ai',
        'type': 'quantum_improved',
        'learning_method': 'reinforcement',
        
        # ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­è¨ˆ
        'placement': 'aggressive',
        'estimator': 'cqcnn',
        'reward': 'aggressive',
        'qmap': 'dqn',
        'action': 'epsilon',
        
        # æœ€é©åŒ–ã•ã‚ŒãŸãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        'learning_rate': 0.001,
        'batch_size': 32,
        'episodes': 1000,
        'epsilon': 0.1,
        'epsilon_decay': 0.995,
        'epsilon_min': 0.01,
        'gamma': 0.99,
        
        # æœ€é©åŒ–ã•ã‚ŒãŸé‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        'n_qubits': 6,
        'n_layers': 2,
        'embedding_type': 'angle',
        'entanglement': 'linear'
    }


if __name__ == "__main__":
    config = get_ai_config()
    
    print("ğŸš€ æ”¹å–„ã•ã‚ŒãŸ3step AI")
    print("=" * 60)
    print(f"è¨­å®š: {config['name']}")
    print(f"ã™ã¹ã¦ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æ´»ç”¨:")
    print(f"  - epsilon_decay: {config['epsilon_decay']} âœ…")
    print(f"  - gamma: {config['gamma']} âœ…")
    print(f"  - batch_size: {config['batch_size']} âœ…")
    print(f"  - reward_type: {config['reward']} âœ…")
    print(f"  - embedding_type: {config['embedding_type']} âœ…")
    print(f"  - entanglement: {config['entanglement']} âœ…")
    
    # ãƒ†ã‚¹ãƒˆ
    ai = ImprovedQuantumAI(config)
    test_state = torch.randn(1, 36)
    action = ai.select_action(test_state)
    print(f"\nãƒ†ã‚¹ãƒˆè¡Œå‹•é¸æŠ: {action}")
    print(f"ç¾åœ¨ã®Îµå€¤: {ai.epsilon}")
    
    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°ç¢ºèª
    param_count = sum(p.numel() for p in ai.model.parameters())
    print(f"ãƒ¢ãƒ‡ãƒ«ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {param_count} (æœ€é©åŒ–æ¸ˆã¿)")