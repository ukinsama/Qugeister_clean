#!/usr/bin/env python3
"""
é«˜é€Ÿé‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼
é‡å­å›è·¯ã‚’ä½¿ã„ãªãŒã‚‰å®Ÿç”¨çš„ãªé€Ÿåº¦ã§å­¦ç¿’ã‚’å®Ÿç¾
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pennylane as qml
from collections import deque
import random
import time
from tqdm import tqdm

# Import updated FastQuantumCircuit
from .quantum_circuit import FastQuantumCircuit
import pickle
from functools import lru_cache
import hashlib

# ===== é‡å­å›è·¯ã¯quantum_circuit.pyã‹ã‚‰ import =====

# ===== ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰é‡å­-å¤å…¸ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ =====
class FastQuantumNeuralNetwork(nn.Module):
    """é«˜é€ŸåŒ–ã•ã‚ŒãŸé‡å­ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šå¯¾å¿œï¼‰"""
    
    def __init__(self, input_dim=252, output_dim=36, n_qubits=4, n_layers=2, embedding='angle', entanglement='linear'):
        super().__init__()
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.output_dim = output_dim
        self.embedding = embedding
        self.entanglement = entanglement
        
        # 1. å¼·åŒ–ã•ã‚ŒãŸå‰å‡¦ç†CNNï¼ˆDeep Feature Extractionï¼‰
        self.preprocessor = nn.Sequential(
            # First CNN Block - Pattern Recognition  
            nn.Linear(input_dim, 512),
            nn.LayerNorm(512),  # LayerNormä½¿ç”¨ã§ãƒãƒƒãƒã‚µã‚¤ã‚º1ã«å¯¾å¿œ
            nn.ReLU(),
            nn.Dropout(0.2),
            
            # Second CNN Block - Feature Combination
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # Third CNN Block - Strategic Analysis
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            # Fourth CNN Block - Quantum Preparation
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.05),
            
            # Final Quantum Interface Layer
            nn.Linear(64, n_qubits),
            nn.Tanh()  # é‡å­å›è·¯ã®å…¥åŠ›ç¯„å›²ã«æ­£è¦åŒ– [-1, 1]
        )
        
        # 2. å¼·åŒ–é‡å­å›è·¯å±¤ï¼ˆMulti-Layer Quantum Processingï¼‰
        self.quantum_layer = FastQuantumCircuit(
            n_qubits=n_qubits, 
            n_layers=n_layers,
            embedding=embedding,
            entanglement=entanglement
        )
        
        # é‡å­ç‰¹å¾´å¢—å¼·å±¤ï¼ˆQuantum Feature Enhancementï¼‰
        self.quantum_enhancer = nn.Sequential(
            nn.Linear(n_qubits, n_qubits * 2),
            nn.Tanh(),
            nn.Linear(n_qubits * 2, n_qubits),
            nn.Tanh()
        )
        
        # 3. è¶…å¼·åŒ–å¾Œå‡¦ç†CNNï¼ˆDeep Q-Value Generationï¼‰
        self.postprocessor = nn.Sequential(
            # First Expansion Block - Quantum Feature Amplification
            nn.Linear(n_qubits, 128),
            nn.LayerNorm(128),  # LayerNormä½¿ç”¨ã§ãƒãƒƒãƒã‚µã‚¤ã‚º1ã«å¯¾å¿œ
            nn.ReLU(),
            nn.Dropout(0.1),
            
            # Second Expansion Block - Strategic Pattern Formation  
            nn.Linear(128, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # Third Expansion Block - Spatial Understanding
            nn.Linear(256, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.2),
            
            # Fourth Block - Advanced Strategy Synthesis
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            
            # Fifth Block - Q-Value Refinement
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            
            # Sixth Block - Spatial Mapping Preparation
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.05),
            
            # Final Q-Value Generation Layer
            nn.Linear(64, output_dim),  # 36 outputs for 6x6 Q-value map
            nn.Tanh()  # Normalize Q-values to [-1, 1] range for stability
        )
        
        # 4. å¼·åŒ–é‡å­å›è·¯ã®é‡ã¿ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°å¯¾å¿œï¼‰
        self.quantum_weights = nn.Parameter(torch.randn(n_layers, n_qubits, 2) * 0.1)  # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†ã®é‡ã¿
    
    def forward(self, x):
        # ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–
        batch_size = x.shape[0] if x.dim() > 1 else 1
        if x.dim() == 1:
            x = x.unsqueeze(0)
        
        # 1. å¼·åŒ–å‰å‡¦ç†ã§æ·±å±¤ç‰¹å¾´æŠ½å‡ºï¼ˆ252â†’4ï¼‰
        compressed = self.preprocessor(x)
        
        # 2. å¼·åŒ–é‡å­å›è·¯å‡¦ç†ï¼ˆ4â†’4ï¼‰
        quantum_outputs = []
        for i in range(batch_size):
            # å„ã‚µãƒ³ãƒ—ãƒ«ã‚’å¼·åŒ–é‡å­å›è·¯ã«é€šã™
            quantum_input = compressed[i] * np.pi  # [-Ï€, Ï€]ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            quantum_output = self.quantum_layer.forward(
                quantum_input, 
                self.quantum_weights
            )
            quantum_outputs.append(quantum_output)
        
        # 3. é‡å­å‡ºåŠ›ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        if batch_size > 1:
            quantum_features = torch.stack([torch.tensor(out, dtype=torch.float32) for out in quantum_outputs])
        else:
            quantum_features = torch.tensor(quantum_outputs[0], dtype=torch.float32).unsqueeze(0)
        
        # 4. é‡å­ç‰¹å¾´å¢—å¼·å‡¦ç†ï¼ˆ4â†’8â†’4ï¼‰
        enhanced_features = self.quantum_enhancer(quantum_features)
        
        # 5. é‡å­ç‰¹å¾´ã¨å¢—å¼·ç‰¹å¾´ã‚’çµåˆ
        combined_features = quantum_features + enhanced_features
        
        # 6. è¶…å¼·åŒ–å¾Œå‡¦ç†ã§36æ¬¡å…ƒQå€¤ãƒãƒƒãƒ—ç”Ÿæˆï¼ˆ4â†’36ï¼‰
        output = self.postprocessor(combined_features)
        
        return output.squeeze(0) if batch_size == 1 else output
    
    def get_qvalue_map(self, x):
        """36æ¬¡å…ƒå‡ºåŠ›ã‚’6x6ã®Qå€¤ãƒãƒƒãƒ—ã«å¤‰æ›"""
        output = self.forward(x)
        if output.dim() == 1:
            # Single sample: reshape to 6x6
            return output.reshape(6, 6)
        else:
            # Batch: reshape each sample to 6x6
            return output.reshape(-1, 6, 6)
    
    def get_action_from_qmap(self, x):
        """Qå€¤ãƒãƒƒãƒ—ã‹ã‚‰æœ€é©è¡Œå‹•ã‚’é¸æŠï¼ˆå¾“æ¥ã®5è¡Œå‹•ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰"""
        qvalue_map = self.get_qvalue_map(x)
        if qvalue_map.dim() == 2:  # Single sample
            # 6x6ãƒãƒƒãƒ—ã‹ã‚‰ä»£è¡¨çš„ãª5ã¤ã®é ˜åŸŸã®æœ€å¤§å€¤ã‚’å–å¾—
            regions = {
                0: qvalue_map[0:2, 0:3].max(),  # å·¦ä¸Šé ˜åŸŸ
                1: qvalue_map[0:2, 3:6].max(),  # å³ä¸Šé ˜åŸŸ
                2: qvalue_map[2:4, 1:5].max(),  # ä¸­å¤®é ˜åŸŸ
                3: qvalue_map[4:6, 0:3].max(),  # å·¦ä¸‹é ˜åŸŸ
                4: qvalue_map[4:6, 3:6].max(),  # å³ä¸‹é ˜åŸŸ
            }
            # 5ã¤ã®è¡Œå‹•ã«å¯¾å¿œã™ã‚‹Qå€¤ã‚’è¿”ã™
            return torch.tensor([regions[i] for i in range(5)])
        else:  # Batch
            batch_size = qvalue_map.shape[0]
            batch_actions = []
            for i in range(batch_size):
                single_map = qvalue_map[i]
                regions = {
                    0: single_map[0:2, 0:3].max(),
                    1: single_map[0:2, 3:6].max(),
                    2: single_map[2:4, 1:5].max(),
                    3: single_map[4:6, 0:3].max(),
                    4: single_map[4:6, 3:6].max(),
                }
                batch_actions.append([regions[i] for i in range(5)])
            return torch.tensor(batch_actions)

# ===== é«˜é€Ÿå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  =====
class FastQuantumTrainer:
    """é«˜é€Ÿé‡å­å›è·¯å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """
    
    def __init__(self, model, lr=0.001):
        self.model = model
        self.optimizer = optim.Adam(model.parameters(), lr=lr)
        self.replay_buffer = deque(maxlen=1000)
        
        # å­¦ç¿’çµ±è¨ˆ
        self.losses = []
        self.rewards = []
        
    def train_step(self, batch_size=8):
        """åŠ¹ç‡çš„ãªãƒãƒƒãƒå­¦ç¿’"""
        if len(self.replay_buffer) < batch_size:
            return None
        
        # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = random.sample(self.replay_buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)
        
        # ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›ï¼ˆåŠ¹ç‡åŒ–ï¼‰
        states = torch.cat(states)
        actions = torch.tensor(actions, dtype=torch.long)
        rewards = torch.tensor(rewards, dtype=torch.float32)
        next_states = torch.cat(next_states)
        dones = torch.tensor(dones, dtype=torch.float32)
        
        # Qå€¤è¨ˆç®—ï¼ˆ36æ¬¡å…ƒå‡ºåŠ›ã‹ã‚‰5è¡Œå‹•ç”¨Qå€¤ã‚’æŠ½å‡ºï¼‰
        current_q_actions = self.model.get_action_from_qmap(states)
        current_q = current_q_actions.gather(1, actions.unsqueeze(1)).squeeze()
        
        with torch.no_grad():
            next_q_actions = self.model.get_action_from_qmap(next_states)
            next_q = next_q_actions.max(1)[0]
            target_q = rewards + 0.99 * next_q * (1 - dones)
        
        # æå¤±è¨ˆç®—
        loss = nn.MSELoss()(current_q, target_q)
        
        # æœ€é©åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()
        
        self.losses.append(loss.item())
        return loss.item()

# ===== ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ«ãƒ¼ãƒ— =====
def train_fast_quantum(episodes=1000, n_qubits=4):
    """é«˜é€Ÿé‡å­å›è·¯å­¦ç¿’ã®å®Ÿè¡Œ"""
    
    print("=" * 60)
    print("ğŸš€ é«˜é€Ÿé‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å­¦ç¿’")
    print("=" * 60)
    print(f"é‡å­ãƒ“ãƒƒãƒˆæ•°: {n_qubits}")
    print(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {episodes}")
    print("=" * 60)
    
    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
    model = FastQuantumNeuralNetwork(n_qubits=n_qubits)
    trainer = FastQuantumTrainer(model)
    
    # é€²æ—è¡¨ç¤º
    episode_rewards = []
    start_time = time.time()
    
    with tqdm(total=episodes, desc="é‡å­å›è·¯å­¦ç¿’") as pbar:
        for episode in range(episodes):
            # ç’°å¢ƒãƒªã‚»ãƒƒãƒˆï¼ˆç°¡ç•¥åŒ–ï¼‰
            state = torch.randn(1, 252)
            episode_reward = 0
            done = False
            steps = 0
            
            while not done and steps < 100:
                # è¡Œå‹•é¸æŠï¼ˆÎµ-greedyï¼‰
                epsilon = max(0.01, 0.1 * (0.995 ** episode))
                if random.random() < epsilon:
                    action = random.randrange(5)
                else:
                    with torch.no_grad():
                        q_values = model.get_action_from_qmap(state)
                        action = q_values.argmax().item()
                
                # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—ï¼ˆç°¡ç•¥åŒ–ï¼‰
                next_state = torch.randn(1, 252)
                reward = random.uniform(-1, 1)
                done = random.random() < 0.1
                
                # çµŒé¨“ã‚’ä¿å­˜
                trainer.replay_buffer.append(
                    (state, action, reward, next_state, done)
                )
                
                # å­¦ç¿’
                loss = trainer.train_step()
                
                # æ›´æ–°
                state = next_state
                episode_reward += reward
                steps += 1
            
            episode_rewards.append(episode_reward)
            
            # é€²æ—æ›´æ–°
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(episode_rewards[-10:])
                elapsed = time.time() - start_time
                speed = (episode + 1) / elapsed
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
                cache_size = len(model.quantum_layer.lookup_table)
                
                pbar.set_postfix({
                    'Avg Reward': f'{avg_reward:.2f}',
                    'Speed': f'{speed:.1f} eps/s',
                    'Cache': f'{cache_size}/{model.quantum_layer.cache_size}',
                    'Îµ': f'{epsilon:.3f}'
                })
            
            pbar.update(1)
    
    # çµæœè¡¨ç¤º
    total_time = time.time() - start_time
    print(f"\nâœ… å­¦ç¿’å®Œäº†ï¼")
    print(f"ç·æ™‚é–“: {total_time:.1f}ç§’")
    print(f"é€Ÿåº¦: {episodes/total_time:.1f} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰/ç§’")
    print(f"æœ€çµ‚å ±é…¬: {np.mean(episode_rewards[-100:]):.2f}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    torch.save({
        'model_state_dict': model.state_dict(),
        'quantum_cache': model.quantum_layer.lookup_table,
        'rewards': episode_rewards
    }, 'fast_quantum_model.pth')
    
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ 'fast_quantum_model.pth' ã¨ã—ã¦ä¿å­˜")
    
    return model, episode_rewards

# ===== ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ =====
def benchmark_quantum_speedup():
    """é‡å­å›è·¯ã®é«˜é€ŸåŒ–åŠ¹æœã‚’æ¸¬å®š"""
    
    print("\n" + "=" * 60)
    print("ğŸ“Š é‡å­å›è·¯é«˜é€ŸåŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 60)
    
    # é€šå¸¸ã®é‡å­å›è·¯
    print("\n1. é€šå¸¸ã®é‡å­å›è·¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰:")
    circuit_nocache = FastQuantumCircuit(n_qubits=4)
    circuit_nocache._cache_enabled = False
    
    start = time.time()
    for _ in range(100):
        inputs = torch.randn(4) * np.pi
        weights = torch.randn(1, 4, 2) * 0.1
        _ = circuit_nocache.forward(inputs, weights)
    normal_time = time.time() - start
    print(f"   100å›å®Ÿè¡Œ: {normal_time:.2f}ç§’")
    
    # é«˜é€ŸåŒ–é‡å­å›è·¯
    print("\n2. é«˜é€ŸåŒ–é‡å­å›è·¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰:")
    circuit_fast = FastQuantumCircuit(n_qubits=4)
    
    start = time.time()
    for i in range(100):
        # ä¸€éƒ¨ã¯åŒã˜å…¥åŠ›ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰
        if i % 3 == 0:
            inputs = torch.zeros(4)
        else:
            inputs = torch.randn(4) * np.pi
        weights = torch.randn(1, 4, 2) * 0.1
        _ = circuit_fast.forward(inputs, weights)
    fast_time = time.time() - start
    print(f"   100å›å®Ÿè¡Œ: {fast_time:.2f}ç§’")
    print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {len(circuit_fast.lookup_table)}/100")
    
    print(f"\nâš¡ é«˜é€ŸåŒ–å€ç‡: {normal_time/fast_time:.1f}å€")
    
    # ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print("\n3. ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰:")
    model = FastQuantumNeuralNetwork(n_qubits=4)
    
    start = time.time()
    for _ in range(10):
        state = torch.randn(1, 252)
        _ = model(state)
    model_time = time.time() - start
    print(f"   10ã‚¹ãƒ†ãƒƒãƒ—: {model_time:.2f}ç§’")
    print(f"   æ¨å®š1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {model_time * 100:.0f}ç§’")

if __name__ == "__main__":
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    benchmark_quantum_speedup()
    
    # å­¦ç¿’å®Ÿè¡Œ
    print("\né‡å­å›è·¯å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ")
    print("1. ãƒ‡ãƒ¢ï¼ˆ100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰")
    print("2. æ¨™æº–ï¼ˆ1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰")
    print("3. ãƒ•ãƒ«ï¼ˆ10000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰")
    
    import sys
    if len(sys.argv) > 1:
        choice = sys.argv[1]
        if choice == "1":
            model, rewards = train_fast_quantum(100, n_qubits=4)
        elif choice == "2":
            model, rewards = train_fast_quantum(1000, n_qubits=4)
        elif choice == "3":
            model, rewards = train_fast_quantum(10000, n_qubits=4)
    else:
        print("\nãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ")
        model, rewards = train_fast_quantum(100, n_qubits=4)