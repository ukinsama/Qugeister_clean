{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本格CQCNN量子強化学習実験\n",
    "## JSON Config完全対応版 - 収束まで継続学習\n",
    "\n",
    "- **設定ファイル**: quantum_cqcnn_config_2025-09-24.json\n",
    "- **量子構成**: 4Q1L (4量子ビット, 1レイヤー)\n",
    "- **収束閾値**: 0.95 (Balance)\n",
    "- **最大エピソード**: 50,000\n",
    "- **アーキテクチャ**: 完全CQCNN (Frontend CNN → Quantum → Backend CNN)\n",
    "\n",
    "### 実験目標\n",
    "1. 0.95収束閾値での学習収束確認\n",
    "2. Ultra-strict実験(0.995)との比較分析\n",
    "3. 初期状態依存性の影響評価\n",
    "4. 量子効果の定量的測定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 量子強化学習実験環境 ===\n",
      "PyTorch version: 2.8.0+cpu\n",
      "PennyLane version: 0.42.3\n",
      "NumPy version: 2.3.3\n",
      "Random seed: 42\n",
      "Device: CPU\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# === 環境設定とライブラリ導入 ===\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 乱数シード設定（再現性確保）\n",
    "EXPERIMENT_SEED = 42\n",
    "random.seed(EXPERIMENT_SEED)\n",
    "np.random.seed(EXPERIMENT_SEED)\n",
    "torch.manual_seed(EXPERIMENT_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"=== 量子強化学習実験環境 ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PennyLane version: {qml.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 実験設定サマリー ===\n",
      "Algorithm: quantum_stable_spatial_36d\n",
      "Quantum: 4Q2L\n",
      "Embedding: angle\n",
      "Entanglement: full\n",
      "State Dimension: 252\n",
      "Action Space: 36D\n",
      "Batch Size: 96\n",
      "Learning Rate: 0.0006\n",
      "Epochs: 50000\n",
      "Epsilon: 0.5 → 0.08 (decay: 0.9998)\n",
      "Replay Buffer: 6000\n",
      "Target Update: every 150 episodes\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# === JSON設定読み込み ===\n",
    "config_path = \"quantum_recovery_stable_config_2025-09-25.json\"\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"=== 実験設定サマリー ===\")\n",
    "print(f\"Algorithm: {config['learning_config']['algorithm']}\")\n",
    "print(f\"Quantum: {config['module_config']['quantum']['n_qubits']}Q{config['module_config']['quantum']['n_layers']}L\")\n",
    "print(f\"Embedding: {config['module_config']['quantum']['embedding_type']}\")\n",
    "print(f\"Entanglement: {config['module_config']['quantum']['entanglement']}\")\n",
    "print(f\"State Dimension: {config['module_config']['quantum']['state_dimension']}\")\n",
    "print(f\"Action Space: {config['module_config']['qmap']['action_dim']}D\")\n",
    "print(f\"Batch Size: {config['hyperparameters']['batch_size']}\")\n",
    "print(f\"Learning Rate: {config['hyperparameters']['learning_rate']}\")\n",
    "print(f\"Epochs: {config['hyperparameters']['epochs']}\")\n",
    "print(f\"Epsilon: {config['hyperparameters']['epsilon']} → {config['hyperparameters']['epsilon_min']} (decay: {config['hyperparameters']['epsilon_decay']})\")\n",
    "print(f\"Replay Buffer: {config['hyperparameters']['replay_buffer_size']}\")\n",
    "print(f\"Target Update: every {config['hyperparameters']['target_update_freq']} episodes\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 設定値を変数に展開\n",
    "N_QUBITS = config['module_config']['quantum']['n_qubits']\n",
    "N_LAYERS = config['module_config']['quantum']['n_layers']\n",
    "STATE_DIM = config['module_config']['quantum']['state_dimension']\n",
    "ACTION_DIM = config['module_config']['qmap']['action_dim']\n",
    "BATCH_SIZE = config['hyperparameters']['batch_size']\n",
    "LEARNING_RATE = config['hyperparameters']['learning_rate']\n",
    "MAX_EPISODES = config['hyperparameters']['epochs']\n",
    "EPSILON_START = config['hyperparameters']['epsilon']\n",
    "EPSILON_DECAY = config['hyperparameters']['epsilon_decay']\n",
    "EPSILON_MIN = config['hyperparameters']['epsilon_min']\n",
    "BUFFER_SIZE = config['hyperparameters']['replay_buffer_size']\n",
    "TARGET_UPDATE = config['hyperparameters']['target_update_freq']\n",
    "GAMMA = config['hyperparameters']['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完全版ガイスター環境定義完了\n",
      "ガイスター環境初期化完了 - 報酬戦略: adaptive_anti_collapse\n",
      "状態ベクトル形状: (252,)\n",
      "有効手数: 8\n"
     ]
    }
   ],
   "source": [
    "# === 完全版ガイスター環境 ===\n",
    "class GeisterEnvironment:\n",
    "    \"\"\"完全なガイスターゲーム環境（JSON設定対応）\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.board_size = 6\n",
    "        self.forced_p2_setup_id = None\n",
    "        self.p2_backrow_cells = [(0, 1), (0, 2), (0, 3), (0, 4),\n",
    "                                (1, 1), (1, 2), (1, 3), (1, 4)]\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "        # 報酬設定（JSON設定から読み込み）\n",
    "        reward_config = config['module_config']['reward']\n",
    "        self.capture_good_reward = reward_config['capture_good_reward']\n",
    "        self.capture_bad_penalty = reward_config['capture_bad_penalty']\n",
    "        self.escape_reward = reward_config['escape_reward']\n",
    "        self.captured_good_penalty = reward_config['captured_good_penalty']\n",
    "        self.captured_bad_reward = reward_config['captured_bad_reward']\n",
    "        self.position_rewards = reward_config['position_rewards']\n",
    "        \n",
    "        print(f\"ガイスター環境初期化完了 - 報酬戦略: {reward_config['strategy']}\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"ゲーム状態をリセット\"\"\"\n",
    "        self.board = np.zeros((6, 6), dtype=int)\n",
    "        self.turn = 0\n",
    "        self.current_player = 1\n",
    "        self.game_over = False\n",
    "        self.winner = None\n",
    "        self.captured_pieces = {'player_1': [], 'player_2': []}\n",
    "        self.move_history = []\n",
    "\n",
    "        # 配置設定（JSON設定から読み込み）\n",
    "        placement_config = self.config['module_config']['placement']\n",
    "        \n",
    "        if placement_config['type'] == 'custom':\n",
    "            # カスタム配置（JSON指定）\n",
    "            my_pieces = placement_config['my_pieces_config']\n",
    "            \n",
    "            # プレイヤー1（下側）\n",
    "            for i, row in enumerate(my_pieces):\n",
    "                for j, piece in enumerate(row):\n",
    "                    if piece != 0:\n",
    "                        self.board[4 + i][j] = piece\n",
    "\n",
    "            # ===== ここから P2（上側）の配置 =====\n",
    "            # 善=+2, 悪=-2 を前提にしています（必要ならあなたの符号に合わせて変更）\n",
    "            GOOD, BAD = 2, -2\n",
    "\n",
    "            # 8マス（2x4）の順序は __init__ で定義した p2_backrow_cells を使用\n",
    "            # もし未定義なら定義しておく\n",
    "            if not hasattr(self, 'p2_backrow_cells'):\n",
    "                self.p2_backrow_cells = [(0, 1), (0, 2), (0, 3), (0, 4),\n",
    "                                        (1, 1), (1, 2), (1, 3), (1, 4)]\n",
    "\n",
    "            # ---- 8C4 の「k番目の組合せ」を復元する関数（辞書順）----\n",
    "            from math import comb\n",
    "            def kth_comb_8_4(k: int):\n",
    "                \"\"\"0<=k<70 を 8要素から4要素を選ぶ辞書順のk番目に対応させ、昇順インデックス(4つ)を返す\"\"\"\n",
    "                res = []\n",
    "                n, r = 8, 4\n",
    "                x = 0\n",
    "                for i in range(r, 0, -1):\n",
    "                    for v in range(x, n):\n",
    "                        c = comb(n - v - 1, i - 1)\n",
    "                        if k < c:\n",
    "                            res.append(v)\n",
    "                            x = v + 1\n",
    "                            break\n",
    "                        k -= c\n",
    "                return res\n",
    "\n",
    "            # ---- P2配置の決定：強制IDがあれば適用、なければランダム ----\n",
    "            if getattr(self, 'forced_p2_setup_id', None) is not None:\n",
    "                sid = int(self.forced_p2_setup_id) % 70\n",
    "                red_idx = kth_comb_8_4(sid)          # 赤（悪）の位置(0..7)を取得\n",
    "                # まず全て善にしてから、赤インデックスの所だけ悪に置き換え\n",
    "                for idx, (r, c) in enumerate(self.p2_backrow_cells):\n",
    "                    self.board[r][c] = GOOD\n",
    "                for idx in red_idx:\n",
    "                    r, c = self.p2_backrow_cells[idx]\n",
    "                    self.board[r][c] = BAD\n",
    "            else:\n",
    "                # 従来のランダム配置（善4、悪4）を維持\n",
    "                pieces = [GOOD, GOOD, GOOD, GOOD, BAD, BAD, BAD, BAD]\n",
    "                random.shuffle(pieces)\n",
    "                for i, (r, c) in enumerate(self.p2_backrow_cells):\n",
    "                    self.board[r][c] = pieces[i]\n",
    "\n",
    "            # ---- 実際に用いられた P2 配置のID（0..69）を計算して保持 ----\n",
    "            # 赤（悪=-2）が置かれたインデックスを 0..7 で収集\n",
    "            red_positions = []\n",
    "            for idx, (r, c) in enumerate(self.p2_backrow_cells):\n",
    "                if self.board[r][c] == BAD:\n",
    "                    red_positions.append(idx)\n",
    "            red_positions.sort()\n",
    "\n",
    "            # rank（=組合せの辞書順ランク）を計算： 8C4 = 70\n",
    "            rank = 0\n",
    "            last = -1\n",
    "            for i, rr in enumerate(red_positions):\n",
    "                start = last + 1\n",
    "                for x in range(start, rr):\n",
    "                    rank += comb(8 - (x + 1), 4 - (i + 1))\n",
    "                last = rr\n",
    "            self.p2_setup_id = int(rank)   # ← これを後でモニタリングに使います\n",
    "\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"252次元状態ベクトル生成（7チャンネルエンコーディング）\"\"\"\n",
    "        state = np.zeros(STATE_DIM)  # 6*6*7 = 252\n",
    "        \n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                base_idx = (i * 6 + j) * 7\n",
    "                value = self.board[i][j]\n",
    "                \n",
    "                # 7チャンネル: 空, P1善, P1悪, P2善, P2悪, プレイヤー, ターン\n",
    "                if value == 0:     # 空\n",
    "                    state[base_idx] = 1\n",
    "                elif value == 1:   # プレイヤー1善玉\n",
    "                    state[base_idx + 1] = 1\n",
    "                elif value == -1:  # プレイヤー1悪玉\n",
    "                    state[base_idx + 2] = 1\n",
    "                elif value == 2:   # プレイヤー2善玉\n",
    "                    state[base_idx + 3] = 1\n",
    "                elif value == -2:  # プレイヤー2悪玉\n",
    "                    state[base_idx + 4] = 1\n",
    "                \n",
    "                # 追加情報\n",
    "                state[base_idx + 5] = 1 if self.current_player == 1 else 0\n",
    "                state[base_idx + 6] = min(self.turn / 100.0, 1.0)  # 正規化ターン\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, player=None):\n",
    "        \"\"\"有効な手を36次元インデックス（toセル）で取得\"\"\"\n",
    "        if player is None:\n",
    "            player = self.current_player\n",
    "\n",
    "        valid_moves = []\n",
    "        piece_values = [1, -1] if player == 1 else [2, -2]\n",
    "\n",
    "        for from_i in range(6):\n",
    "            for from_j in range(6):\n",
    "                if self.board[from_i][from_j] in piece_values:\n",
    "                    for di, dj in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n",
    "                        to_i, to_j = from_i + di, from_j + dj\n",
    "                        if 0 <= to_i < 6 and 0 <= to_j < 6:\n",
    "                            target = self.board[to_i][to_j]\n",
    "                            if (target == 0 or\n",
    "                                (player == 1 and abs(target) == 2) or\n",
    "                                (player == 2 and abs(target) == 1)):\n",
    "                                idx = to_i * 6 + to_j  # ← 0..35 に固定\n",
    "                                valid_moves.append((idx, \"move\", (from_i, from_j), (to_i, to_j)))\n",
    "        return valid_moves\n",
    "\n",
    "    def make_move(self, move):\n",
    "        \"\"\"手を実行し、報酬を計算\"\"\"\n",
    "        move_index, direction, from_pos, to_pos = move\n",
    "        from_i, from_j = from_pos\n",
    "        to_i, to_j = to_pos\n",
    "        \n",
    "        piece = self.board[from_i][from_j]\n",
    "        target = self.board[to_i][to_j]\n",
    "        \n",
    "        # 移動記録\n",
    "        self.move_history.append({\n",
    "            'turn': self.turn,\n",
    "            'player': self.current_player,\n",
    "            'move': move,\n",
    "            'piece': piece,\n",
    "            'captured': target if target != 0 else None\n",
    "        })\n",
    "        \n",
    "        # 駒を移動\n",
    "        self.board[from_i][from_j] = 0\n",
    "        self.board[to_i][to_j] = piece\n",
    "        \n",
    "        # 報酬計算\n",
    "        reward = self._calculate_reward(piece, target, from_pos, to_pos)\n",
    "        \n",
    "        # 勝利条件チェック\n",
    "        done = self._check_win_condition(piece, to_pos, target)\n",
    "        \n",
    "        # ターン進行\n",
    "        self.turn += 1\n",
    "        \n",
    "        # 最大ターン数チェック\n",
    "        if self.turn >= 200:  # 長期戦対応\n",
    "            self.game_over = True\n",
    "            self.winner = None\n",
    "            done = True\n",
    "        \n",
    "        if not done:\n",
    "            self.current_player = 2 if self.current_player == 1 else 1\n",
    "        \n",
    "        return self.get_state(), reward, done, {\n",
    "            'captured': target,\n",
    "            'winner': self.winner,\n",
    "            'turn': self.turn\n",
    "        }\n",
    "    \n",
    "    def _calculate_reward(self, piece, target, from_pos, to_pos):\n",
    "        \"\"\"詳細報酬計算（JSON設定準拠）\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # 駒捕獲報酬\n",
    "        if target != 0:\n",
    "            if target > 0:  # 善玉捕獲\n",
    "                reward += self.capture_good_reward\n",
    "            else:  # 悪玉捕獲\n",
    "                reward += self.capture_bad_penalty\n",
    "        \n",
    "        # 位置的報酬\n",
    "        from_i, from_j = from_pos\n",
    "        to_i, to_j = to_pos\n",
    "        \n",
    "        # 前進報酬\n",
    "        if abs(piece) == 1 and piece > 0:  # 善玉の場合\n",
    "            if self.current_player == 1 and to_i < from_i:  # 前進\n",
    "                reward += self.position_rewards['advance_toward_escape']\n",
    "            elif self.current_player == 2 and to_i > from_i:  # 前進\n",
    "                reward += self.position_rewards['advance_toward_escape']\n",
    "        \n",
    "        # 中央制御\n",
    "        if 2 <= to_i <= 3 and 2 <= to_j <= 3:\n",
    "            reward += self.position_rewards['center_control']\n",
    "        \n",
    "        # 相手陣地進入\n",
    "        if ((self.current_player == 1 and to_i <= 2) or \n",
    "            (self.current_player == 2 and to_i >= 3)):\n",
    "            reward += self.position_rewards['opponent_territory']\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _check_win_condition(self, piece, to_pos, captured):\n",
    "        \"\"\"勝利条件判定\"\"\"\n",
    "        to_i, to_j = to_pos\n",
    "        \n",
    "        # 脱出勝利\n",
    "        if abs(piece) == 1 and piece > 0:  # 善玉\n",
    "            if ((self.current_player == 1 and to_i == 0 and to_j in [0, 5]) or\n",
    "                (self.current_player == 2 and to_i == 5 and to_j in [0, 5])):\n",
    "                self.game_over = True\n",
    "                self.winner = self.current_player\n",
    "                return True\n",
    "        \n",
    "        # 善玉全捕獲勝利\n",
    "        if captured is not None and captured > 0:\n",
    "            opponent_good_count = 0\n",
    "            search_value = 2 if self.current_player == 1 else 1\n",
    "            \n",
    "            for i in range(6):\n",
    "                for j in range(6):\n",
    "                    if self.board[i][j] == search_value:\n",
    "                        opponent_good_count += 1\n",
    "            \n",
    "            if opponent_good_count == 0:\n",
    "                self.game_over = True\n",
    "                self.winner = self.current_player\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "print(\"完全版ガイスター環境定義完了\")\n",
    "\n",
    "# 環境テスト\n",
    "test_env = GeisterEnvironment(config)\n",
    "test_state = test_env.reset()\n",
    "print(f\"状態ベクトル形状: {test_state.shape}\")\n",
    "print(f\"有効手数: {len(test_env.get_valid_moves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完全版CQCNN定義完了\n",
      "CQCNN初期化: 252D → 4Q2L → 36D\n",
      "Frontend: 4 Linear layers\n",
      "Quantum: 4 qubits, 2 layers, angle embedding\n",
      "Backend: 5 Linear layers\n",
      "\n",
      "モデル構造テスト中...\n",
      "入力形状: torch.Size([4, 252])\n",
      "出力形状: torch.Size([4, 36])\n",
      "出力範囲: [-0.786, 0.611]\n",
      "\n",
      "パラメータ数:\n",
      "  Classical: 73,128\n",
      "  Quantum: 16\n",
      "  Total: 73,144\n",
      "\n",
      "CQCNNテスト完了!\n"
     ]
    }
   ],
   "source": [
    "# === 完全版CQCNN実装 ===\n",
    "class CQCNN(nn.Module):\n",
    "    \"\"\"完全なClassical-Quantum Convolutional Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 設定読み込み\n",
    "        arch_config = config['architecture']\n",
    "        quantum_config = config['module_config']['quantum']\n",
    "        \n",
    "        self.n_qubits = quantum_config['n_qubits']\n",
    "        self.n_layers = quantum_config['n_layers']\n",
    "        self.state_dim = quantum_config['state_dimension']\n",
    "        self.action_dim = config['module_config']['qmap']['action_dim']\n",
    "        \n",
    "        print(f\"CQCNN初期化: {self.state_dim}D → {self.n_qubits}Q{self.n_layers}L → {self.action_dim}D\")\n",
    "        \n",
    "        # Frontend CNN (252 → 4)\n",
    "        self.frontend_layers = self._build_layers(arch_config['frontend_cnn']['layers'])\n",
    "        \n",
    "        # Quantum Section\n",
    "        self.dev = qml.device(\n",
    "            arch_config['quantum_section']['device'], \n",
    "            wires=self.n_qubits\n",
    "        )\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(self.n_layers, self.n_qubits, 2) * 0.1\n",
    "        )\n",
    "        self.quantum_node = qml.QNode(\n",
    "            self._quantum_circuit, \n",
    "            self.dev, \n",
    "            interface='torch'\n",
    "        )\n",
    "        \n",
    "        # Backend CNN (4 → 36)\n",
    "        self.backend_layers = self._build_layers(arch_config['backend_cnn']['layers'])\n",
    "        \n",
    "        print(f\"Frontend: {len([l for l in self.frontend_layers if isinstance(l, nn.Linear)])} Linear layers\")\n",
    "        print(f\"Quantum: {self.n_qubits} qubits, {self.n_layers} layers, {quantum_config['embedding_type']} embedding\")\n",
    "        print(f\"Backend: {len([l for l in self.backend_layers if isinstance(l, nn.Linear)])} Linear layers\")\n",
    "\n",
    "    def _build_layers(self, layer_configs):\n",
    "        \"\"\"JSON設定からレイヤー構築\"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        for layer_config in layer_configs:\n",
    "            layer_type = layer_config['type']\n",
    "            \n",
    "            if layer_type == 'linear':\n",
    "                layers.append(nn.Linear(\n",
    "                    layer_config['in_features'],\n",
    "                    layer_config['out_features']\n",
    "                ))\n",
    "            elif layer_type == 'batch_norm':\n",
    "                layers.append(nn.BatchNorm1d(layer_config['num_features']))\n",
    "            elif layer_type == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif layer_type == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif layer_type == 'dropout':\n",
    "                layers.append(nn.Dropout(layer_config['p']))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _quantum_circuit(self, features, params):\n",
    "        \"\"\"量子回路（JSON設定準拠）\"\"\"\n",
    "        # Angle embedding\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(features[i], wires=i)\n",
    "        \n",
    "        # Variational layers\n",
    "        for layer in range(self.n_layers):\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RY(params[layer, i, 0], wires=i)\n",
    "                qml.RZ(params[layer, i, 1], wires=i)\n",
    "            \n",
    "            # Linear entanglement\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"順伝播処理\"\"\"\n",
    "        batch_size = state.shape[0]\n",
    "        \n",
    "        # Frontend processing\n",
    "        state_flat = state.view(batch_size, -1)  # Flatten\n",
    "        frontend_out = self.frontend_layers(state_flat)  # (batch, 4)\n",
    "        \n",
    "        # Quantum processing (batch対応)\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            q_out = self.quantum_node(frontend_out[i], self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_out = torch.stack(quantum_outputs)  # (batch, 4)\n",
    "        \n",
    "        # Backend processing\n",
    "        output = self.backend_layers(quantum_out)  # (batch, 36)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"完全版CQCNN定義完了\")\n",
    "\n",
    "# モデル初期化テスト\n",
    "test_model = CQCNN(config)\n",
    "test_input = torch.randn(4, STATE_DIM)  # バッチサイズ4\n",
    "\n",
    "print(\"\\nモデル構造テスト中...\")\n",
    "with torch.no_grad():\n",
    "    test_output = test_model(test_input)\n",
    "    print(f\"入力形状: {test_input.shape}\")\n",
    "    print(f\"出力形状: {test_output.shape}\")\n",
    "    print(f\"出力範囲: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "    \n",
    "    # パラメータ数計算\n",
    "    total_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "    quantum_params = test_model.quantum_params.numel()\n",
    "    classical_params = total_params - quantum_params\n",
    "    \n",
    "    print(f\"\\nパラメータ数:\")\n",
    "    print(f\"  Classical: {classical_params:,}\")\n",
    "    print(f\"  Quantum: {quantum_params:,}\")\n",
    "    print(f\"  Total: {total_params:,}\")\n",
    "\n",
    "print(\"\\nCQCNNテスト完了!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "収束検出システム定義完了\n"
     ]
    }
   ],
   "source": [
    "# === 収束検出システム ===\n",
    "class ConvergenceDetector:\n",
    "    \"\"\"収束検出システム（0.95閾値）\"\"\"\n",
    "\n",
    "    def __init__(self, patience=50, min_games=1000, balance_threshold=0.95):\n",
    "        self.patience = patience\n",
    "        self.min_games = min_games\n",
    "        self.balance_threshold = balance_threshold\n",
    "        self.consecutive_good = 0\n",
    "        self.best_balance = 0.0\n",
    "        self.convergence_history = []\n",
    "        \n",
    "        \n",
    "        print(f\"収束検出器: 閾値={balance_threshold}, patience={patience}, 最小ゲーム数={min_games}\")\n",
    "\n",
    "    def check_convergence(self, game_results, episode):\n",
    "        \"\"\"収束判定\"\"\"\n",
    "        if len(game_results) < self.min_games:\n",
    "            return False, 0.0, {'reason': 'insufficient_games', 'games': len(game_results)}\n",
    "\n",
    "        # 最近の結果を分析\n",
    "        recent_games = game_results[-500:]\n",
    "        \n",
    "        wins_1 = sum(1 for r in recent_games if r.get('winner') == 1)\n",
    "        wins_2 = sum(1 for r in recent_games if r.get('winner') == 2)\n",
    "        draws = sum(1 for r in recent_games if r.get('winner') is None)\n",
    "        \n",
    "        total_games = len(recent_games)\n",
    "        decisive_games = wins_1 + wins_2\n",
    "        \n",
    "        # バランス計算\n",
    "        if decisive_games > 0:\n",
    "            balance = min(wins_1, wins_2) / max(wins_1, wins_2)\n",
    "            win_rate_1 = wins_1 / total_games\n",
    "            win_rate_2 = wins_2 / total_games\n",
    "            draw_rate = draws / total_games\n",
    "        else:\n",
    "            balance = 1.0  # 全引き分けの場合\n",
    "            win_rate_1 = win_rate_2 = 0.0\n",
    "            draw_rate = 1.0\n",
    "        \n",
    "        # 収束判定\n",
    "        is_balanced = balance >= self.balance_threshold\n",
    "        has_active_games = decisive_games >= 50  # 少なくとも50ゲームは決着\n",
    "        \n",
    "        metrics = {\n",
    "            'balance': balance,\n",
    "            'win_rate_1': win_rate_1,\n",
    "            'win_rate_2': win_rate_2,\n",
    "            'draw_rate': draw_rate,\n",
    "            'decisive_games': decisive_games,\n",
    "            'total_games': total_games,\n",
    "            'is_balanced': is_balanced,\n",
    "            'has_active_games': has_active_games\n",
    "        }\n",
    "        \n",
    "        # 連続カウント\n",
    "        if is_balanced and has_active_games:\n",
    "            self.consecutive_good += 1\n",
    "            metrics['status'] = 'converging'\n",
    "        else:\n",
    "            self.consecutive_good = 0\n",
    "            if not is_balanced:\n",
    "                metrics['status'] = 'unbalanced'\n",
    "            elif not has_active_games:\n",
    "                metrics['status'] = 'too_many_draws'\n",
    "        \n",
    "        metrics['consecutive_good'] = self.consecutive_good\n",
    "        self.best_balance = max(self.best_balance, balance)\n",
    "        metrics['best_balance'] = self.best_balance\n",
    "        \n",
    "        # 履歴記録\n",
    "        self.convergence_history.append({\n",
    "            'episode': episode,\n",
    "            'metrics': metrics.copy()\n",
    "        })\n",
    "        \n",
    "        # 収束判定\n",
    "        converged = self.consecutive_good >= self.patience\n",
    "        \n",
    "        return converged, balance, metrics\n",
    "\n",
    "print(\"収束検出システム定義完了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完全版CQCNN実験クラス定義完了\n"
     ]
    }
   ],
   "source": [
    "# === メイン実験クラス ===\n",
    "class CQCNNExperiment:\n",
    "    \"\"\"完全版CQCNN自己対戦実験\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = GeisterEnvironment(config)\n",
    "\n",
    "        # 70配置の戦績とスケジュール設定\n",
    "        self.setup_stats = {sid: {'w':0, 'd':0, 'l':0, 'n':0} for sid in range(70)}\n",
    "        self.min_coverage_per_sid = 50     # 各setupの最低対局数\n",
    "        self.ucb_alpha = 0.8               # UCBの探索重み\n",
    "        self.ucb_top_k = 8                 # UCBスコア上位から抽選\n",
    "        self.role_mirror_prob = 0.15       # （将来）P1可変/P2固定のミラーを混ぜる確率\n",
    "\n",
    "        # モニター用CSV\n",
    "        self.metrics_csv_path = 'training_metrics.csv'\n",
    "        with open(self.metrics_csv_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('episode,mean_wr,worst_decile,variance,coverage_min,coverage_ok\\n')\n",
    "        \n",
    "        # モデル初期化\n",
    "        self.cqcnn_1 = CQCNN(config)\n",
    "        self.cqcnn_2 = CQCNN(config)\n",
    "        \n",
    "        # プレイヤー2のパラメータを少し変更（多様性確保）\n",
    "        with torch.no_grad():\n",
    "            for param in self.cqcnn_2.parameters():\n",
    "                param.add_(torch.randn_like(param) * 0.01)\n",
    "        \n",
    "        # オプティマイザー\n",
    "        self.optimizer_1 = optim.Adam(self.cqcnn_1.parameters(), lr=LEARNING_RATE)\n",
    "        self.optimizer_2 = optim.Adam(self.cqcnn_2.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # 学習関連\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.replay_buffer_1 = deque(maxlen=BUFFER_SIZE)\n",
    "        self.replay_buffer_2 = deque(maxlen=BUFFER_SIZE)\n",
    "        \n",
    "        # 統計記録\n",
    "        self.game_results = []\n",
    "        self.losses_1 = []\n",
    "        self.losses_2 = []\n",
    "        self.epsilon_history = []\n",
    "        self.training_metrics = []\n",
    "        \n",
    "        # 収束検出\n",
    "        self.convergence_detector = ConvergenceDetector(\n",
    "            balance_threshold=0.95,  # 95%バランス閾値\n",
    "            patience=50,\n",
    "            min_games=1000\n",
    "        )\n",
    "        \n",
    "        # ===== 追加: 配置スケジューリング & モニタ初期化 =====\n",
    "        self.setup_stats = {sid: {'w':0, 'd':0, 'l':0, 'n':0} for sid in range(70)}\n",
    "        self.min_coverage_per_sid = 50     # 各setupの最低対局数\n",
    "        self.ucb_alpha = 0.8               # UCB探索重み\n",
    "        self.ucb_top_k = 8                 # UCB上位からランダム抽選\n",
    "        self.role_mirror_prob = 0.15       # （拡張用）P1可変/P2固定を混ぜる確率\n",
    "        self.metrics_csv_path = 'training_metrics.csv'\n",
    "        with open(self.metrics_csv_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('episode,mean_wr,worst_decile,variance,coverage_min,coverage_ok\\n')\n",
    "        # ================================================\n",
    "\n",
    "        # その他\n",
    "        self.start_time = time.time()\n",
    "        self.episode_times = []\n",
    "        \n",
    "        print(\"\\n=== CQCNN実験初期化完了 ===\")\n",
    "        print(f\"モデル1パラメータ: {sum(p.numel() for p in self.cqcnn_1.parameters()):,}\")\n",
    "        print(f\"モデル2パラメータ: {sum(p.numel() for p in self.cqcnn_2.parameters()):,}\")\n",
    "        print(f\"リプレイバッファサイズ: {BUFFER_SIZE:,}\")\n",
    "        print(f\"バッチサイズ: {BATCH_SIZE}\")\n",
    "        print(f\"最大エピソード: {MAX_EPISODES:,}\")\n",
    "\n",
    "    # ===== 追加: 配置モニタ/スケジューラのユーティリティ =====\n",
    "    def _record_setup_result(self, setup_id: int, winner: int | None):\n",
    "        s = self.setup_stats[setup_id]\n",
    "        if winner is None: s['d'] += 1\n",
    "        elif winner == 1:  s['w'] += 1\n",
    "        else:              s['l'] += 1\n",
    "        s['n'] += 1\n",
    "\n",
    "    def _compute_setup_metrics(self):\n",
    "        wrs, counts = [], []\n",
    "        for sid in range(70):\n",
    "            s = self.setup_stats[sid]; n = max(1, s['n'])\n",
    "            wr = (s['w'] + 0.5*s['d']) / n\n",
    "            wrs.append(wr); counts.append(s['n'])\n",
    "        wrs = np.array(wrs); counts = np.array(counts)\n",
    "        mean_wr = float(wrs.mean())\n",
    "        worst_decile = float(np.sort(wrs)[:max(1, 70//10)].mean())\n",
    "        variance = float(wrs.var())\n",
    "        coverage_ok = bool(counts.min() >= self.min_coverage_per_sid)\n",
    "        return dict(mean_wr=mean_wr, worst_decile=worst_decile,\n",
    "                    variance=variance, coverage_ok=coverage_ok,\n",
    "                    coverage_min=int(counts.min()))\n",
    "\n",
    "    def _pick_setup_min_coverage(self):\n",
    "        need = [sid for sid in range(70) if self.setup_stats[sid]['n'] < self.min_coverage_per_sid]\n",
    "        return random.choice(need) if need else None\n",
    "\n",
    "    def _pick_setup_ucb(self):\n",
    "        import math  # 局所importで依存を閉じる\n",
    "        total = 1 + sum(self.setup_stats[sid]['n'] for sid in range(70))\n",
    "        scores = []\n",
    "        for sid in range(70):\n",
    "            s = self.setup_stats[sid]; n = s['n']\n",
    "            wr = (s['w'] + 0.5*s['d']) / max(1, n)\n",
    "            exploit = 1.0 - wr\n",
    "            explore = self.ucb_alpha * math.sqrt(math.log(total) / (1 + n))\n",
    "            scores.append((exploit + explore, sid))\n",
    "        scores.sort(reverse=True)\n",
    "        cand = [sid for _, sid in scores[:self.ucb_top_k]]\n",
    "        return random.choice(cand)\n",
    "\n",
    "    def choose_p2_setup_id(self):\n",
    "        sid = self._pick_setup_min_coverage()\n",
    "        if sid is not None: return sid\n",
    "        return self._pick_setup_ucb()\n",
    "    # =====================================================\n",
    "\n",
    "    def select_action(self, model, state, valid_moves, epsilon):\n",
    "        \"\"\"36次元空間でのアクション選択\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(valid_moves)\n",
    "\n",
    "        was_training = model.training\n",
    "        model.eval()  # 推論モードに設定\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).flatten().unsqueeze(0)  # (1, 252)\n",
    "            q_values = model(state_tensor).squeeze(0)  # (36,)\n",
    "        if was_training:\n",
    "            model.train()  # 元のモードに戻す\n",
    "\n",
    "        # 有効な手の中から最大Q値を選択\n",
    "        best_score = float('-inf')\n",
    "        best_move = valid_moves[0]\n",
    "\n",
    "        for move in valid_moves:\n",
    "            move_index, direction, from_pos, to_pos = move\n",
    "            if move_index < len(q_values):\n",
    "                score = q_values[move_index].item()\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_move = move\n",
    "        return best_move\n",
    "\n",
    "    def play_game(self, episode):\n",
    "        \"\"\"1ゲームを実行\"\"\"\n",
    "        # （将来: 役割ミラーをするならここで分岐）\n",
    "        self.env.forced_p2_setup_id = self.choose_p2_setup_id()\n",
    "\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        \n",
    "        game_states_1 = []\n",
    "        game_states_2 = []\n",
    "        \n",
    "        epsilon = max(EPSILON_MIN, EPSILON_START * (EPSILON_DECAY ** episode))\n",
    "        \n",
    "        while not done:\n",
    "            valid_moves = self.env.get_valid_moves()\n",
    "            \n",
    "            if not valid_moves:\n",
    "                # 有効手なし（稀なケース）\n",
    "                self.env.game_over = True\n",
    "                self.env.winner = None\n",
    "                break\n",
    "            \n",
    "            current_state = state.copy()\n",
    "            current_player = self.env.current_player\n",
    "            \n",
    "            if current_player == 1:\n",
    "                chosen_move = self.select_action(self.cqcnn_1, current_state, valid_moves, epsilon)\n",
    "            else:\n",
    "                chosen_move = self.select_action(self.cqcnn_2, current_state, valid_moves, epsilon)\n",
    "            \n",
    "            next_state, reward, done, info = self.env.make_move(chosen_move)\n",
    "            \n",
    "            # 経験を記録（move_indexはタプル先頭を使用）\n",
    "            if isinstance(chosen_move, (tuple, list)) and len(chosen_move) >= 1:\n",
    "                move_index = int(chosen_move[0])  # 0..35\n",
    "            else:\n",
    "                move_index = int(chosen_move)\n",
    "            experience = (current_state, move_index, reward, next_state, done)\n",
    "            \n",
    "            if current_player == 1:\n",
    "                game_states_1.append(experience)\n",
    "            else:\n",
    "                game_states_2.append(experience)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # ゲーム結果を記録\n",
    "        result = {\n",
    "            'episode': episode,\n",
    "            'winner': self.env.winner,\n",
    "            'turns': self.env.turn,\n",
    "            'player_1_moves': len(game_states_1),\n",
    "            'player_2_moves': len(game_states_2)\n",
    "        }\n",
    "        \n",
    "        # 最終報酬の配布（勝者に+1、敗者に-1、引き分けは0）\n",
    "        final_reward_1 = 1.0 if self.env.winner == 1 else (-1.0 if self.env.winner == 2 else 0.0)\n",
    "        final_reward_2 = 1.0 if self.env.winner == 2 else (-1.0 if self.env.winner == 1 else 0.0)\n",
    "        \n",
    "        # 経験をリプレイバッファに追加（sid付き6タプル）\n",
    "        sid = getattr(self.env, 'p2_setup_id', None)\n",
    "        if sid is not None:\n",
    "            # P1視点で w/d/l を更新（winner: 1=先手勝ち, 2=後手勝ち, None=引分）\n",
    "            self._record_setup_result(sid, self.env.winner)\n",
    "\n",
    "        for state, action, reward, next_state, done in game_states_1:\n",
    "            final_exp = (state, action, reward + final_reward_1, next_state, done, sid)\n",
    "            self.replay_buffer_1.append(final_exp)\n",
    "\n",
    "        for state, action, reward, next_state, done in game_states_2:\n",
    "            final_exp = (state, action, reward + final_reward_2, next_state, done, sid)\n",
    "            self.replay_buffer_2.append(final_exp)\n",
    "        \n",
    "        return result, epsilon\n",
    "\n",
    "    def train_model(self, model, optimizer, replay_buffer, losses_list):\n",
    "        \"\"\"モデル学習\"\"\"\n",
    "        if len(replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # バッチサンプリング（※ sid付き6タプル/旧5タプル両対応）\n",
    "        batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "        # 後方互換で展開\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        # sids は今は未使用だが将来の分層学習で利用予定\n",
    "        for exp in batch:\n",
    "            if len(exp) == 6:\n",
    "                s, a, r, ns, d, sid = exp\n",
    "            else:\n",
    "                s, a, r, ns, d = exp\n",
    "                sid = None\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            next_states.append(ns)\n",
    "            dones.append(d)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatFloatTensor(next_states) if hasattr(torch, \"FloatFloatTensor\") else torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        # 現在のQ値\n",
    "        current_q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # ターゲットQ値（簡単なTD学習）\n",
    "        with torch.no_grad():\n",
    "            next_q_values = model(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (GAMMA * next_q_values * ~dones)\n",
    "        \n",
    "        # 損失計算と更新\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses_list.append(loss.item())\n",
    "\n",
    "    def run_experiment(self):\n",
    "        \"\"\"メイン実験実行\"\"\"\n",
    "        print(f\"\\n=== 実験開始: 収束まで最大{MAX_EPISODES:,}エピソード ===\")\n",
    "        print(f\"収束条件: Balance ≥ 0.95, {self.convergence_detector.patience}回連続\")\n",
    "        \n",
    "        for episode in range(MAX_EPISODES):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            # ゲーム実行\n",
    "            result, epsilon = self.play_game(episode)\n",
    "            self.game_results.append(result)\n",
    "            self.epsilon_history.append(epsilon)\n",
    "            \n",
    "            # 学習実行\n",
    "            if episode % 5 == 0:  # 5エピソードごとに学習\n",
    "                self.train_model(self.cqcnn_1, self.optimizer_1, self.replay_buffer_1, self.losses_1)\n",
    "                self.train_model(self.cqcnn_2, self.optimizer_2, self.replay_buffer_2, self.losses_2)\n",
    "            \n",
    "            episode_time = time.time() - episode_start\n",
    "            self.episode_times.append(episode_time)\n",
    "            \n",
    "            # 進捗報告\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                self._print_progress(episode)\n",
    "                \n",
    "                # 収束チェック\n",
    "                converged, balance, metrics = self.convergence_detector.check_convergence(\n",
    "                    self.game_results, episode\n",
    "                )\n",
    "                \n",
    "                if converged:\n",
    "                    print(f\"\\n🎉 収束達成! Episode {episode+1}\")\n",
    "                    print(f\"Final Balance: {balance:.4f}\")\n",
    "                    print(f\"Consecutive Good: {metrics['consecutive_good']}\")\n",
    "                    break\n",
    "\n",
    "            # ===== 追加: 配置メトリクスの定期モニタ =====\n",
    "            if (episode + 1) % 500 == 0:\n",
    "                m = self._compute_setup_metrics()\n",
    "                print(f\"[monitor] ep={episode+1} mean={m['mean_wr']:.3f} \"\n",
    "                      f\"worst10%={m['worst_decile']:.3f} var={m['variance']:.4f} \"\n",
    "                      f\"cov_min={m['coverage_min']} cov_ok={m['coverage_ok']}\")\n",
    "                with open(self.metrics_csv_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"{episode+1},{m['mean_wr']:.6f},{m['worst_decile']:.6f},\"\n",
    "                            f\"{m['variance']:.6f},{m['coverage_min']},{int(m['coverage_ok'])}\\n\")\n",
    "            # =======================================\n",
    "\n",
    "            # 早期収束チェック（1000エピソードごと）\n",
    "            if (episode + 1) % 1000 == 0:\n",
    "                converged, balance, metrics = self.convergence_detector.check_convergence(\n",
    "                    self.game_results, episode\n",
    "                )\n",
    "                if converged:\n",
    "                    print(f\"\\n🎉 収束達成! Episode {episode+1}\")\n",
    "                    break\n",
    "        \n",
    "        total_time = time.time() - self.start_time\n",
    "        \n",
    "        print(f\"\\n=== 実験完了 ===\")\n",
    "        print(f\"Total Episodes: {len(self.game_results):,}\")\n",
    "        print(f\"Total Time: {total_time:.1f}s ({total_time/3600:.1f}h)\")\n",
    "        print(f\"Average Episode Time: {np.mean(self.episode_times):.3f}s\")\n",
    "        \n",
    "        # 最終収束チェック\n",
    "        final_converged, final_balance, final_metrics = self.convergence_detector.check_convergence(\n",
    "            self.game_results, len(self.game_results)-1\n",
    "        )\n",
    "        \n",
    "        return len(self.game_results), total_time, {\n",
    "            'converged': final_converged,\n",
    "            'balance': final_balance,\n",
    "            'metrics': final_metrics,\n",
    "            'total_games': len(self.game_results)\n",
    "        }\n",
    "\n",
    "    def _print_progress(self, episode):\n",
    "        \"\"\"進捗表示\"\"\"\n",
    "        recent_window = min(100, len(self.game_results))\n",
    "        recent_results = self.game_results[-recent_window:]\n",
    "        \n",
    "        wins_1 = sum(1 for r in recent_results if r['winner'] == 1)\n",
    "        wins_2 = sum(1 for r in recent_results if r['winner'] == 2)\n",
    "        draws = sum(1 for r in recent_results if r['winner'] is None)\n",
    "        \n",
    "        win_rate_1 = wins_1 / recent_window\n",
    "        win_rate_2 = wins_2 / recent_window\n",
    "        draw_rate = draws / recent_window\n",
    "        \n",
    "        balance = min(wins_1, wins_2) / max(wins_1, wins_2) if max(wins_1, wins_2) > 0 else 1.0\n",
    "        avg_turns = np.mean([r['turns'] for r in recent_results])\n",
    "        \n",
    "        current_epsilon = self.epsilon_history[-1] if self.epsilon_history else EPSILON_START\n",
    "        avg_loss_1 = np.mean(self.losses_1[-50:]) if len(self.losses_1) >= 50 else 0\n",
    "        avg_loss_2 = np.mean(self.losses_2[-50:]) if len(self.losses_2) >= 50 else 0\n",
    "        \n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        \n",
    "        print(f\"Episode {episode+1:5d} | \"\n",
    "              f\"P1={win_rate_1:.3f} P2={win_rate_2:.3f} D={draw_rate:.3f} | \"\n",
    "              f\"Balance={balance:.4f} | \"\n",
    "              f\"Turns={avg_turns:.1f} | \"\n",
    "              f\"ε={current_epsilon:.4f} | \"\n",
    "              f\"Loss={avg_loss_1:.4f}/{avg_loss_2:.4f} | \"\n",
    "              f\"Time={elapsed_time:.0f}s\")\n",
    "\n",
    "print(\"完全版CQCNN実験クラス定義完了\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CQCNN量子強化学習実験を開始します\n",
      "設定: quantum_recovery_stable_config_2025-09-25.json\n",
      "最大エピソード: 50,000\n",
      "目標: Balance ≥ 0.95, 50回連続達成で収束\n",
      "ガイスター環境初期化完了 - 報酬戦略: adaptive_anti_collapse\n",
      "CQCNN初期化: 252D → 4Q2L → 36D\n",
      "Frontend: 4 Linear layers\n",
      "Quantum: 4 qubits, 2 layers, angle embedding\n",
      "Backend: 5 Linear layers\n",
      "CQCNN初期化: 252D → 4Q2L → 36D\n",
      "Frontend: 4 Linear layers\n",
      "Quantum: 4 qubits, 2 layers, angle embedding\n",
      "Backend: 5 Linear layers\n",
      "収束検出器: 閾値=0.95, patience=50, 最小ゲーム数=1000\n",
      "\n",
      "=== CQCNN実験初期化完了 ===\n",
      "モデル1パラメータ: 73,144\n",
      "モデル2パラメータ: 73,144\n",
      "リプレイバッファサイズ: 6,000\n",
      "バッチサイズ: 96\n",
      "最大エピソード: 50,000\n",
      "\n",
      "=== 実験開始: 収束まで最大50,000エピソード ===\n",
      "収束条件: Balance ≥ 0.95, 50回連続\n",
      "Episode   100 | P1=0.880 P2=0.110 D=0.010 | Balance=0.1250 | Turns=61.0 | ε=0.4902 | Loss=0.0000/0.0000 | Time=19s\n",
      "Episode   200 | P1=0.810 P2=0.090 D=0.100 | Balance=0.1111 | Turns=80.0 | ε=0.4805 | Loss=0.0000/0.0000 | Time=43s\n",
      "Episode   300 | P1=0.840 P2=0.130 D=0.030 | Balance=0.1548 | Turns=67.1 | ε=0.4710 | Loss=35.0755/12.1163 | Time=67s\n",
      "Episode   400 | P1=0.820 P2=0.130 D=0.050 | Balance=0.1585 | Turns=68.8 | ε=0.4616 | Loss=55.9043/14.2098 | Time=88s\n",
      "Episode   500 | P1=0.840 P2=0.080 D=0.080 | Balance=0.0952 | Turns=65.7 | ε=0.4525 | Loss=102.3389/21.2446 | Time=109s\n",
      "Episode   600 | P1=0.700 P2=0.120 D=0.180 | Balance=0.1714 | Turns=92.7 | ε=0.4435 | Loss=205.5905/35.1771 | Time=133s\n",
      "Episode   700 | P1=0.680 P2=0.210 D=0.110 | Balance=0.3088 | Turns=80.0 | ε=0.4348 | Loss=311.4930/66.3092 | Time=157s\n",
      "Episode   800 | P1=0.800 P2=0.120 D=0.080 | Balance=0.1500 | Turns=74.5 | ε=0.4262 | Loss=357.9278/113.3702 | Time=182s\n",
      "Episode   900 | P1=0.800 P2=0.090 D=0.110 | Balance=0.1125 | Turns=92.2 | ε=0.4177 | Loss=350.7546/183.2004 | Time=210s\n",
      "Episode  1000 | P1=0.690 P2=0.200 D=0.110 | Balance=0.2899 | Turns=95.8 | ε=0.4094 | Loss=356.0003/257.2716 | Time=239s\n",
      "Episode  1100 | P1=0.930 P2=0.040 D=0.030 | Balance=0.0430 | Turns=67.0 | ε=0.4013 | Loss=339.2891/349.9188 | Time=263s\n",
      "Episode  1200 | P1=0.720 P2=0.170 D=0.110 | Balance=0.2361 | Turns=99.4 | ε=0.3934 | Loss=305.3844/407.0797 | Time=288s\n",
      "Episode  1300 | P1=0.580 P2=0.230 D=0.190 | Balance=0.3966 | Turns=108.0 | ε=0.3856 | Loss=260.1883/425.1022 | Time=314s\n",
      "Episode  1400 | P1=0.710 P2=0.110 D=0.180 | Balance=0.1549 | Turns=100.8 | ε=0.3780 | Loss=299.4309/372.6582 | Time=344s\n",
      "Episode  1500 | P1=0.320 P2=0.300 D=0.380 | Balance=0.9375 | Turns=145.4 | ε=0.3705 | Loss=311.3550/331.3585 | Time=380s\n",
      "Episode  1600 | P1=0.700 P2=0.140 D=0.160 | Balance=0.2000 | Turns=85.6 | ε=0.3631 | Loss=293.7011/318.9203 | Time=408s\n",
      "Episode  1700 | P1=0.550 P2=0.050 D=0.400 | Balance=0.0909 | Turns=117.6 | ε=0.3559 | Loss=247.0874/325.6772 | Time=440s\n",
      "Episode  1800 | P1=0.420 P2=0.240 D=0.340 | Balance=0.5714 | Turns=122.7 | ε=0.3489 | Loss=284.4727/332.9238 | Time=474s\n",
      "Episode  1900 | P1=0.340 P2=0.330 D=0.330 | Balance=0.9706 | Turns=132.3 | ε=0.3420 | Loss=367.2160/365.1043 | Time=503s\n",
      "Episode  2000 | P1=0.600 P2=0.160 D=0.240 | Balance=0.2667 | Turns=142.2 | ε=0.3352 | Loss=375.1163/339.7856 | Time=534s\n",
      "Episode  2100 | P1=0.440 P2=0.160 D=0.400 | Balance=0.3636 | Turns=139.2 | ε=0.3286 | Loss=338.4631/304.0280 | Time=565s\n",
      "Episode  2200 | P1=0.630 P2=0.060 D=0.310 | Balance=0.0952 | Turns=116.0 | ε=0.3221 | Loss=294.2337/297.8775 | Time=598s\n",
      "Episode  2300 | P1=0.310 P2=0.150 D=0.540 | Balance=0.4839 | Turns=152.4 | ε=0.3157 | Loss=285.9225/339.3861 | Time=631s\n",
      "Episode  2400 | P1=0.300 P2=0.170 D=0.530 | Balance=0.5667 | Turns=156.9 | ε=0.3094 | Loss=301.2171/351.1950 | Time=664s\n",
      "Episode  2500 | P1=0.700 P2=0.170 D=0.130 | Balance=0.2429 | Turns=96.2 | ε=0.3033 | Loss=321.8808/325.5001 | Time=690s\n",
      "Episode  2600 | P1=0.530 P2=0.120 D=0.350 | Balance=0.2264 | Turns=131.1 | ε=0.2973 | Loss=377.5986/324.6408 | Time=720s\n",
      "Episode  2700 | P1=0.460 P2=0.140 D=0.400 | Balance=0.3043 | Turns=136.9 | ε=0.2914 | Loss=426.9856/381.2336 | Time=751s\n",
      "Episode  2800 | P1=0.310 P2=0.150 D=0.540 | Balance=0.4839 | Turns=150.1 | ε=0.2856 | Loss=420.5974/617.3885 | Time=783s\n",
      "Episode  2900 | P1=0.560 P2=0.230 D=0.210 | Balance=0.4107 | Turns=97.4 | ε=0.2800 | Loss=450.3555/892.2150 | Time=810s\n",
      "Episode  3000 | P1=0.790 P2=0.130 D=0.080 | Balance=0.1646 | Turns=73.1 | ε=0.2744 | Loss=618.3347/1059.5914 | Time=834s\n",
      "Episode  3100 | P1=0.820 P2=0.120 D=0.060 | Balance=0.1463 | Turns=78.1 | ε=0.2690 | Loss=784.9222/899.8251 | Time=863s\n",
      "Episode  3200 | P1=0.810 P2=0.090 D=0.100 | Balance=0.1111 | Turns=95.1 | ε=0.2637 | Loss=880.0163/764.7620 | Time=896s\n",
      "Episode  3300 | P1=0.180 P2=0.130 D=0.690 | Balance=0.7222 | Turns=170.9 | ε=0.2585 | Loss=850.8854/676.0537 | Time=943s\n",
      "Episode  3400 | P1=0.280 P2=0.440 D=0.280 | Balance=0.6364 | Turns=122.7 | ε=0.2533 | Loss=747.9949/770.3604 | Time=978s\n",
      "Episode  3500 | P1=0.210 P2=0.470 D=0.320 | Balance=0.4468 | Turns=131.9 | ε=0.2483 | Loss=617.0609/790.7516 | Time=1011s\n",
      "Episode  3600 | P1=0.450 P2=0.440 D=0.110 | Balance=0.9778 | Turns=98.0 | ε=0.2434 | Loss=599.5679/798.7782 | Time=1043s\n",
      "Episode  3700 | P1=0.310 P2=0.200 D=0.490 | Balance=0.6452 | Turns=159.8 | ε=0.2386 | Loss=533.4281/756.0515 | Time=1084s\n",
      "Episode  3800 | P1=0.670 P2=0.190 D=0.140 | Balance=0.2836 | Turns=115.0 | ε=0.2339 | Loss=549.4805/762.3786 | Time=1113s\n",
      "Episode  3900 | P1=0.540 P2=0.330 D=0.130 | Balance=0.6111 | Turns=111.4 | ε=0.2292 | Loss=567.6911/707.8130 | Time=1150s\n",
      "Episode  4000 | P1=0.360 P2=0.440 D=0.200 | Balance=0.8182 | Turns=124.1 | ε=0.2247 | Loss=610.5861/775.5736 | Time=1191s\n",
      "Episode  4100 | P1=0.400 P2=0.350 D=0.250 | Balance=0.8750 | Turns=132.3 | ε=0.2202 | Loss=564.4658/794.8652 | Time=1232s\n",
      "Episode  4200 | P1=0.290 P2=0.250 D=0.460 | Balance=0.8621 | Turns=151.9 | ε=0.2159 | Loss=599.9228/692.1823 | Time=1279s\n",
      "Episode  4300 | P1=0.670 P2=0.150 D=0.180 | Balance=0.2239 | Turns=111.5 | ε=0.2116 | Loss=593.1891/587.3912 | Time=1318s\n",
      "Episode  4400 | P1=0.700 P2=0.210 D=0.090 | Balance=0.3000 | Turns=88.0 | ε=0.2074 | Loss=589.5590/478.9067 | Time=1347s\n",
      "Episode  4500 | P1=0.830 P2=0.130 D=0.040 | Balance=0.1566 | Turns=86.7 | ε=0.2033 | Loss=658.7451/445.3758 | Time=1373s\n",
      "Episode  4600 | P1=0.800 P2=0.110 D=0.090 | Balance=0.1375 | Turns=87.5 | ε=0.1993 | Loss=629.3536/395.3331 | Time=1400s\n",
      "Episode  4700 | P1=0.830 P2=0.110 D=0.060 | Balance=0.1325 | Turns=90.5 | ε=0.1953 | Loss=603.9421/360.3720 | Time=1429s\n",
      "Episode  4800 | P1=0.840 P2=0.070 D=0.090 | Balance=0.0833 | Turns=85.3 | ε=0.1915 | Loss=525.7109/312.1880 | Time=1457s\n",
      "Episode  4900 | P1=0.830 P2=0.050 D=0.120 | Balance=0.0602 | Turns=91.3 | ε=0.1877 | Loss=516.2178/282.7973 | Time=1486s\n",
      "Episode  5000 | P1=0.750 P2=0.150 D=0.100 | Balance=0.2000 | Turns=96.6 | ε=0.1840 | Loss=568.5063/274.2003 | Time=1515s\n",
      "Episode  5100 | P1=0.800 P2=0.070 D=0.130 | Balance=0.0875 | Turns=106.6 | ε=0.1803 | Loss=632.1545/278.9406 | Time=1546s\n",
      "Episode  5200 | P1=0.630 P2=0.150 D=0.220 | Balance=0.2381 | Turns=111.0 | ε=0.1767 | Loss=605.2455/287.2635 | Time=1577s\n",
      "Episode  5300 | P1=0.670 P2=0.100 D=0.230 | Balance=0.1493 | Turns=113.7 | ε=0.1732 | Loss=512.5884/278.7499 | Time=1608s\n",
      "Episode  5400 | P1=0.710 P2=0.100 D=0.190 | Balance=0.1408 | Turns=110.9 | ε=0.1698 | Loss=463.9224/279.9076 | Time=1640s\n",
      "Episode  5500 | P1=0.770 P2=0.100 D=0.130 | Balance=0.1299 | Turns=106.8 | ε=0.1665 | Loss=448.4272/272.8262 | Time=1670s\n",
      "Episode  5600 | P1=0.730 P2=0.150 D=0.120 | Balance=0.2055 | Turns=105.3 | ε=0.1632 | Loss=476.4226/267.3409 | Time=1699s\n",
      "Episode  5700 | P1=0.600 P2=0.260 D=0.140 | Balance=0.4333 | Turns=101.8 | ε=0.1599 | Loss=448.7426/307.3555 | Time=1726s\n",
      "Episode  5800 | P1=0.900 P2=0.030 D=0.070 | Balance=0.0333 | Turns=92.6 | ε=0.1568 | Loss=418.4909/314.1517 | Time=1755s\n",
      "Episode  5900 | P1=0.700 P2=0.180 D=0.120 | Balance=0.2571 | Turns=100.2 | ε=0.1537 | Loss=374.5375/308.9412 | Time=1784s\n",
      "Episode  6000 | P1=0.600 P2=0.250 D=0.150 | Balance=0.4167 | Turns=92.8 | ε=0.1506 | Loss=398.4830/305.8276 | Time=1813s\n",
      "Episode  6100 | P1=0.900 P2=0.030 D=0.070 | Balance=0.0333 | Turns=93.6 | ε=0.1476 | Loss=399.3799/332.2400 | Time=1842s\n",
      "Episode  6200 | P1=0.830 P2=0.110 D=0.060 | Balance=0.1325 | Turns=85.3 | ε=0.1447 | Loss=428.2790/301.0291 | Time=1870s\n",
      "Episode  6300 | P1=0.720 P2=0.120 D=0.160 | Balance=0.1667 | Turns=92.9 | ε=0.1418 | Loss=473.5592/232.9936 | Time=1897s\n",
      "Episode  6400 | P1=0.760 P2=0.130 D=0.110 | Balance=0.1711 | Turns=96.9 | ε=0.1390 | Loss=485.7662/241.8851 | Time=1925s\n",
      "Episode  6500 | P1=0.880 P2=0.060 D=0.060 | Balance=0.0682 | Turns=66.0 | ε=0.1363 | Loss=470.0366/254.9864 | Time=1951s\n",
      "Episode  6600 | P1=0.760 P2=0.090 D=0.150 | Balance=0.1184 | Turns=74.6 | ε=0.1336 | Loss=438.7070/224.4858 | Time=1977s\n",
      "Episode  6700 | P1=0.760 P2=0.140 D=0.100 | Balance=0.1842 | Turns=85.2 | ε=0.1309 | Loss=499.5823/218.0954 | Time=2002s\n",
      "Episode  6800 | P1=0.740 P2=0.140 D=0.120 | Balance=0.1892 | Turns=86.1 | ε=0.1283 | Loss=586.2629/227.8721 | Time=2027s\n",
      "Episode  6900 | P1=0.550 P2=0.170 D=0.280 | Balance=0.3091 | Turns=110.5 | ε=0.1258 | Loss=630.7920/243.9481 | Time=2055s\n",
      "Episode  7000 | P1=0.680 P2=0.120 D=0.200 | Balance=0.1765 | Turns=112.7 | ε=0.1233 | Loss=541.0789/273.2455 | Time=2084s\n",
      "Episode  7100 | P1=0.620 P2=0.150 D=0.230 | Balance=0.2419 | Turns=108.2 | ε=0.1209 | Loss=459.0595/296.0592 | Time=2112s\n",
      "Episode  7200 | P1=0.440 P2=0.320 D=0.240 | Balance=0.7273 | Turns=106.0 | ε=0.1185 | Loss=430.0110/325.7952 | Time=2140s\n",
      "Episode  7300 | P1=0.620 P2=0.190 D=0.190 | Balance=0.3065 | Turns=98.2 | ε=0.1161 | Loss=406.5311/362.5242 | Time=2167s\n",
      "Episode  7400 | P1=0.570 P2=0.240 D=0.190 | Balance=0.4211 | Turns=101.9 | ε=0.1138 | Loss=390.6642/329.5376 | Time=2194s\n",
      "Episode  7500 | P1=0.380 P2=0.350 D=0.270 | Balance=0.9211 | Turns=112.8 | ε=0.1116 | Loss=370.6828/338.6355 | Time=2223s\n",
      "Episode  7600 | P1=0.490 P2=0.320 D=0.190 | Balance=0.6531 | Turns=113.2 | ε=0.1094 | Loss=374.0803/398.4359 | Time=2252s\n",
      "Episode  7700 | P1=0.660 P2=0.130 D=0.210 | Balance=0.1970 | Turns=111.2 | ε=0.1072 | Loss=371.5349/422.1767 | Time=2281s\n",
      "Episode  7800 | P1=0.480 P2=0.250 D=0.270 | Balance=0.5208 | Turns=121.4 | ε=0.1051 | Loss=360.1029/442.7446 | Time=2312s\n",
      "Episode  7900 | P1=0.700 P2=0.160 D=0.140 | Balance=0.2286 | Turns=95.3 | ε=0.1030 | Loss=334.5951/430.7141 | Time=2338s\n",
      "Episode  8000 | P1=0.450 P2=0.100 D=0.450 | Balance=0.2222 | Turns=135.4 | ε=0.1010 | Loss=330.3007/470.4318 | Time=2371s\n",
      "Episode  8100 | P1=0.630 P2=0.100 D=0.270 | Balance=0.1587 | Turns=106.8 | ε=0.0990 | Loss=367.5489/445.7218 | Time=2400s\n",
      "Episode  8200 | P1=0.770 P2=0.020 D=0.210 | Balance=0.0260 | Turns=85.0 | ε=0.0970 | Loss=405.6118/412.4987 | Time=2426s\n",
      "Episode  8300 | P1=0.750 P2=0.090 D=0.160 | Balance=0.1200 | Turns=90.5 | ε=0.0951 | Loss=412.0347/383.8191 | Time=2453s\n",
      "Episode  8400 | P1=0.650 P2=0.050 D=0.300 | Balance=0.0769 | Turns=111.9 | ε=0.0932 | Loss=415.9913/394.2260 | Time=2482s\n",
      "Episode  8500 | P1=0.390 P2=0.220 D=0.390 | Balance=0.5641 | Turns=127.6 | ε=0.0913 | Loss=394.9622/386.2768 | Time=2514s\n",
      "Episode  8600 | P1=0.580 P2=0.140 D=0.280 | Balance=0.2414 | Turns=113.9 | ε=0.0895 | Loss=408.0481/363.1026 | Time=2544s\n",
      "Episode  8700 | P1=0.560 P2=0.180 D=0.260 | Balance=0.3214 | Turns=127.4 | ε=0.0878 | Loss=366.0834/332.3520 | Time=2579s\n",
      "Episode  8800 | P1=0.560 P2=0.090 D=0.350 | Balance=0.1607 | Turns=120.2 | ε=0.0860 | Loss=358.1009/296.8960 | Time=2616s\n",
      "Episode  8900 | P1=0.700 P2=0.030 D=0.270 | Balance=0.0429 | Turns=103.2 | ε=0.0843 | Loss=342.7088/300.2738 | Time=2650s\n",
      "Episode  9000 | P1=0.550 P2=0.060 D=0.390 | Balance=0.1091 | Turns=124.9 | ε=0.0827 | Loss=359.7725/304.5362 | Time=2683s\n",
      "Episode  9100 | P1=0.190 P2=0.140 D=0.670 | Balance=0.7368 | Turns=158.8 | ε=0.0810 | Loss=319.0729/304.4303 | Time=2720s\n",
      "Episode  9200 | P1=0.580 P2=0.120 D=0.300 | Balance=0.2069 | Turns=81.3 | ε=0.0800 | Loss=333.9392/301.5027 | Time=2746s\n",
      "Episode  9300 | P1=0.430 P2=0.280 D=0.290 | Balance=0.6512 | Turns=106.7 | ε=0.0800 | Loss=356.0906/309.6422 | Time=2775s\n",
      "Episode  9400 | P1=0.580 P2=0.160 D=0.260 | Balance=0.2759 | Turns=109.0 | ε=0.0800 | Loss=375.1143/324.9014 | Time=2804s\n",
      "Episode  9500 | P1=0.520 P2=0.100 D=0.380 | Balance=0.1923 | Turns=125.5 | ε=0.0800 | Loss=302.6699/304.3823 | Time=2836s\n",
      "Episode  9600 | P1=0.470 P2=0.110 D=0.420 | Balance=0.2340 | Turns=116.1 | ε=0.0800 | Loss=285.9058/293.4227 | Time=2866s\n",
      "Episode  9700 | P1=0.680 P2=0.040 D=0.280 | Balance=0.0588 | Turns=99.7 | ε=0.0800 | Loss=314.2758/268.2908 | Time=2893s\n",
      "Episode  9800 | P1=0.610 P2=0.070 D=0.320 | Balance=0.1148 | Turns=109.0 | ε=0.0800 | Loss=292.8883/251.6879 | Time=2922s\n",
      "Episode  9900 | P1=0.530 P2=0.120 D=0.350 | Balance=0.2264 | Turns=109.5 | ε=0.0800 | Loss=302.4572/238.4948 | Time=2952s\n",
      "Episode 10000 | P1=0.620 P2=0.100 D=0.280 | Balance=0.1613 | Turns=109.7 | ε=0.0800 | Loss=298.9504/221.4625 | Time=2981s\n",
      "Episode 10100 | P1=0.490 P2=0.180 D=0.330 | Balance=0.3673 | Turns=113.4 | ε=0.0800 | Loss=294.5871/242.0606 | Time=3010s\n",
      "Episode 10200 | P1=0.620 P2=0.130 D=0.250 | Balance=0.2097 | Turns=102.4 | ε=0.0800 | Loss=288.2414/266.4143 | Time=3038s\n",
      "Episode 10300 | P1=0.620 P2=0.070 D=0.310 | Balance=0.1129 | Turns=113.7 | ε=0.0800 | Loss=273.9738/278.1195 | Time=3068s\n",
      "Episode 10400 | P1=0.580 P2=0.200 D=0.220 | Balance=0.3448 | Turns=110.5 | ε=0.0800 | Loss=260.5242/281.5225 | Time=3097s\n",
      "Episode 10500 | P1=0.540 P2=0.160 D=0.300 | Balance=0.2963 | Turns=111.4 | ε=0.0800 | Loss=227.8916/257.5013 | Time=3127s\n",
      "Episode 10600 | P1=0.550 P2=0.210 D=0.240 | Balance=0.3818 | Turns=99.8 | ε=0.0800 | Loss=226.4915/253.7593 | Time=3155s\n",
      "Episode 10700 | P1=0.630 P2=0.050 D=0.320 | Balance=0.0794 | Turns=98.0 | ε=0.0800 | Loss=278.4111/242.0193 | Time=3183s\n",
      "Episode 10800 | P1=0.350 P2=0.100 D=0.550 | Balance=0.2857 | Turns=134.6 | ε=0.0800 | Loss=294.1348/271.7439 | Time=3216s\n",
      "Episode 10900 | P1=0.170 P2=0.100 D=0.730 | Balance=0.5882 | Turns=177.3 | ε=0.0800 | Loss=263.6453/256.0552 | Time=3255s\n",
      "Episode 11000 | P1=0.280 P2=0.180 D=0.540 | Balance=0.6429 | Turns=141.3 | ε=0.0800 | Loss=224.5654/261.3697 | Time=3288s\n",
      "Episode 11100 | P1=0.500 P2=0.200 D=0.300 | Balance=0.4000 | Turns=115.9 | ε=0.0800 | Loss=222.2037/250.6971 | Time=3318s\n",
      "Episode 11200 | P1=0.620 P2=0.170 D=0.210 | Balance=0.2742 | Turns=128.7 | ε=0.0800 | Loss=235.1091/227.3287 | Time=3350s\n",
      "Episode 11300 | P1=0.470 P2=0.260 D=0.270 | Balance=0.5532 | Turns=126.3 | ε=0.0800 | Loss=223.7979/204.4275 | Time=3382s\n",
      "Episode 11400 | P1=0.450 P2=0.240 D=0.310 | Balance=0.5333 | Turns=132.4 | ε=0.0800 | Loss=199.1101/189.3758 | Time=3414s\n",
      "Episode 11500 | P1=0.370 P2=0.450 D=0.180 | Balance=0.8222 | Turns=106.8 | ε=0.0800 | Loss=187.7260/208.6077 | Time=3443s\n",
      "Episode 11600 | P1=0.060 P2=0.270 D=0.670 | Balance=0.2222 | Turns=153.9 | ε=0.0800 | Loss=175.9785/259.2891 | Time=3478s\n",
      "Episode 11700 | P1=0.280 P2=0.300 D=0.420 | Balance=0.9333 | Turns=141.5 | ε=0.0800 | Loss=171.5336/283.7819 | Time=3512s\n",
      "Episode 11800 | P1=0.550 P2=0.050 D=0.400 | Balance=0.0909 | Turns=133.4 | ε=0.0800 | Loss=160.7366/314.6775 | Time=3544s\n",
      "Episode 11900 | P1=0.330 P2=0.200 D=0.470 | Balance=0.6061 | Turns=135.4 | ε=0.0800 | Loss=186.5730/278.9390 | Time=3577s\n",
      "Episode 12000 | P1=0.340 P2=0.270 D=0.390 | Balance=0.7941 | Turns=126.6 | ε=0.0800 | Loss=208.0267/281.6587 | Time=3609s\n",
      "Episode 12100 | P1=0.420 P2=0.220 D=0.360 | Balance=0.5238 | Turns=121.4 | ε=0.0800 | Loss=219.0570/293.2469 | Time=3640s\n",
      "Episode 12200 | P1=0.410 P2=0.230 D=0.360 | Balance=0.5610 | Turns=128.6 | ε=0.0800 | Loss=208.1052/348.8121 | Time=3672s\n",
      "Episode 12300 | P1=0.350 P2=0.150 D=0.500 | Balance=0.4286 | Turns=156.5 | ε=0.0800 | Loss=186.2964/315.0135 | Time=3708s\n",
      "Episode 12400 | P1=0.430 P2=0.410 D=0.160 | Balance=0.9535 | Turns=116.1 | ε=0.0800 | Loss=180.8279/296.1389 | Time=3739s\n",
      "Episode 12500 | P1=0.180 P2=0.260 D=0.560 | Balance=0.6923 | Turns=160.3 | ε=0.0800 | Loss=185.7224/256.1189 | Time=3775s\n",
      "Episode 12600 | P1=0.580 P2=0.040 D=0.380 | Balance=0.0690 | Turns=119.1 | ε=0.0800 | Loss=210.5045/275.3876 | Time=3806s\n",
      "Episode 12700 | P1=0.410 P2=0.220 D=0.370 | Balance=0.5366 | Turns=124.6 | ε=0.0800 | Loss=230.6759/263.3101 | Time=3838s\n",
      "Episode 12800 | P1=0.150 P2=0.560 D=0.290 | Balance=0.2679 | Turns=113.4 | ε=0.0800 | Loss=247.7779/300.4092 | Time=3867s\n",
      "Episode 12900 | P1=0.600 P2=0.050 D=0.350 | Balance=0.0833 | Turns=114.2 | ε=0.0800 | Loss=250.7367/327.5404 | Time=3897s\n",
      "Episode 13000 | P1=0.600 P2=0.040 D=0.360 | Balance=0.0667 | Turns=115.5 | ε=0.0800 | Loss=246.1744/292.9104 | Time=3927s\n",
      "Episode 13100 | P1=0.690 P2=0.060 D=0.250 | Balance=0.0870 | Turns=99.0 | ε=0.0800 | Loss=271.9418/242.7885 | Time=3955s\n",
      "Episode 13200 | P1=0.390 P2=0.170 D=0.440 | Balance=0.4359 | Turns=129.2 | ε=0.0800 | Loss=320.0707/210.2983 | Time=3987s\n",
      "Episode 13300 | P1=0.600 P2=0.060 D=0.340 | Balance=0.1000 | Turns=115.2 | ε=0.0800 | Loss=332.6754/238.8662 | Time=4018s\n",
      "Episode 13400 | P1=0.240 P2=0.130 D=0.630 | Balance=0.5417 | Turns=142.9 | ε=0.0800 | Loss=292.1227/263.4283 | Time=4052s\n",
      "Episode 13500 | P1=0.260 P2=0.030 D=0.710 | Balance=0.1154 | Turns=163.9 | ε=0.0800 | Loss=238.8031/269.2437 | Time=4090s\n",
      "Episode 13600 | P1=0.530 P2=0.140 D=0.330 | Balance=0.2642 | Turns=115.4 | ε=0.0800 | Loss=223.4304/254.5167 | Time=4120s\n",
      "Episode 13700 | P1=0.220 P2=0.180 D=0.600 | Balance=0.8182 | Turns=150.5 | ε=0.0800 | Loss=256.1941/241.8096 | Time=4156s\n",
      "Episode 13800 | P1=0.310 P2=0.120 D=0.570 | Balance=0.3871 | Turns=151.7 | ε=0.0800 | Loss=238.4272/238.4680 | Time=4191s\n",
      "Episode 13900 | P1=0.610 P2=0.220 D=0.170 | Balance=0.3607 | Turns=92.2 | ε=0.0800 | Loss=194.6979/245.5611 | Time=4217s\n",
      "Episode 14000 | P1=0.300 P2=0.350 D=0.350 | Balance=0.8571 | Turns=117.5 | ε=0.0800 | Loss=200.9231/249.0570 | Time=4247s\n",
      "Episode 14100 | P1=0.160 P2=0.160 D=0.680 | Balance=1.0000 | Turns=163.5 | ε=0.0800 | Loss=205.6352/277.9587 | Time=4284s\n",
      "Episode 14200 | P1=0.730 P2=0.120 D=0.150 | Balance=0.1644 | Turns=96.2 | ε=0.0800 | Loss=166.3009/274.5987 | Time=4312s\n",
      "Episode 14300 | P1=0.860 P2=0.040 D=0.100 | Balance=0.0465 | Turns=71.3 | ε=0.0800 | Loss=186.6192/223.4864 | Time=4339s\n",
      "Episode 14400 | P1=0.350 P2=0.180 D=0.470 | Balance=0.5143 | Turns=125.6 | ε=0.0800 | Loss=231.0745/203.3016 | Time=4375s\n",
      "Episode 14500 | P1=0.260 P2=0.130 D=0.610 | Balance=0.5000 | Turns=151.0 | ε=0.0800 | Loss=230.8692/218.4137 | Time=4415s\n",
      "Episode 14600 | P1=0.330 P2=0.050 D=0.620 | Balance=0.1515 | Turns=148.4 | ε=0.0800 | Loss=228.6930/239.9400 | Time=4455s\n",
      "Episode 14700 | P1=0.200 P2=0.160 D=0.640 | Balance=0.8000 | Turns=159.0 | ε=0.0800 | Loss=201.2874/219.3980 | Time=4494s\n",
      "Episode 14800 | P1=0.460 P2=0.340 D=0.200 | Balance=0.7391 | Turns=105.0 | ε=0.0800 | Loss=191.4315/215.2381 | Time=4527s\n",
      "Episode 14900 | P1=0.470 P2=0.230 D=0.300 | Balance=0.4894 | Turns=125.1 | ε=0.0800 | Loss=169.6380/231.0699 | Time=4560s\n",
      "Episode 15000 | P1=0.160 P2=0.170 D=0.670 | Balance=0.9412 | Turns=164.8 | ε=0.0800 | Loss=169.4978/237.6980 | Time=4601s\n",
      "Episode 15100 | P1=0.540 P2=0.040 D=0.420 | Balance=0.0741 | Turns=129.8 | ε=0.0800 | Loss=171.7599/201.4089 | Time=4638s\n",
      "Episode 15200 | P1=0.490 P2=0.050 D=0.460 | Balance=0.1020 | Turns=143.1 | ε=0.0800 | Loss=184.0576/186.4913 | Time=4677s\n",
      "Episode 15300 | P1=0.440 P2=0.280 D=0.280 | Balance=0.6364 | Turns=111.1 | ε=0.0800 | Loss=218.4528/193.0050 | Time=4711s\n",
      "Episode 15400 | P1=0.180 P2=0.360 D=0.460 | Balance=0.5000 | Turns=139.1 | ε=0.0800 | Loss=217.6492/230.9813 | Time=4746s\n",
      "Episode 15500 | P1=0.490 P2=0.170 D=0.340 | Balance=0.3469 | Turns=107.1 | ε=0.0800 | Loss=183.8355/227.0125 | Time=4778s\n",
      "Episode 15600 | P1=0.520 P2=0.230 D=0.250 | Balance=0.4423 | Turns=106.4 | ε=0.0800 | Loss=227.4981/185.5869 | Time=4808s\n",
      "Episode 15700 | P1=0.320 P2=0.220 D=0.460 | Balance=0.6875 | Turns=125.2 | ε=0.0800 | Loss=235.0759/188.0107 | Time=4841s\n",
      "Episode 15800 | P1=0.570 P2=0.220 D=0.210 | Balance=0.3860 | Turns=89.3 | ε=0.0800 | Loss=279.6603/199.4941 | Time=4867s\n",
      "Episode 15900 | P1=0.510 P2=0.120 D=0.370 | Balance=0.2353 | Turns=120.4 | ε=0.0800 | Loss=251.1490/194.9154 | Time=4898s\n",
      "Episode 16000 | P1=0.410 P2=0.230 D=0.360 | Balance=0.5610 | Turns=109.9 | ε=0.0800 | Loss=219.9064/181.6289 | Time=4931s\n",
      "Episode 16100 | P1=0.430 P2=0.190 D=0.380 | Balance=0.4419 | Turns=121.0 | ε=0.0800 | Loss=208.3104/227.7351 | Time=4967s\n",
      "Episode 16200 | P1=0.460 P2=0.160 D=0.380 | Balance=0.3478 | Turns=127.7 | ε=0.0800 | Loss=240.1742/244.3640 | Time=5004s\n",
      "Episode 16300 | P1=0.410 P2=0.130 D=0.460 | Balance=0.3171 | Turns=127.5 | ε=0.0800 | Loss=218.0495/218.0469 | Time=5037s\n",
      "Episode 16400 | P1=0.670 P2=0.050 D=0.280 | Balance=0.0746 | Turns=93.4 | ε=0.0800 | Loss=209.1562/191.4666 | Time=5069s\n",
      "Episode 16500 | P1=0.290 P2=0.250 D=0.460 | Balance=0.8621 | Turns=135.5 | ε=0.0800 | Loss=246.5312/194.8606 | Time=5107s\n",
      "Episode 16600 | P1=0.430 P2=0.210 D=0.360 | Balance=0.4884 | Turns=124.3 | ε=0.0800 | Loss=232.4076/220.6081 | Time=5141s\n",
      "Episode 16700 | P1=0.540 P2=0.030 D=0.430 | Balance=0.0556 | Turns=121.5 | ε=0.0800 | Loss=218.3608/210.4603 | Time=5177s\n",
      "Episode 16800 | P1=0.370 P2=0.020 D=0.610 | Balance=0.0541 | Turns=146.7 | ε=0.0800 | Loss=200.1621/190.8722 | Time=5217s\n",
      "Episode 16900 | P1=0.430 P2=0.010 D=0.560 | Balance=0.0233 | Turns=139.0 | ε=0.0800 | Loss=220.1951/181.3041 | Time=5254s\n",
      "Episode 17000 | P1=0.230 P2=0.120 D=0.650 | Balance=0.5217 | Turns=162.5 | ε=0.0800 | Loss=194.6517/172.1057 | Time=5292s\n",
      "Episode 17100 | P1=0.530 P2=0.010 D=0.460 | Balance=0.0189 | Turns=132.1 | ε=0.0800 | Loss=192.2499/167.6677 | Time=5326s\n",
      "Episode 17200 | P1=0.310 P2=0.130 D=0.560 | Balance=0.4194 | Turns=139.6 | ε=0.0800 | Loss=183.3975/159.6719 | Time=5365s\n",
      "Episode 17300 | P1=0.450 P2=0.200 D=0.350 | Balance=0.4444 | Turns=112.8 | ε=0.0800 | Loss=189.7092/196.5381 | Time=5396s\n",
      "Episode 17400 | P1=0.530 P2=0.040 D=0.430 | Balance=0.0755 | Turns=126.6 | ε=0.0800 | Loss=218.1190/215.5429 | Time=5427s\n",
      "Episode 17500 | P1=0.480 P2=0.200 D=0.320 | Balance=0.4167 | Turns=128.6 | ε=0.0800 | Loss=210.4396/242.7329 | Time=5459s\n",
      "Episode 17600 | P1=0.500 P2=0.120 D=0.380 | Balance=0.2400 | Turns=123.6 | ε=0.0800 | Loss=194.7143/212.5739 | Time=5495s\n",
      "Episode 17700 | P1=0.420 P2=0.340 D=0.240 | Balance=0.8095 | Turns=93.5 | ε=0.0800 | Loss=208.0517/201.3223 | Time=5521s\n",
      "Episode 17800 | P1=0.330 P2=0.360 D=0.310 | Balance=0.9167 | Turns=97.8 | ε=0.0800 | Loss=207.8701/244.0807 | Time=5553s\n",
      "Episode 17900 | P1=0.630 P2=0.170 D=0.200 | Balance=0.2698 | Turns=87.2 | ε=0.0800 | Loss=228.2880/253.2752 | Time=5583s\n",
      "Episode 18000 | P1=0.300 P2=0.320 D=0.380 | Balance=0.9375 | Turns=104.9 | ε=0.0800 | Loss=233.7724/204.3596 | Time=5612s\n",
      "Episode 18100 | P1=0.700 P2=0.080 D=0.220 | Balance=0.1143 | Turns=82.7 | ε=0.0800 | Loss=265.7210/180.8959 | Time=5637s\n",
      "Episode 18200 | P1=0.280 P2=0.430 D=0.290 | Balance=0.6512 | Turns=103.4 | ε=0.0800 | Loss=242.2714/251.6451 | Time=5670s\n",
      "Episode 18300 | P1=0.180 P2=0.110 D=0.710 | Balance=0.6111 | Turns=155.1 | ε=0.0800 | Loss=209.3012/333.0512 | Time=5711s\n",
      "Episode 18400 | P1=0.670 P2=0.070 D=0.260 | Balance=0.1045 | Turns=93.7 | ε=0.0800 | Loss=173.9161/325.4315 | Time=5742s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m experiment = CQCNNExperiment(config)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# 実験実行\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m final_episode, training_time, analysis = \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m✅ 実験完了!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mCQCNNExperiment.run_experiment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# 5エピソードごとに学習\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_model(\u001b[38;5;28mself\u001b[39m.cqcnn_1, \u001b[38;5;28mself\u001b[39m.optimizer_1, \u001b[38;5;28mself\u001b[39m.replay_buffer_1, \u001b[38;5;28mself\u001b[39m.losses_1)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcqcnn_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_buffer_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlosses_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m episode_time = time.time() - episode_start\n\u001b[32m    192\u001b[39m \u001b[38;5;28mself\u001b[39m.episode_times.append(episode_time)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mCQCNNExperiment.train_model\u001b[39m\u001b[34m(self, model, optimizer, replay_buffer, losses_list)\u001b[39m\n\u001b[32m    154\u001b[39m dones = torch.BoolTensor([exp[\u001b[32m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# 現在のQ値\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m current_q_values = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m.gather(\u001b[32m1\u001b[39m, actions.unsqueeze(\u001b[32m1\u001b[39m)).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# ターゲットQ値（簡単なTD学習）\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mCQCNN.forward\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     93\u001b[39m quantum_outputs = []\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     q_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantum_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrontend_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantum_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     quantum_outputs.append(torch.stack(q_out))\n\u001b[32m     98\u001b[39m quantum_out = torch.stack(quantum_outputs)  \u001b[38;5;66;03m# (batch, 4)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:922\u001b[39m, in \u001b[36mQNode.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_capture_qnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m capture_qnode  \u001b[38;5;66;03m# pylint: disable=import-outside-toplevel\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m capture_qnode(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:895\u001b[39m, in \u001b[36mQNode._impl_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Calculate the classical jacobians if necessary\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._transform_program.set_classical_component(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m res = \u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    904\u001b[39m res = res[\u001b[32m0\u001b[39m]\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\execution.py:233\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(tapes, device, diff_method, interface, grad_on_execution, cache, cachesize, max_diff, device_vjp, postselect_mode, mcm_method, gradient_kwargs, transform_program, executor_backend)\u001b[39m\n\u001b[32m    229\u001b[39m tapes, outer_post_processing = outer_transform(tapes)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m outer_transform.is_informative, \u001b[33m\"\u001b[39m\u001b[33mshould only contain device preprocessing\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m results = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m user_post_processing(outer_post_processing(results))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\run.py:338\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(tapes, device, config, inner_transform_program)\u001b[39m\n\u001b[32m    335\u001b[39m         params = tape.get_parameters(trainable_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    336\u001b[39m         tape.trainable_params = qml.math.get_trainable_indices(params)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m results = \u001b[43mml_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\interfaces\\torch.py:240\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(tapes, execute_fn, jpc, device)\u001b[39m\n\u001b[32m    232\u001b[39m     parameters.extend(tape.get_parameters())\n\u001b[32m    234\u001b[39m kwargs = {\n\u001b[32m    235\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtapes\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(tapes),\n\u001b[32m    236\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexecute_fn\u001b[39m\u001b[33m\"\u001b[39m: execute_fn,\n\u001b[32m    237\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mjpc\u001b[39m\u001b[33m\"\u001b[39m: jpc,\n\u001b[32m    238\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExecuteTapes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\interfaces\\torch.py:89\u001b[39m, in \u001b[36mpytreeify.<locals>.new_apply\u001b[39m\u001b[34m(*inp)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_apply\u001b[39m(*inp):\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Inputs already flat\u001b[39;00m\n\u001b[32m     88\u001b[39m     out_struct_holder = []\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     flat_out = \u001b[43morig_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_struct_holder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pytree.tree_unflatten(flat_out, out_struct_holder[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\torch\\autograd\\function.py:576\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\interfaces\\torch.py:93\u001b[39m, in \u001b[36mpytreeify.<locals>.new_forward\u001b[39m\u001b[34m(ctx, out_struct_holder, *inp)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(ctx, out_struct_holder, *inp):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     out = \u001b[43morig_fw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     flat_out, out_struct = pytree.tree_flatten(out)\n\u001b[32m     95\u001b[39m     ctx._out_struct = out_struct\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\interfaces\\torch.py:162\u001b[39m, in \u001b[36mExecuteTapes.forward\u001b[39m\u001b[34m(ctx, kwargs, *parameters)\u001b[39m\n\u001b[32m    159\u001b[39m ctx.tapes = kwargs[\u001b[33m\"\u001b[39m\u001b[33mtapes\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    160\u001b[39m ctx.jpc = kwargs[\u001b[33m\"\u001b[39m\u001b[33mjpc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m res = \u001b[38;5;28mtuple\u001b[39m(\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexecute_fn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# if any input tensor uses the GPU, the output should as well\u001b[39;00m\n\u001b[32m    165\u001b[39m ctx.torch_device = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\jacobian_products.py:487\u001b[39m, in \u001b[36mDeviceDerivatives.execute_and_cache_jacobian\u001b[39m\u001b[34m(self, tapes)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logger.isEnabledFor(logging.DEBUG):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    486\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mForward pass called with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, tapes)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m results, jac = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dev_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._results_cache[tapes] = results\n\u001b[32m    489\u001b[39m \u001b[38;5;28mself\u001b[39m._jacs_cache[tapes] = jac\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\jacobian_products.py:451\u001b[39m, in \u001b[36mDeviceDerivatives._dev_execute_and_compute_derivatives\u001b[39m\u001b[34m(self, tapes)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_dev_execute_and_compute_derivatives\u001b[39m(\u001b[38;5;28mself\u001b[39m, tapes: QuantumScriptBatch):\n\u001b[32m    446\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[33;03m    Converts tapes to numpy before computing the the results and derivatives on the device.\u001b[39;00m\n\u001b[32m    448\u001b[39m \n\u001b[32m    449\u001b[39m \u001b[33;03m    Dispatches between the two different device interfaces.\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     numpy_tapes, _ = \u001b[43mqml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_numpy_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._device.execute_and_compute_derivatives(numpy_tapes, \u001b[38;5;28mself\u001b[39m._execution_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\core\\transform_dispatcher.py:281\u001b[39m, in \u001b[36mTransformDispatcher.__call__\u001b[39m\u001b[34m(self, *targs, **tkwargs)\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qfunc_transform(obj, targs, tkwargs)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(q, qml.tape.QuantumScript) \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m obj):\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# Input is not a QNode nor a quantum tape nor a device.\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Assume Python decorator syntax:\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# result = some_transform(*transform_args)(qnode)(*qnode_args)\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m TransformError(\n\u001b[32m    289\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDecorating a QNode with @transform_fn(**transform_kwargs) has been \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mremoved. Please decorate with @functools.partial(transform_fn, **transform_kwargs) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    293\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.pennylane.ai/en/stable/development/deprecations.html#completed-deprecation-cycles\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    294\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\core\\transform_dispatcher.py:489\u001b[39m, in \u001b[36mTransformDispatcher._batch_transform\u001b[39m\u001b[34m(self, original_batch, targs, tkwargs)\u001b[39m\n\u001b[32m    483\u001b[39m tape_counts = []\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m original_batch:\n\u001b[32m    486\u001b[39m     \u001b[38;5;66;03m# Preprocess the tapes by applying transforms\u001b[39;00m\n\u001b[32m    487\u001b[39m     \u001b[38;5;66;03m# to each tape, and storing corresponding tapes\u001b[39;00m\n\u001b[32m    488\u001b[39m     \u001b[38;5;66;03m# for execution, processing functions, and list of tape lengths.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     new_tapes, fn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m     execution_tapes.extend(new_tapes)\n\u001b[32m    491\u001b[39m     batch_fns.append(fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\core\\transform_dispatcher.py:253\u001b[39m, in \u001b[36mTransformDispatcher.__call__\u001b[39m\u001b[34m(self, *targs, **tkwargs)\u001b[39m\n\u001b[32m    250\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m expand_processing(processed_results)\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     transformed_tapes, processing_fn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_informative:\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m processing_fn(transformed_tapes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\convert_to_numpy_parameters.py:84\u001b[39m, in \u001b[36mconvert_to_numpy_parameters\u001b[39m\u001b[34m(tape)\u001b[39m\n\u001b[32m     82\u001b[39m new_ops = (_convert_op_to_numpy_data(op) \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m tape.operations)\n\u001b[32m     83\u001b[39m new_measurements = (_convert_measurement_to_numpy_data(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m tape.measurements)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m new_circuit = \u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_measurements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshots\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainable_params\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnull_postprocessing\u001b[39m(results):\n\u001b[32m     89\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"A postprocesing function returned by a transform that only converts the batch of results\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    into a result for a single ``QuantumTape``.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\tape\\qscript.py:194\u001b[39m, in \u001b[36mQuantumScript.__init__\u001b[39m\u001b[34m(self, ops, measurements, shots, trainable_params)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    189\u001b[39m     ops: Optional[Iterable[Operator]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m     trainable_params: Optional[Sequence[\u001b[38;5;28mint\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    193\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28mself\u001b[39m._ops = [] \u001b[38;5;28;01mif\u001b[39;00m ops \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m._measurements = [] \u001b[38;5;28;01mif\u001b[39;00m measurements \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(measurements)\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mself\u001b[39m._shots = Shots(shots)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\convert_to_numpy_parameters.py:82\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;129m@transform\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_numpy_parameters\u001b[39m(tape: QuantumScript) -> \u001b[38;5;28mtuple\u001b[39m[QuantumScriptBatch, PostprocessingFn]:\n\u001b[32m     49\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Transforms a circuit to one with purely numpy parameters.\u001b[39;00m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m \n\u001b[32m     81\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     new_ops = (\u001b[43m_convert_op_to_numpy_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m tape.operations)\n\u001b[32m     83\u001b[39m     new_measurements = (_convert_measurement_to_numpy_data(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m tape.measurements)\n\u001b[32m     84\u001b[39m     new_circuit = tape.\u001b[34m__class__\u001b[39m(\n\u001b[32m     85\u001b[39m         new_ops, new_measurements, shots=tape.shots, trainable_params=tape.trainable_params\n\u001b[32m     86\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\convert_to_numpy_parameters.py:30\u001b[39m, in \u001b[36m_convert_op_to_numpy_data\u001b[39m\u001b[34m(op)\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Use operator method to change parameters when it become available\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m qml.ops.functions.bind_new_parameters(op, \u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43munwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\math\\multi_dispatch.py:814\u001b[39m, in \u001b[36munwrap\u001b[39m\u001b[34m(values, max_depth)\u001b[39m\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_val.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_val, ndarray) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_val.shape \u001b[38;5;28;01melse\u001b[39;00m new_val\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    816\u001b[39m     np.to_numpy(values, max_depth=max_depth)\n\u001b[32m    817\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ArrayBox)\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m np.to_numpy(values)\n\u001b[32m    819\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\math\\multi_dispatch.py:814\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_val.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_val, ndarray) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_val.shape \u001b[38;5;28;01melse\u001b[39;00m new_val\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(values)(\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m values)\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    816\u001b[39m     np.to_numpy(values, max_depth=max_depth)\n\u001b[32m    817\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ArrayBox)\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m np.to_numpy(values)\n\u001b[32m    819\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\math\\multi_dispatch.py:809\u001b[39m, in \u001b[36munwrap.<locals>.convert\u001b[39m\u001b[34m(val)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unwrap(val)\n\u001b[32m    808\u001b[39m new_val = (\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m     np.to_numpy(val, max_depth=max_depth) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, ArrayBox) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    810\u001b[39m )\n\u001b[32m    811\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_val.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_val, ndarray) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_val.shape \u001b[38;5;28;01melse\u001b[39;00m new_val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\autoray\\autoray.py:81\u001b[39m, in \u001b[36mdo\u001b[39m\u001b[34m(fn, like, *args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m backend = _choose_backend(fn, args, kwargs, like=like)\n\u001b[32m     80\u001b[39m func = get_lib_fn(backend, fn)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\math\\single_dispatch.py:613\u001b[39m, in \u001b[36m_to_numpy_torch\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_to_numpy_torch\u001b[39m(x):\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mis_conj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_conj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    614\u001b[39m         \u001b[38;5;66;03m# The following line is only covered if using Torch <v1.10.0\u001b[39;00m\n\u001b[32m    615\u001b[39m         x = x.resolve_conj()\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x.detach().cpu().numpy()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === 実験実行 ===\n",
    "print(\"🚀 CQCNN量子強化学習実験を開始します\")\n",
    "print(f\"設定: {config_path}\")\n",
    "print(f\"最大エピソード: {MAX_EPISODES:,}\")\n",
    "print(f\"目標: Balance ≥ 0.95, 50回連続達成で収束\")\n",
    "\n",
    "# 実験インスタンス生成\n",
    "experiment = CQCNNExperiment(config)\n",
    "\n",
    "# 実験実行\n",
    "final_episode, training_time, analysis = experiment.run_experiment()\n",
    "\n",
    "print(\"\\n✅ 実験完了!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 結果分析と可視化 ===\n",
    "print(\"📊 実験結果を分析中...\")\n",
    "\n",
    "# データ準備\n",
    "episodes = [r['episode'] for r in experiment.game_results]\n",
    "winners = [r['winner'] for r in experiment.game_results]\n",
    "turns = [r['turns'] for r in experiment.game_results]\n",
    "\n",
    "# 勝率計算（移動平均）\n",
    "window = 100\n",
    "p1_wins = [1 if w == 1 else 0 for w in winners]\n",
    "p2_wins = [1 if w == 2 else 0 for w in winners]\n",
    "draws = [1 if w is None else 0 for w in winners]\n",
    "\n",
    "p1_rate = np.convolve(p1_wins, np.ones(window)/window, mode='valid')\n",
    "p2_rate = np.convolve(p2_wins, np.ones(window)/window, mode='valid')\n",
    "draw_rate = np.convolve(draws, np.ones(window)/window, mode='valid')\n",
    "episodes_smooth = np.array(episodes[window-1:])\n",
    "\n",
    "# バランス計算\n",
    "balance_history = []\n",
    "for i in range(window-1, len(winners)):\n",
    "    recent = winners[i-window+1:i+1]\n",
    "    w1 = sum(1 for w in recent if w == 1)\n",
    "    w2 = sum(1 for w in recent if w == 2)\n",
    "    balance = min(w1, w2) / max(w1, w2) if max(w1, w2) > 0 else 1.0\n",
    "    balance_history.append(balance)\n",
    "\n",
    "# 収束メトリクス\n",
    "convergence_episodes = [h['episode'] for h in experiment.convergence_detector.convergence_history]\n",
    "convergence_balance = [h['metrics']['balance'] for h in experiment.convergence_detector.convergence_history]\n",
    "consecutive_good = [h['metrics']['consecutive_good'] for h in experiment.convergence_detector.convergence_history]\n",
    "\n",
    "# 可視化\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "fig.suptitle(f'CQCNN量子強化学習実験結果 (Episodes: {final_episode:,})', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. 勝率推移\n",
    "axes[0,0].plot(episodes_smooth, p1_rate, label='Player 1', color='blue', linewidth=2)\n",
    "axes[0,0].plot(episodes_smooth, p2_rate, label='Player 2', color='red', linewidth=2)\n",
    "axes[0,0].plot(episodes_smooth, draw_rate, label='Draws', color='gray', linewidth=2)\n",
    "axes[0,0].axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0,0].set_title('Win Rate Trends')\n",
    "axes[0,0].set_xlabel('Episode')\n",
    "axes[0,0].set_ylabel('Win Rate')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. バランス推移\n",
    "axes[0,1].plot(episodes_smooth, balance_history, color='green', linewidth=2)\n",
    "axes[0,1].axhline(y=0.95, color='red', linestyle='--', label='Target (0.95)')\n",
    "axes[0,1].axhline(y=0.995, color='orange', linestyle='--', label='Ultra-strict (0.995)')\n",
    "axes[0,1].set_title('Balance Evolution')\n",
    "axes[0,1].set_xlabel('Episode')\n",
    "axes[0,1].set_ylabel('Balance')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].set_ylim([0, 1.05])\n",
    "\n",
    "# 3. 収束進行\n",
    "if convergence_episodes:\n",
    "    axes[0,2].plot(convergence_episodes, consecutive_good, color='purple', linewidth=2, marker='o', markersize=3)\n",
    "    axes[0,2].axhline(y=50, color='red', linestyle='--', label='Target (50)')\n",
    "    axes[0,2].set_title('Convergence Progress')\n",
    "    axes[0,2].set_xlabel('Episode')\n",
    "    axes[0,2].set_ylabel('Consecutive Good Checks')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ゲーム長推移\n",
    "turns_smooth = np.convolve(turns, np.ones(window)/window, mode='valid')\n",
    "axes[1,0].plot(episodes_smooth, turns_smooth, color='brown', linewidth=2)\n",
    "axes[1,0].set_title('Average Game Length')\n",
    "axes[1,0].set_xlabel('Episode')\n",
    "axes[1,0].set_ylabel('Turns per Game')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. ε減衰\n",
    "if experiment.epsilon_history:\n",
    "    axes[1,1].plot(experiment.epsilon_history, color='orange', linewidth=2)\n",
    "    axes[1,1].set_title('Epsilon Decay')\n",
    "    axes[1,1].set_xlabel('Episode')\n",
    "    axes[1,1].set_ylabel('Epsilon')\n",
    "    axes[1,1].set_yscale('log')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. 損失推移\n",
    "if experiment.losses_1 and experiment.losses_2:\n",
    "    loss_episodes_1 = np.linspace(0, final_episode, len(experiment.losses_1))\n",
    "    loss_episodes_2 = np.linspace(0, final_episode, len(experiment.losses_2))\n",
    "    axes[1,2].plot(loss_episodes_1, experiment.losses_1, alpha=0.7, color='blue', label='Player 1')\n",
    "    axes[1,2].plot(loss_episodes_2, experiment.losses_2, alpha=0.7, color='red', label='Player 2')\n",
    "    \n",
    "    # 移動平均\n",
    "    if len(experiment.losses_1) > 50:\n",
    "        loss1_smooth = np.convolve(experiment.losses_1, np.ones(50)/50, mode='valid')\n",
    "        loss2_smooth = np.convolve(experiment.losses_2, np.ones(50)/50, mode='valid')\n",
    "        axes[1,2].plot(loss_episodes_1[49:], loss1_smooth, color='darkblue', linewidth=2)\n",
    "        axes[1,2].plot(loss_episodes_2[49:], loss2_smooth, color='darkred', linewidth=2)\n",
    "    \n",
    "    axes[1,2].set_title('Training Loss')\n",
    "    axes[1,2].set_xlabel('Episode')\n",
    "    axes[1,2].set_ylabel('Loss')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. 最終結果分布\n",
    "final_1000 = winners[-1000:] if len(winners) >= 1000 else winners\n",
    "w1_final = sum(1 for w in final_1000 if w == 1)\n",
    "w2_final = sum(1 for w in final_1000 if w == 2)\n",
    "d_final = sum(1 for w in final_1000 if w is None)\n",
    "\n",
    "categories = ['Player 1', 'Player 2', 'Draws']\n",
    "values = [w1_final, w2_final, d_final]\n",
    "colors = ['blue', 'red', 'gray']\n",
    "axes[2,0].pie(values, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[2,0].set_title(f'Final Results (Last {len(final_1000)} games)')\n",
    "\n",
    "# 8. 量子パラメータ分布\n",
    "params1 = experiment.cqcnn_1.quantum_params.detach().numpy().flatten()\n",
    "params2 = experiment.cqcnn_2.quantum_params.detach().numpy().flatten()\n",
    "axes[2,1].hist(params1, bins=30, alpha=0.7, label='Player 1', color='blue', density=True)\n",
    "axes[2,1].hist(params2, bins=30, alpha=0.7, label='Player 2', color='red', density=True)\n",
    "axes[2,1].set_title('Quantum Parameter Distribution')\n",
    "axes[2,1].set_xlabel('Parameter Value')\n",
    "axes[2,1].set_ylabel('Density')\n",
    "axes[2,1].legend()\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 9. エピソード時間\n",
    "if experiment.episode_times:\n",
    "    time_smooth = np.convolve(experiment.episode_times, np.ones(min(100, len(experiment.episode_times)))//min(100, len(experiment.episode_times)), mode='valid')\n",
    "    time_episodes = range(len(time_smooth))\n",
    "    axes[2,2].plot(time_episodes, time_smooth, color='green', linewidth=2)\n",
    "    axes[2,2].set_title('Episode Time')\n",
    "    axes[2,2].set_xlabel('Episode')\n",
    "    axes[2,2].set_ylabel('Time (seconds)')\n",
    "    axes[2,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"可視化完了!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 詳細分析レポート ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"          CQCNN量子強化学習実験 - 最終レポート\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 基本統計\n",
    "total_games = len(experiment.game_results)\n",
    "total_p1_wins = sum(1 for r in experiment.game_results if r['winner'] == 1)\n",
    "total_p2_wins = sum(1 for r in experiment.game_results if r['winner'] == 2)\n",
    "total_draws = sum(1 for r in experiment.game_results if r['winner'] is None)\n",
    "\n",
    "print(f\"\\n📈 基本統計\")\n",
    "print(f\"  総エピソード数: {total_games:,}\")\n",
    "print(f\"  Player 1 勝利: {total_p1_wins:,} ({total_p1_wins/total_games*100:.1f}%)\")\n",
    "print(f\"  Player 2 勝利: {total_p2_wins:,} ({total_p2_wins/total_games*100:.1f}%)\")\n",
    "print(f\"  引き分け: {total_draws:,} ({total_draws/total_games*100:.1f}%)\")\n",
    "print(f\"  平均ゲーム長: {np.mean(turns):.1f} ターン\")\n",
    "print(f\"  実験時間: {training_time:.1f}秒 ({training_time/3600:.2f}時間)\")\n",
    "\n",
    "# 最終期間の詳細分析\n",
    "final_period = min(1000, total_games)\n",
    "final_results = experiment.game_results[-final_period:]\n",
    "final_p1 = sum(1 for r in final_results if r['winner'] == 1)\n",
    "final_p2 = sum(1 for r in final_results if r['winner'] == 2)\n",
    "final_draws = sum(1 for r in final_results if r['winner'] is None)\n",
    "final_balance = min(final_p1, final_p2) / max(final_p1, final_p2) if max(final_p1, final_p2) > 0 else 1.0\n",
    "\n",
    "print(f\"\\n🎯 最終期間分析 (直近{final_period}ゲーム)\")\n",
    "print(f\"  Player 1: {final_p1} ({final_p1/final_period*100:.1f}%)\")\n",
    "print(f\"  Player 2: {final_p2} ({final_p2/final_period*100:.1f}%)\")\n",
    "print(f\"  引き分け: {final_draws} ({final_draws/final_period*100:.1f}%)\")\n",
    "print(f\"  バランス: {final_balance:.4f}\")\n",
    "print(f\"  目標達成: {'✅ YES' if final_balance >= 0.95 else '❌ NO'} (目標: ≥0.95)\")\n",
    "\n",
    "# 収束分析\n",
    "print(f\"\\n🔄 収束分析\")\n",
    "if analysis['converged']:\n",
    "    print(f\"  ✅ 収束達成!\")\n",
    "    print(f\"  最終バランス: {analysis['balance']:.4f}\")\n",
    "    print(f\"  連続達成回数: {analysis['metrics']['consecutive_good']}\")\n",
    "else:\n",
    "    print(f\"  ❌ 収束未達成\")\n",
    "    print(f\"  現在バランス: {analysis['balance']:.4f}\")\n",
    "    print(f\"  連続達成回数: {analysis['metrics']['consecutive_good']}/50\")\n",
    "    print(f\"  ステータス: {analysis['metrics']['status']}\")\n",
    "\n",
    "# Ultra-strict実験との比較\n",
    "ultra_strict_balance = 1.000  # Ultra-strict実験の結果\n",
    "ultra_strict_episodes = 46400\n",
    "ultra_strict_draws = 1.0\n",
    "\n",
    "print(f\"\\n⚖️  Ultra-strict実験との比較\")\n",
    "print(f\"  設定      │ Ultra-strict │ 現在の実験\")\n",
    "print(f\"  ─────────┼─────────────┼──────────────\")\n",
    "print(f\"  閾値      │     0.995    │    0.95\")\n",
    "print(f\"  エピソード│   {ultra_strict_episodes:,}    │   {total_games:,}\")\n",
    "print(f\"  バランス  │   {ultra_strict_balance:.3f}    │   {final_balance:.3f}\")\n",
    "print(f\"  引き分け率│   {ultra_strict_draws*100:.1f}%     │   {final_draws/final_period*100:.1f}%\")\n",
    "print(f\"  収束      │     未達成    │   {'達成' if analysis['converged'] else '未達成'}\")\n",
    "\n",
    "# 量子効果の分析\n",
    "quantum_std_1 = np.std(params1)\n",
    "quantum_std_2 = np.std(params2)\n",
    "quantum_diff = np.mean(np.abs(params1 - params2))\n",
    "\n",
    "print(f\"\\n🌌 量子効果分析\")\n",
    "print(f\"  Player 1 量子パラメータ範囲: [{params1.min():.3f}, {params1.max():.3f}]\")\n",
    "print(f\"  Player 2 量子パラメータ範囲: [{params2.min():.3f}, {params2.max():.3f}]\")\n",
    "print(f\"  Player 1 標準偏差: {quantum_std_1:.3f}\")\n",
    "print(f\"  Player 2 標準偏差: {quantum_std_2:.3f}\")\n",
    "print(f\"  プレイヤー間差異: {quantum_diff:.3f}\")\n",
    "\n",
    "# 学習効率\n",
    "if experiment.losses_1 and experiment.losses_2:\n",
    "    initial_loss_1 = np.mean(experiment.losses_1[:50]) if len(experiment.losses_1) >= 50 else 0\n",
    "    final_loss_1 = np.mean(experiment.losses_1[-50:]) if len(experiment.losses_1) >= 50 else 0\n",
    "    initial_loss_2 = np.mean(experiment.losses_2[:50]) if len(experiment.losses_2) >= 50 else 0\n",
    "    final_loss_2 = np.mean(experiment.losses_2[-50:]) if len(experiment.losses_2) >= 50 else 0\n",
    "    \n",
    "    print(f\"\\n📚 学習効率\")\n",
    "    print(f\"  Player 1 損失: {initial_loss_1:.4f} → {final_loss_1:.4f} ({((final_loss_1-initial_loss_1)/initial_loss_1*100):+.1f}%)\")\n",
    "    print(f\"  Player 2 損失: {initial_loss_2:.4f} → {final_loss_2:.4f} ({((final_loss_2-initial_loss_2)/initial_loss_2*100):+.1f}%)\")\n",
    "    print(f\"  総学習ステップ: {len(experiment.losses_1) + len(experiment.losses_2):,}\")\n",
    "\n",
    "# 実験設定サマリー\n",
    "print(f\"\\n⚙️  実験設定\")\n",
    "print(f\"  量子構成: {N_QUBITS}Q{N_LAYERS}L\")\n",
    "print(f\"  状態次元: {STATE_DIM}\")\n",
    "print(f\"  行動空間: {ACTION_DIM}\")\n",
    "print(f\"  バッチサイズ: {BATCH_SIZE}\")\n",
    "print(f\"  学習率: {LEARNING_RATE}\")\n",
    "print(f\"  ε減衰: {EPSILON_START} → {EPSILON_MIN} (係数: {EPSILON_DECAY})\")\n",
    "print(f\"  バッファサイズ: {BUFFER_SIZE:,}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"実験完了! 結果は上記の通りです。\")\n",
    "print(f\"ノートブック保存推奨: このセルの結果を記録してください。\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qugeister_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
