{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  é‡å­ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AIé–‹ç™ºãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€PennyLaneã¨PyTorchã‚’ä½¿ã£ãŸé‡å­æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AIã®é–‹ç™ºæ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“š ç›®æ¬¡\n",
    "1. [ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](#1-ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—)\n",
    "2. [åŸºæœ¬çš„ãªé‡å­å›è·¯](#2-åŸºæœ¬çš„ãªé‡å­å›è·¯)\n",
    "3. [ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³](#3-ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³)\n",
    "4. [é‡å­AIã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#4-é‡å­aiã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)\n",
    "5. [å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹](#5-å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹)\n",
    "6. [æ€§èƒ½è©•ä¾¡ã¨èª¿æ•´](#6-æ€§èƒ½è©•ä¾¡ã¨èª¿æ•´)\n",
    "7. [é«˜åº¦ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º](#7-é«˜åº¦ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ \n",
    "sys.path.append(str(Path.cwd()))\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "print(\"ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(f\"ğŸ”¥ PyTorch: {torch.__version__}\")\n",
    "print(f\"âš›ï¸ PennyLane: {qml.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŸºæœ¬çš„ãªé‡å­å›è·¯\n",
    "\n",
    "é‡å­ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AIã®åŸºç›¤ã¨ãªã‚‹é‡å­å›è·¯ã‚’ç†è§£ã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6é‡å­ãƒ“ãƒƒãƒˆé‡å­ãƒ‡ãƒã‚¤ã‚¹ã‚’ä½œæˆ\n",
    "n_qubits = 6\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def simple_quantum_circuit(inputs):\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ³ãƒ—ãƒ«ãªé‡å­å›è·¯ã®ä¾‹\n",
    "    \n",
    "    Args:\n",
    "        inputs: å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ6æ¬¡å…ƒï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        é‡å­çŠ¶æ…‹ã®æœŸå¾…å€¤\n",
    "    \"\"\"\n",
    "    # å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    \n",
    "    # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒªãƒ³ã‚°å±¤\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    \n",
    "    # æ¸¬å®š\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "test_input = np.random.rand(6) * np.pi\n",
    "result = simple_quantum_circuit(test_input)\n",
    "print(f\"å…¥åŠ›: {test_input[:3]:.3f}...\")\n",
    "print(f\"å‡ºåŠ›: {result[:3]:.3f}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³\n",
    "\n",
    "ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã®åŸºæœ¬çš„ãªæ“ä½œã‚’ç†è§£ã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qugeister import GeisterEngine\n",
    "\n",
    "# ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–\n",
    "game = GeisterEngine()\n",
    "game.reset_game()\n",
    "\n",
    "print(\"ğŸ® ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ åˆæœŸåŒ–å®Œäº†\")\n",
    "print(f\"ç¾åœ¨ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼: {game.current_player}\")\n",
    "print(f\"ã‚²ãƒ¼ãƒ çŠ¶æ…‹: {'çµ‚äº†' if game.game_over else 'é€²è¡Œä¸­'}\")\n",
    "\n",
    "# åˆæ³•æ‰‹ã‚’å–å¾—\n",
    "legal_moves = game.get_legal_moves('A')\n",
    "print(f\"ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Aã®åˆæ³•æ‰‹æ•°: {len(legal_moves)}\")\n",
    "print(f\"æœ€åˆã®3æ‰‹: {legal_moves[:3]}\")\n",
    "\n",
    "# ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã®å¯è¦–åŒ–\n",
    "def visualize_board(game):\n",
    "    \"\"\"\n",
    "    ã‚²ãƒ¼ãƒ ãƒœãƒ¼ãƒ‰ã‚’ç°¡å˜ã«å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    board = game.get_board_state()\n",
    "    print(\"\\nğŸ“‹ ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ (6x6):\")\n",
    "    for row in board:\n",
    "        print(' '.join([f'{cell:2.0f}' for cell in row]))\n",
    "\n",
    "visualize_board(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. é‡å­AIã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£\n",
    "\n",
    "CNNé¢¨è¨­è¨ˆã®é‡å­AIã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’æ§‹ç¯‰ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¢å­˜ã®é‡å­AIã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from test_qubits_6 import QuantumBattleAI_6Qubits\n",
    "\n",
    "class CustomQuantumAI(nn.Module):\n",
    "    \"\"\"\n",
    "    ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªé‡å­AI\n",
    "    \n",
    "    ã“ã®ã‚¯ãƒ©ã‚¹ã§ã¯å­¦ç¿’ã®å„éƒ¨åˆ†ã‚’ç´°ã‹ãåˆ¶å¾¡ã§ãã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # é‡å­ãƒ‡ãƒã‚¤ã‚¹\n",
    "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "        \n",
    "        # å‰å‡¦ç†å±¤ï¼ˆCNNé¢¨ï¼‰\n",
    "        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # é‡å­å›è·¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(n_layers, n_qubits) * 0.1\n",
    "        )\n",
    "        \n",
    "        # å¾Œå‡¦ç†å±¤\n",
    "        self.fc1 = nn.Linear(n_qubits, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)  # Qå€¤å‡ºåŠ›\n",
    "        \n",
    "    def quantum_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªé‡å­å›è·¯\n",
    "        \"\"\"\n",
    "        # å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(inputs[i], wires=i)\n",
    "        \n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸå±¤\n",
    "        for layer in range(self.n_layers):\n",
    "            # å›è»¢ã‚²ãƒ¼ãƒˆ\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RY(params[layer, i], wires=i)\n",
    "            \n",
    "            # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒªãƒ³ã‚°\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            qml.CNOT(wires=[self.n_qubits - 1, 0])  # å¾ªç’°çµåˆ\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def forward(self, game_state):\n",
    "        \"\"\"\n",
    "        ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
    "        \"\"\"\n",
    "        batch_size = game_state.size(0)\n",
    "        \n",
    "        # CNNå‡¦ç†\n",
    "        x = torch.relu(self.conv1(game_state))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x).view(batch_size, -1)\n",
    "        \n",
    "        # é‡å­å‡¦ç†\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            quantum_input = torch.tanh(x[i]) * np.pi / 2\n",
    "            \n",
    "            # é‡å­å›è·¯ã‚’QNodeã¨ã—ã¦ä½œæˆ\n",
    "            @qml.qnode(self.dev)\n",
    "            def circuit(inputs, params):\n",
    "                return self.quantum_circuit(inputs, params)\n",
    "            \n",
    "            q_out = circuit(quantum_input, self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_tensor = torch.stack(quantum_outputs)\n",
    "        \n",
    "        # å¾Œå‡¦ç†\n",
    "        x = torch.relu(self.fc1(quantum_tensor))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.output(x)\n",
    "        \n",
    "        return q_value\n",
    "\n",
    "# ã‚«ã‚¹ã‚¿ãƒ AIã‚’ãƒ†ã‚¹ãƒˆ\n",
    "custom_ai = CustomQuantumAI(n_qubits=6, n_layers=3)\n",
    "print(f\"ğŸ¤– ã‚«ã‚¹ã‚¿ãƒ é‡å­AIä½œæˆå®Œäº†\")\n",
    "print(f\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in custom_ai.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹\n",
    "\n",
    "é‡å­AIã®å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹ã‚’ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ã§ç†è§£ã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_training_loop(model, episodes=100, batch_size=8):\n",
    "    \"\"\"\n",
    "    ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªå­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "    \n",
    "    ã“ã®é–¢æ•°ã§ã¯å­¦ç¿’ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è©³ç´°ã«åˆ¶å¾¡ã§ãã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    # å­¦ç¿’è¨­å®š\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ãƒ¼\n",
    "    replay_buffer = deque(maxlen=1000)\n",
    "    \n",
    "    # å­¦ç¿’çµ±è¨ˆ\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"ğŸ¯ å­¦ç¿’é–‹å§‹: {episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # ã‚²ãƒ¼ãƒ åˆæœŸåŒ–\n",
    "        game = GeisterEngine()\n",
    "        game.reset_game()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        max_steps = 50  # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·åˆ¶é™\n",
    "        \n",
    "        while not game.game_over and step_count < max_steps:\n",
    "            current_player = game.current_player\n",
    "            legal_moves = game.get_legal_moves(current_player)\n",
    "            \n",
    "            if not legal_moves:\n",
    "                break\n",
    "            \n",
    "            if current_player == 'A':  # AIã®æ‰‹ç•ª\n",
    "                # çŠ¶æ…‹ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "                state = game.get_board_state()\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # ãƒãƒ£ãƒ³ãƒãƒ«æ¬¡å…ƒè¿½åŠ \n",
    "                \n",
    "                # 7ãƒãƒ£ãƒ³ãƒãƒ«å½¢å¼ã«å¤‰æ›ï¼ˆç°¡ç•¥åŒ–ï¼‰\n",
    "                state_7ch = torch.zeros(1, 7, 6, 6)\n",
    "                state_7ch[0, 0] = state_tensor[0]  # åŸºæœ¬ãƒœãƒ¼ãƒ‰æƒ…å ±\n",
    "                \n",
    "                # Îµ-greedyè¡Œå‹•é¸æŠ\n",
    "                epsilon = max(0.01, 0.5 * (0.995 ** episode))\n",
    "                \n",
    "                if random.random() < epsilon:\n",
    "                    # ãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:\n",
    "                    # AIã«ã‚ˆã‚‹è¡Œå‹•é¸æŠï¼ˆç°¡ç•¥åŒ–ï¼‰\n",
    "                    action = random.choice(legal_moves)\n",
    "                \n",
    "                # è¡Œå‹•å®Ÿè¡Œ\n",
    "                game.make_move(current_player, action)\n",
    "                \n",
    "                # å ±é…¬è¨ˆç®—\n",
    "                if game.game_over:\n",
    "                    if game.winner == 'A':\n",
    "                        reward = 100  # å‹åˆ©\n",
    "                    elif game.winner == 'B':\n",
    "                        reward = -100  # æ•—åŒ—\n",
    "                    else:\n",
    "                        reward = 0  # å¼•ãåˆ†ã‘\n",
    "                else:\n",
    "                    reward = 1  # ç¶™ç¶šå ±é…¬\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "                # çµŒé¨“ã‚’ãƒãƒƒãƒ•ã‚¡ãƒ¼ã«ä¿å­˜\n",
    "                replay_buffer.append((state_7ch, action, reward))\n",
    "                \n",
    "            else:  # ç›¸æ‰‹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã®æ‰‹ç•ª\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "            \n",
    "            step_count += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # ãƒãƒƒãƒå­¦ç¿’\n",
    "        if len(replay_buffer) >= batch_size and episode % 10 == 0:\n",
    "            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            \n",
    "            # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
    "            states = torch.cat([item[0] for item in batch])\n",
    "            rewards = torch.FloatTensor([item[2] for item in batch])\n",
    "            \n",
    "            # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
    "            optimizer.zero_grad()\n",
    "            q_values = model(states).squeeze()\n",
    "            \n",
    "            # æå¤±è¨ˆç®—\n",
    "            loss = loss_fn(q_values, rewards)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # é€²æ—è¡¨ç¤º\n",
    "        if episode % 20 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-20:]) if episode_rewards else 0\n",
    "            avg_loss = np.mean(losses[-10:]) if losses else 0\n",
    "            print(f\"Episode {episode}: å¹³å‡å ±é…¬={avg_reward:.1f}, æå¤±={avg_loss:.4f}\")\n",
    "    \n",
    "    return episode_rewards, losses\n",
    "\n",
    "# çŸ­æœŸå­¦ç¿’ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ğŸ§ª çŸ­æœŸå­¦ç¿’ãƒ†ã‚¹ãƒˆé–‹å§‹...\")\n",
    "rewards, losses = custom_training_loop(custom_ai, episodes=50, batch_size=4)\n",
    "\n",
    "# çµæœå¯è¦–åŒ–\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards)\n",
    "plt.title('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬')\n",
    "plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')\n",
    "plt.ylabel('å ±é…¬')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if losses:\n",
    "    plt.plot(losses)\n",
    "    plt.title('å­¦ç¿’æå¤±')\n",
    "    plt.xlabel('æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—')\n",
    "    plt.ylabel('æå¤±')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… å­¦ç¿’å®Œäº†: å¹³å‡å ±é…¬ = {np.mean(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€§èƒ½è©•ä¾¡ã¨èª¿æ•´\n",
    "\n",
    "å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, n_games=10):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    losses = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for game_idx in range(n_games):\n",
    "            game = GeisterEngine()\n",
    "            game.reset_game()\n",
    "            \n",
    "            step_count = 0\n",
    "            max_steps = 50\n",
    "            \n",
    "            while not game.game_over and step_count < max_steps:\n",
    "                current_player = game.current_player\n",
    "                legal_moves = game.get_legal_moves(current_player)\n",
    "                \n",
    "                if not legal_moves:\n",
    "                    break\n",
    "                \n",
    "                # ä¸¡ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "                step_count += 1\n",
    "            \n",
    "            # çµæœé›†è¨ˆ\n",
    "            if game.winner == 'A':\n",
    "                wins += 1\n",
    "            elif game.winner == 'B':\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        'wins': wins,\n",
    "        'draws': draws, \n",
    "        'losses': losses,\n",
    "        'win_rate': wins / n_games * 100\n",
    "    }\n",
    "\n",
    "# æ€§èƒ½è©•ä¾¡\n",
    "results = evaluate_model(custom_ai, n_games=20)\n",
    "print(\"ğŸ“Š æ€§èƒ½è©•ä¾¡çµæœ:\")\n",
    "print(f\"å‹åˆ©: {results['wins']}/20 ({results['win_rate']:.1f}%)\")\n",
    "print(f\"å¼•åˆ†: {results['draws']}/20\")\n",
    "print(f\"æ•—åŒ—: {results['losses']}/20\")\n",
    "\n",
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®ãƒ’ãƒ³ãƒˆ\n",
    "print(\"\\nğŸ”§ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®ãƒ’ãƒ³ãƒˆ:\")\n",
    "print(\"1. å­¦ç¿’ç‡: 0.001-0.01 (é«˜ã™ãã‚‹ã¨ä¸å®‰å®šã€ä½ã™ãã‚‹ã¨é…ã„)\")\n",
    "print(\"2. ãƒãƒƒãƒã‚µã‚¤ã‚º: 8-32 (ãƒ¡ãƒ¢ãƒªã¨ã®å…¼ã­åˆã„)\")\n",
    "print(\"3. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: 1000-5000 (ååˆ†ãªå­¦ç¿’ã®ãŸã‚)\")\n",
    "print(\"4. Îµå€¤: 0.1-0.5 é–‹å§‹ã€0.01ã¾ã§æ¸›è¡° (æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹)\")\n",
    "print(\"5. é‡å­å›è·¯å±¤æ•°: 2-5å±¤ (è¡¨ç¾åŠ›ã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. é«˜åº¦ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
    "\n",
    "ã‚ˆã‚Šé«˜åº¦ãªé‡å­å›è·¯è¨­è¨ˆã‚„å­¦ç¿’æ‰‹æ³•ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜åº¦ãªé‡å­å›è·¯è¨­è¨ˆ\n",
    "class AdvancedQuantumCircuit:\n",
    "    \"\"\"\n",
    "    ã‚ˆã‚Šè¤‡é›‘ãªé‡å­å›è·¯è¨­è¨ˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    def amplitude_encoding_circuit(self, data, params):\n",
    "        \"\"\"\n",
    "        æŒ¯å¹…ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ãŸå›è·¯\n",
    "        \"\"\"\n",
    "        # æ­£è¦åŒ–\n",
    "        norm = torch.norm(data)\n",
    "        if norm > 0:\n",
    "            data = data / norm\n",
    "        \n",
    "        # æŒ¯å¹…ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆç°¡ç•¥ç‰ˆï¼‰\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(data[i] * np.pi, wires=i)\n",
    "        \n",
    "        # Strongly Entangling Layers\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(self.n_qubits))\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def variational_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        å¤‰åˆ†é‡å­å›è·¯\n",
    "        \"\"\"\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "            qml.RZ(inputs[i], wires=i)\n",
    "        \n",
    "        # å¤‰åˆ†å±¤\n",
    "        for layer in range(len(params)):\n",
    "            # å›è»¢ã‚²ãƒ¼ãƒˆ\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[layer][i][0], wires=i)\n",
    "                qml.RY(params[layer][i][1], wires=i)\n",
    "                qml.RZ(params[layer][i][2], wires=i)\n",
    "            \n",
    "            # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒªãƒ³ã‚°\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            if self.n_qubits > 2:\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "# é«˜åº¦ãªå­¦ç¿’æ‰‹æ³•\n",
    "def parameter_shift_training(model, data, target, lr=0.01):\n",
    "    \"\"\"\n",
    "    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚·ãƒ•ãƒˆæ³•ã«ã‚ˆã‚‹å‹¾é…è¨ˆç®—\n",
    "    \n",
    "    é‡å­å›è·¯ç‰¹æœ‰ã®å‹¾é…è¨ˆç®—æ‰‹æ³•\n",
    "    \"\"\"\n",
    "    gradients = []\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param_grad = torch.zeros_like(param)\n",
    "            \n",
    "            for i in range(param.numel()):\n",
    "                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’+Ï€/2ã‚·ãƒ•ãƒˆ\n",
    "                param.data.view(-1)[i] += np.pi / 2\n",
    "                output_plus = model(data)\n",
    "                \n",
    "                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’-Ï€/2ã‚·ãƒ•ãƒˆ\n",
    "                param.data.view(-1)[i] -= np.pi\n",
    "                output_minus = model(data)\n",
    "                \n",
    "                # å…ƒã«æˆ»ã™\n",
    "                param.data.view(-1)[i] += np.pi / 2\n",
    "                \n",
    "                # å‹¾é…è¨ˆç®—\n",
    "                grad = (output_plus - output_minus) / 2\n",
    "                param_grad.view(-1)[i] = grad.item()\n",
    "            \n",
    "            gradients.append(param_grad)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# é‡å­å›è·¯ã®å¯è¦–åŒ–\n",
    "def visualize_quantum_circuit():\n",
    "    \"\"\"\n",
    "    é‡å­å›è·¯ã®æ§‹é€ ã‚’å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    n_qubits = 4  # å¯è¦–åŒ–ç”¨ã«å°ã•ãã™ã‚‹\n",
    "    dev = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def demo_circuit(params):\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(params[i], wires=i)\n",
    "        \n",
    "        # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒªãƒ³ã‚°\n",
    "        for i in range(n_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        \n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "    \n",
    "    # å›è·¯å›³ã‚’ç”Ÿæˆ\n",
    "    params = np.random.rand(n_qubits)\n",
    "    print(\"ğŸ”§ é‡å­å›è·¯ã®æ§‹é€ :\")\n",
    "    print(qml.draw(demo_circuit)(params))\n",
    "\n",
    "visualize_quantum_circuit()\n",
    "\n",
    "print(\"\\nğŸ“ å­¦ç¿’ã®ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"1. é‡å­å›è·¯ã®æ·±ã•: æµ…ã„å›è·¯ã‹ã‚‰å§‹ã‚ã¦å¾ã€…ã«è¤‡é›‘ã«ã™ã‚‹\")\n",
    "print(\"2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ–¹æ³•: Angle, Amplitude, IQPãªã©æ§˜ã€…ãªæ‰‹æ³•ã‚’è©¦ã™\")\n",
    "print(\"3. æ¸¬å®šæ–¹æ³•: PauliZ, PauliX, PauliYã®çµ„ã¿åˆã‚ã›ã§æƒ…å ±ã‚’æŠ½å‡º\")\n",
    "print(\"4. ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒŠãƒ«å±¤: è¡¨ç¾åŠ›ã¨å­¦ç¿’å¯èƒ½æ€§ã®ãƒãƒ©ãƒ³ã‚¹\")\n",
    "print(\"5. ãƒã‚¤ã‚ºè€æ€§: å®Ÿæ©Ÿã§ã¯é‡å­ãƒã‚¤ã‚ºã‚’è€ƒæ…®ã—ãŸè¨­è¨ˆãŒé‡è¦\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å®Ÿè·µçš„ãªä½¿ã„æ–¹\n",
    "\n",
    "### åŸºæœ¬çš„ãªå­¦ç¿’ãƒ•ãƒ­ãƒ¼:\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿æº–å‚™**: ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã‚’7ãƒãƒ£ãƒ³ãƒãƒ«å½¢å¼ã§è¡¨ç¾\n",
    "2. **å›è·¯è¨­è¨ˆ**: å•é¡Œã«å¿œã˜ã¦é‡å­å›è·¯ã‚’è¨­è¨ˆ\n",
    "3. **å­¦ç¿’å®Ÿè¡Œ**: DQNã¾ãŸã¯ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§è¨“ç·´\n",
    "4. **è©•ä¾¡**: å¯¾æˆ¦ãƒ†ã‚¹ãƒˆã§æ€§èƒ½ã‚’ç¢ºèª\n",
    "5. **èª¿æ•´**: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–\n",
    "\n",
    "### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®æŒ‡é‡:\n",
    "\n",
    "- **å­¦ç¿’ãŒä¸å®‰å®š**: å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹\n",
    "- **å­¦ç¿’ãŒé…ã„**: å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã‚’å¢—ã‚„ã™\n",
    "- **éå­¦ç¿’**: æ­£å‰‡åŒ–ã‚’å¼·ã‚ã‚‹ã€å›è·¯ã‚’æµ…ãã™ã‚‹\n",
    "- **æ±åŒ–æ€§èƒ½ãŒä½ã„**: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆè¿½åŠ \n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
    "\n",
    "1. å®Ÿéš›ã®é‡å­ãƒ‡ãƒã‚¤ã‚¹ï¼ˆIBM Quantumã€Rigettiï¼‰ã§ã®å®Ÿè¡Œ\n",
    "2. ã‚ˆã‚Šè¤‡é›‘ãªã‚²ãƒ¼ãƒ ç’°å¢ƒã§ã®è©•ä¾¡\n",
    "3. ä»–ã®é‡å­æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã®æ¯”è¼ƒ\n",
    "4. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¤å…¸-é‡å­ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¢ç´¢"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}