{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 量子ガイスターAI開発チュートリアル\n",
    "\n",
    "このノートブックでは、PennyLaneとPyTorchを使った量子機械学習によるガイスターAIの開発方法を学びます。\n",
    "\n",
    "## 📚 目次\n",
    "1. [環境セットアップ](#1-環境セットアップ)\n",
    "2. [基本的な量子回路](#2-基本的な量子回路)\n",
    "3. [ガイスターゲームエンジン](#3-ガイスターゲームエンジン)\n",
    "4. [量子AIアーキテクチャ](#4-量子aiアーキテクチャ)\n",
    "5. [学習プロセス](#5-学習プロセス)\n",
    "6. [性能評価と調整](#6-性能評価と調整)\n",
    "7. [高度なカスタマイズ](#7-高度なカスタマイズ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境セットアップ\n",
    "\n",
    "必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 設定ファイル読み込み完了: quantum_geister_config_2025-09-23 (1).json\n",
      "📅 生成日時: 2025-09-23T07:48:01.440Z\n",
      "🧠 学習方法: reinforcement\n",
      "⚛️ 量子ビット数: 4\n",
      "📚 レイヤー数: 1\n",
      "\n",
      "==================================================\n",
      "🚀 WebUI連携設定完了\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== WebUI設定ファイル読み込み =====\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_latest_config():\n",
    "    \"\"\"\n",
    "    最新の設定ファイルを自動で読み込む\n",
    "    \"\"\"\n",
    "    config_files = glob.glob(\"quantum_geister_config_*.json\")\n",
    "    if not config_files:\n",
    "        print(\"❌ 設定ファイルが見つかりません。\")\n",
    "        print(\"💡 WebUIで設定を生成してから実行してください。\")\n",
    "        return None\n",
    "    \n",
    "    # 最新のファイルを取得\n",
    "    latest_file = max(config_files, key=os.path.getctime)\n",
    "    \n",
    "    try:\n",
    "        with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        print(f\"✅ 設定ファイル読み込み完了: {latest_file}\")\n",
    "        print(f\"📅 生成日時: {config['learning_config']['timestamp']}\")\n",
    "        print(f\"🧠 学習方法: {config['learning_config']['method']}\")\n",
    "        print(f\"⚛️ 量子ビット数: {config['module_config']['quantum']['n_qubits']}\")\n",
    "        print(f\"📚 レイヤー数: {config['module_config']['quantum']['n_layers']}\")\n",
    "        \n",
    "        return config\n",
    "    except Exception as e:\n",
    "        print(f\"❌ 設定ファイル読み込みエラー: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_default_config():\n",
    "    \"\"\"\n",
    "    デフォルト設定を作成（設定ファイルが無い場合）\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"learning_config\": {\n",
    "            \"method\": \"reinforcement\",\n",
    "            \"algorithm\": \"dqn\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"module_config\": {\n",
    "            \"quantum\": {\n",
    "                \"n_qubits\": 6,\n",
    "                \"n_layers\": 2,\n",
    "                \"embedding_type\": \"angle\",\n",
    "                \"entanglement\": \"linear\",\n",
    "                \"total_params\": 36\n",
    "            },\n",
    "            \"reward\": {\n",
    "                \"strategy\": \"balanced\"\n",
    "            },\n",
    "            \"qmap\": {\n",
    "                \"method\": \"dqn\",\n",
    "                \"state_dim\": 252,\n",
    "                \"action_dim\": 5\n",
    "            },\n",
    "            \"action_selection\": {\n",
    "                \"strategy\": \"epsilon\"\n",
    "            }\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 100,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# 設定ファイルを読み込み\n",
    "config = load_latest_config()\n",
    "if config is None:\n",
    "    print(\"🔧 デフォルト設定を使用します\")\n",
    "    config = create_default_config()\n",
    "\n",
    "# グローバル変数として設定\n",
    "learning_config = config['learning_config']\n",
    "module_config = config['module_config']\n",
    "hyperparameters = config['hyperparameters']\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🚀 WebUI連携設定完了\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# プロジェクトパスを追加\n",
    "sys.path.append(str(Path.cwd()))\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "print(\"📦 ライブラリ読み込み完了\")\n",
    "print(f\"🔥 PyTorch: {torch.__version__}\")\n",
    "print(f\"⚛️ PennyLane: {qml.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基本的な量子回路\n",
    "\n",
    "量子ガイスターAIの基盤となる量子回路を理解しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 6量子ビット量子デバイスを作成\u001b[39;00m\n\u001b[0;32m      2\u001b[0m n_qubits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m----> 3\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault.qubit\u001b[39m\u001b[38;5;124m'\u001b[39m, wires\u001b[38;5;241m=\u001b[39mn_qubits)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;129m@qml\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(dev)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimple_quantum_circuit\u001b[39m(inputs):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    シンプルな量子回路の例\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m        量子状態の期待値\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qml' is not defined"
     ]
    }
   ],
   "source": [
    "# 6量子ビット量子デバイスを作成\n",
    "n_qubits = 6\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def simple_quantum_circuit(inputs):\n",
    "    \"\"\"\n",
    "    シンプルな量子回路の例\n",
    "    \n",
    "    Args:\n",
    "        inputs: 入力パラメータ（6次元）\n",
    "    \n",
    "    Returns:\n",
    "        量子状態の期待値\n",
    "    \"\"\"\n",
    "    # 入力エンコーディング\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    \n",
    "    # エンタングリング層\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    \n",
    "    # 測定\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# テスト実行\n",
    "test_input = np.random.rand(6) * np.pi\n",
    "result = simple_quantum_circuit(test_input)\n",
    "print(f\"入力: {test_input[:3]:.3f}...\")\n",
    "print(f\"出力: {result[:3]:.3f}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ガイスターゲームエンジン\n",
    "\n",
    "ガイスターゲームの基本的な操作を理解しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qugeister'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqugeister\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GeisterEngine\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ゲームエンジンを初期化\u001b[39;00m\n\u001b[0;32m      4\u001b[0m game \u001b[38;5;241m=\u001b[39m GeisterEngine()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'qugeister'"
     ]
    }
   ],
   "source": [
    "from qugeister import GeisterEngine\n",
    "\n",
    "# ゲームエンジンを初期化\n",
    "game = GeisterEngine()\n",
    "game.reset_game()\n",
    "\n",
    "print(\"🎮 ガイスターゲーム初期化完了\")\n",
    "print(f\"現在のプレイヤー: {game.current_player}\")\n",
    "print(f\"ゲーム状態: {'終了' if game.game_over else '進行中'}\")\n",
    "\n",
    "# 合法手を取得\n",
    "legal_moves = game.get_legal_moves('A')\n",
    "print(f\"プレイヤーAの合法手数: {len(legal_moves)}\")\n",
    "print(f\"最初の3手: {legal_moves[:3]}\")\n",
    "\n",
    "# ボード状態の可視化\n",
    "def visualize_board(game):\n",
    "    \"\"\"\n",
    "    ゲームボードを簡単に可視化\n",
    "    \"\"\"\n",
    "    board = game.get_board_state()\n",
    "    print(\"\\n📋 ボード状態 (6x6):\")\n",
    "    for row in board:\n",
    "        print(' '.join([f'{cell:2.0f}' for cell in row]))\n",
    "\n",
    "visualize_board(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebUI設定に基づく動的量子AIクラス\n",
    "class WebUIQuantumAI(nn.Module):\n",
    "    \"\"\"\n",
    "    WebUIの設定に基づいて動的に構築される量子AI\n",
    "    \n",
    "    設定ファイルからパラメータを読み込んで自動構築します。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 設定から量子パラメータを取得\n",
    "        quantum_config = config['module_config']['quantum']\n",
    "        self.n_qubits = quantum_config['n_qubits']\n",
    "        self.n_layers = quantum_config['n_layers']\n",
    "        self.embedding_type = quantum_config['embedding_type']\n",
    "        self.entanglement = quantum_config['entanglement']\n",
    "        \n",
    "        # Q値マップ設定\n",
    "        qmap_config = config['module_config']['qmap']\n",
    "        self.action_dim = qmap_config['action_dim']\n",
    "        \n",
    "        print(f\"🏗️ 量子AI構築中...\")\n",
    "        print(f\"   量子ビット数: {self.n_qubits}\")\n",
    "        print(f\"   レイヤー数: {self.n_layers}\")\n",
    "        print(f\"   エンベディング: {self.embedding_type}\")\n",
    "        print(f\"   エンタングルメント: {self.entanglement}\")\n",
    "        print(f\"   行動次元: {self.action_dim}\")\n",
    "        \n",
    "        # 量子デバイス\n",
    "        self.dev = qml.device('default.qubit', wires=self.n_qubits)\n",
    "        \n",
    "        # 前処理層（CNN風）\n",
    "        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 量子回路パラメータ\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(self.n_layers, self.n_qubits, 3) * 0.1\n",
    "        )\n",
    "        \n",
    "        # 後処理層\n",
    "        self.fc1 = nn.Linear(self.n_qubits, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, self.action_dim)\n",
    "        \n",
    "        print(f\"✅ 量子AI構築完了 - パラメータ数: {sum(p.numel() for p in self.parameters())}\")\n",
    "    \n",
    "    def quantum_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        WebUI設定に基づく量子回路\n",
    "        \"\"\"\n",
    "        # データエンコーディング（設定に基づく）\n",
    "        for i in range(self.n_qubits):\n",
    "            if self.embedding_type == 'angle':\n",
    "                qml.RY(inputs[i], wires=i)\n",
    "            elif self.embedding_type == 'amplitude':\n",
    "                qml.RY(inputs[i] * np.pi / 2, wires=i)\n",
    "                qml.RZ(inputs[i] * np.pi / 4, wires=i)\n",
    "            else:  # iqp\n",
    "                qml.RX(inputs[i], wires=i)\n",
    "        \n",
    "        # パラメータ化された層\n",
    "        for layer in range(self.n_layers):\n",
    "            # 回転ゲート\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[layer, i, 0], wires=i)\n",
    "                qml.RY(params[layer, i, 1], wires=i)\n",
    "                qml.RZ(params[layer, i, 2], wires=i)\n",
    "            \n",
    "            # エンタングルメント（設定に基づく）\n",
    "            if self.entanglement == 'linear':\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "            elif self.entanglement == 'circular':\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])  # 循環結合\n",
    "            elif self.entanglement == 'full':\n",
    "                for i in range(self.n_qubits):\n",
    "                    for j in range(i+1, self.n_qubits):\n",
    "                        qml.CZ(wires=[i, j])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def forward(self, game_state):\n",
    "        \"\"\"\n",
    "        フォワードパス\n",
    "        \"\"\"\n",
    "        batch_size = game_state.size(0)\n",
    "        \n",
    "        # CNN処理\n",
    "        x = torch.relu(self.conv1(game_state))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x).view(batch_size, -1)\n",
    "        \n",
    "        # 量子処理\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            quantum_input = torch.tanh(x[i]) * np.pi / 2\n",
    "            \n",
    "            # 量子回路をQNodeとして作成\n",
    "            @qml.qnode(self.dev)\n",
    "            def circuit(inputs, params):\n",
    "                return self.quantum_circuit(inputs, params)\n",
    "            \n",
    "            q_out = circuit(quantum_input, self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_tensor = torch.stack(quantum_outputs)\n",
    "        \n",
    "        # 後処理\n",
    "        x = torch.relu(self.fc1(quantum_tensor))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_values = self.output(x)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "# WebUI設定に基づいてAIを作成\n",
    "webui_ai = WebUIQuantumAI(config)\n",
    "\n",
    "# 設定内容の表示\n",
    "print(f\"\\n📋 WebUI設定サマリー:\")\n",
    "print(f\"   学習方法: {learning_config['method']}\")\n",
    "print(f\"   アルゴリズム: {learning_config['algorithm']}\")\n",
    "print(f\"   報酬戦略: {module_config['reward']['strategy']}\")\n",
    "print(f\"   行動選択: {module_config['action_selection']['strategy']}\")\n",
    "print(f\"   バッチサイズ: {hyperparameters['batch_size']}\")\n",
    "print(f\"   学習率: {hyperparameters['learning_rate']}\")\n",
    "print(f\"   オプティマイザ: {hyperparameters['optimizer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既存の量子AIクラスをインポート\n",
    "from test_qubits_6 import QuantumBattleAI_6Qubits\n",
    "\n",
    "class CustomQuantumAI(nn.Module):\n",
    "    \"\"\"\n",
    "    カスタマイズ可能な量子AI\n",
    "    \n",
    "    このクラスでは学習の各部分を細かく制御できます。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # 量子デバイス\n",
    "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "        \n",
    "        # 前処理層（CNN風）\n",
    "        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 量子回路パラメータ\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(n_layers, n_qubits) * 0.1\n",
    "        )\n",
    "        \n",
    "        # 後処理層\n",
    "        self.fc1 = nn.Linear(n_qubits, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)  # Q値出力\n",
    "        \n",
    "    def quantum_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        カスタマイズ可能な量子回路\n",
    "        \"\"\"\n",
    "        # 入力エンコーディング\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(inputs[i], wires=i)\n",
    "        \n",
    "        # パラメータ化された層\n",
    "        for layer in range(self.n_layers):\n",
    "            # 回転ゲート\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RY(params[layer, i], wires=i)\n",
    "            \n",
    "            # エンタングリング\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            qml.CNOT(wires=[self.n_qubits - 1, 0])  # 循環結合\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def forward(self, game_state):\n",
    "        \"\"\"\n",
    "        フォワードパス\n",
    "        \"\"\"\n",
    "        batch_size = game_state.size(0)\n",
    "        \n",
    "        # CNN処理\n",
    "        x = torch.relu(self.conv1(game_state))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x).view(batch_size, -1)\n",
    "        \n",
    "        # 量子処理\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            quantum_input = torch.tanh(x[i]) * np.pi / 2\n",
    "            \n",
    "            # 量子回路をQNodeとして作成\n",
    "            @qml.qnode(self.dev)\n",
    "            def circuit(inputs, params):\n",
    "                return self.quantum_circuit(inputs, params)\n",
    "            \n",
    "            q_out = circuit(quantum_input, self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_tensor = torch.stack(quantum_outputs)\n",
    "        \n",
    "        # 後処理\n",
    "        x = torch.relu(self.fc1(quantum_tensor))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.output(x)\n",
    "        \n",
    "        return q_value\n",
    "\n",
    "# カスタムAIをテスト\n",
    "custom_ai = CustomQuantumAI(n_qubits=6, n_layers=3)\n",
    "print(f\"🤖 カスタム量子AI作成完了\")\n",
    "print(f\"パラメータ数: {sum(p.numel() for p in custom_ai.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def webui_training_loop(model, config, progress_callback=None):\n",
    "    \"\"\"\n",
    "    WebUI設定に基づく学習ループ\n",
    "    \n",
    "    設定ファイルからハイパーパラメータを読み込んで自動学習\n",
    "    \"\"\"\n",
    "    # ハイパーパラメータを設定から取得\n",
    "    hyperparams = config['hyperparameters']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    episodes = hyperparams['epochs']\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    optimizer_type = hyperparams['optimizer']\n",
    "    \n",
    "    # 報酬戦略を取得\n",
    "    reward_strategy = config['module_config']['reward']['strategy']\n",
    "    action_strategy = config['module_config']['action_selection']['strategy']\n",
    "    \n",
    "    print(f\"🎯 WebUI設定による学習開始\")\n",
    "    print(f\"   エピソード数: {episodes}\")\n",
    "    print(f\"   バッチサイズ: {batch_size}\")\n",
    "    print(f\"   学習率: {learning_rate}\")\n",
    "    print(f\"   オプティマイザ: {optimizer_type}\")\n",
    "    print(f\"   報酬戦略: {reward_strategy}\")\n",
    "    print(f\"   行動戦略: {action_strategy}\")\n",
    "    \n",
    "    # オプティマイザ設定\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    else:  # rmsprop\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # 経験リプレイバッファー\n",
    "    replay_buffer = deque(maxlen=10000)\n",
    "    \n",
    "    # 学習統計\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    convergence_data = {\n",
    "        'episodes': [],\n",
    "        'rewards': [],\n",
    "        'losses': [],\n",
    "        'win_rate': []\n",
    "    }\n",
    "    \n",
    "    # 報酬関数の設定\n",
    "    def calculate_reward(game, player, reward_strategy):\n",
    "        if game.game_over:\n",
    "            if game.winner == player:\n",
    "                if reward_strategy == 'escape':\n",
    "                    return 150  # 脱出戦略では勝利報酬を高く\n",
    "                elif reward_strategy == 'aggressive':\n",
    "                    return 120  # 攻撃戦略\n",
    "                else:\n",
    "                    return 100  # 標準勝利報酬\n",
    "            elif game.winner and game.winner != player:\n",
    "                if reward_strategy == 'defensive':\n",
    "                    return -80  # 守備戦略では敗北ペナルティを軽く\n",
    "                else:\n",
    "                    return -100\n",
    "            else:\n",
    "                return 0  # 引き分け\n",
    "        else:\n",
    "            if reward_strategy == 'balanced':\n",
    "                return 2  # バランス戦略では継続報酬を高く\n",
    "            else:\n",
    "                return 1\n",
    "    \n",
    "    # ε-greedy設定\n",
    "    epsilon_start = 0.9 if action_strategy == 'epsilon' else 0.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # ゲーム初期化\n",
    "        try:\n",
    "            from qugeister import GeisterEngine\n",
    "            game = GeisterEngine()\n",
    "            game.reset_game()\n",
    "        except ImportError:\n",
    "            # ゲームエンジンが無い場合のダミー処理\n",
    "            print(\"⚠️ ゲームエンジンが見つかりません。ダミー学習を実行します。\")\n",
    "            \n",
    "            # ダミーデータで学習\n",
    "            dummy_states = torch.randn(batch_size, 7, 6, 6)\n",
    "            dummy_targets = torch.randn(batch_size, model.action_dim)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(dummy_states)\n",
    "            loss = loss_fn(outputs, dummy_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            episode_rewards.append(random.uniform(-10, 10))\n",
    "            losses.append(loss.item())\n",
    "            continue\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        max_steps = 100\n",
    "        \n",
    "        while not game.game_over and step_count < max_steps:\n",
    "            current_player = game.current_player\n",
    "            legal_moves = game.get_legal_moves(current_player)\n",
    "            \n",
    "            if not legal_moves:\n",
    "                break\n",
    "            \n",
    "            if current_player == 'A':  # AIの手番\n",
    "                # 状態をエンコード\n",
    "                state = game.get_board_state()\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                \n",
    "                # 7チャンネル形式に変換\n",
    "                state_7ch = torch.zeros(1, 7, 6, 6)\n",
    "                state_7ch[0, 0] = state_tensor[0]\n",
    "                \n",
    "                # 行動選択（設定に基づく）\n",
    "                epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
    "                \n",
    "                if action_strategy == 'epsilon' and random.random() < epsilon:\n",
    "                    action = random.choice(legal_moves)\n",
    "                elif action_strategy == 'boltzmann':\n",
    "                    # Boltzmann探索（簡略版）\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:  # greedy or ucb\n",
    "                    action = random.choice(legal_moves)\n",
    "                \n",
    "                # 行動実行\n",
    "                game.make_move(current_player, action)\n",
    "                \n",
    "                # 報酬計算\n",
    "                reward = calculate_reward(game, current_player, reward_strategy)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # 経験をバッファーに保存\n",
    "                replay_buffer.append((state_7ch, action, reward))\n",
    "                \n",
    "            else:  # 相手（ランダム）の手番\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "            \n",
    "            step_count += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # バッチ学習\n",
    "        if len(replay_buffer) >= batch_size and episode % 5 == 0:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            \n",
    "            states = torch.cat([item[0] for item in batch])\n",
    "            rewards = torch.FloatTensor([item[2] for item in batch])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            q_values = model(states).mean(dim=1)  # 平均Q値\n",
    "            loss = loss_fn(q_values, rewards)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        # 統計収集\n",
    "        if episode % 20 == 0:\n",
    "            recent_rewards = episode_rewards[-20:] if episode_rewards else [0]\n",
    "            recent_losses = losses[-10:] if losses else [0]\n",
    "            \n",
    "            convergence_data['episodes'].append(episode)\n",
    "            convergence_data['rewards'].append(np.mean(recent_rewards))\n",
    "            convergence_data['losses'].append(np.mean(recent_losses) if recent_losses else 0)\n",
    "            convergence_data['win_rate'].append(\n",
    "                len([r for r in recent_rewards if r > 50]) / len(recent_rewards) * 100\n",
    "            )\n",
    "            \n",
    "            print(f\"Episode {episode}: 平均報酬={np.mean(recent_rewards):.1f}, \"\n",
    "                  f\"損失={np.mean(recent_losses):.4f}, ε={epsilon:.3f}\")\n",
    "            \n",
    "            # プログレスコールバック（WebUIとの連携用）\n",
    "            if progress_callback:\n",
    "                progress_callback({\n",
    "                    'episode': episode,\n",
    "                    'total_episodes': episodes,\n",
    "                    'avg_reward': np.mean(recent_rewards),\n",
    "                    'avg_loss': np.mean(recent_losses) if recent_losses else 0,\n",
    "                    'epsilon': epsilon,\n",
    "                    'win_rate': convergence_data['win_rate'][-1]\n",
    "                })\n",
    "    \n",
    "    print(f\"✅ 学習完了: 最終平均報酬 = {np.mean(episode_rewards[-20:]):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'losses': losses,\n",
    "        'convergence_data': convergence_data,\n",
    "        'final_performance': {\n",
    "            'avg_reward': np.mean(episode_rewards[-20:]) if episode_rewards else 0,\n",
    "            'total_episodes': episodes,\n",
    "            'final_win_rate': convergence_data['win_rate'][-1] if convergence_data['win_rate'] else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# WebUI設定による学習実行\n",
    "print(\"🧪 WebUI設定学習テスト開始...\")\n",
    "results = webui_training_loop(webui_ai, config)\n",
    "\n",
    "# 結果可視化\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results['rewards'])\n",
    "plt.title('エピソード報酬 (WebUI設定)')\n",
    "plt.xlabel('エピソード')\n",
    "plt.ylabel('報酬')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "if results['losses']:\n",
    "    plt.plot(results['losses'])\n",
    "    plt.title('学習損失')\n",
    "    plt.xlabel('更新ステップ')\n",
    "    plt.ylabel('損失')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "conv_data = results['convergence_data']\n",
    "if conv_data['episodes']:\n",
    "    plt.plot(conv_data['episodes'], conv_data['win_rate'], 'g-', label='勝率')\n",
    "    plt.plot(conv_data['episodes'], conv_data['rewards'], 'b-', label='平均報酬')\n",
    "    plt.title('収束分析')\n",
    "    plt.xlabel('エピソード')\n",
    "    plt.ylabel('値')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# 最終結果表示\n",
    "final_perf = results['final_performance']\n",
    "print(f\"\\n📊 最終性能結果:\")\n",
    "print(f\"   平均報酬: {final_perf['avg_reward']:.2f}\")\n",
    "print(f\"   勝率: {final_perf['final_win_rate']:.1f}%\")\n",
    "print(f\"   総エピソード: {final_perf['total_episodes']}\")\n",
    "\n",
    "# 設定保存（学習結果付き）\n",
    "config['training_results'] = results['final_performance']\n",
    "with open(f\"quantum_training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"💾 学習結果を保存しました\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_training_loop(model, episodes=100, batch_size=8):\n",
    "    \"\"\"\n",
    "    カスタマイズ可能な学習ループ\n",
    "    \n",
    "    この関数では学習の各ステップを詳細に制御できます。\n",
    "    \"\"\"\n",
    "    # 学習設定\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # 経験リプレイバッファー\n",
    "    replay_buffer = deque(maxlen=1000)\n",
    "    \n",
    "    # 学習統計\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"🎯 学習開始: {episodes}エピソード\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # ゲーム初期化\n",
    "        game = GeisterEngine()\n",
    "        game.reset_game()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        max_steps = 50  # エピソード長制限\n",
    "        \n",
    "        while not game.game_over and step_count < max_steps:\n",
    "            current_player = game.current_player\n",
    "            legal_moves = game.get_legal_moves(current_player)\n",
    "            \n",
    "            if not legal_moves:\n",
    "                break\n",
    "            \n",
    "            if current_player == 'A':  # AIの手番\n",
    "                # 状態をエンコード\n",
    "                state = game.get_board_state()\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # チャンネル次元追加\n",
    "                \n",
    "                # 7チャンネル形式に変換（簡略化）\n",
    "                state_7ch = torch.zeros(1, 7, 6, 6)\n",
    "                state_7ch[0, 0] = state_tensor[0]  # 基本ボード情報\n",
    "                \n",
    "                # ε-greedy行動選択\n",
    "                epsilon = max(0.01, 0.5 * (0.995 ** episode))\n",
    "                \n",
    "                if random.random() < epsilon:\n",
    "                    # ランダム行動\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:\n",
    "                    # AIによる行動選択（簡略化）\n",
    "                    action = random.choice(legal_moves)\n",
    "                \n",
    "                # 行動実行\n",
    "                game.make_move(current_player, action)\n",
    "                \n",
    "                # 報酬計算\n",
    "                if game.game_over:\n",
    "                    if game.winner == 'A':\n",
    "                        reward = 100  # 勝利\n",
    "                    elif game.winner == 'B':\n",
    "                        reward = -100  # 敗北\n",
    "                    else:\n",
    "                        reward = 0  # 引き分け\n",
    "                else:\n",
    "                    reward = 1  # 継続報酬\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "                # 経験をバッファーに保存\n",
    "                replay_buffer.append((state_7ch, action, reward))\n",
    "                \n",
    "            else:  # 相手（ランダム）の手番\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "            \n",
    "            step_count += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # バッチ学習\n",
    "        if len(replay_buffer) >= batch_size and episode % 10 == 0:\n",
    "            # ランダムサンプリング\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            \n",
    "            # バッチデータを準備\n",
    "            states = torch.cat([item[0] for item in batch])\n",
    "            rewards = torch.FloatTensor([item[2] for item in batch])\n",
    "            \n",
    "            # フォワードパス\n",
    "            optimizer.zero_grad()\n",
    "            q_values = model(states).squeeze()\n",
    "            \n",
    "            # 損失計算\n",
    "            loss = loss_fn(q_values, rewards)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 進捗表示\n",
    "        if episode % 20 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-20:]) if episode_rewards else 0\n",
    "            avg_loss = np.mean(losses[-10:]) if losses else 0\n",
    "            print(f\"Episode {episode}: 平均報酬={avg_reward:.1f}, 損失={avg_loss:.4f}\")\n",
    "    \n",
    "    return episode_rewards, losses\n",
    "\n",
    "# 短期学習テスト\n",
    "print(\"🧪 短期学習テスト開始...\")\n",
    "rewards, losses = custom_training_loop(custom_ai, episodes=50, batch_size=4)\n",
    "\n",
    "# 結果可視化\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards)\n",
    "plt.title('エピソード報酬')\n",
    "plt.xlabel('エピソード')\n",
    "plt.ylabel('報酬')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if losses:\n",
    "    plt.plot(losses)\n",
    "    plt.title('学習損失')\n",
    "    plt.xlabel('更新ステップ')\n",
    "    plt.ylabel('損失')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ 学習完了: 平均報酬 = {np.mean(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 性能評価と調整\n",
    "\n",
    "学習済みモデルの性能を評価し、ハイパーパラメータを調整します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, n_games=10):\n",
    "    \"\"\"\n",
    "    モデルの性能を評価\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    losses = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for game_idx in range(n_games):\n",
    "            game = GeisterEngine()\n",
    "            game.reset_game()\n",
    "            \n",
    "            step_count = 0\n",
    "            max_steps = 50\n",
    "            \n",
    "            while not game.game_over and step_count < max_steps:\n",
    "                current_player = game.current_player\n",
    "                legal_moves = game.get_legal_moves(current_player)\n",
    "                \n",
    "                if not legal_moves:\n",
    "                    break\n",
    "                \n",
    "                # 両プレイヤーともランダム（デモ用）\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "                step_count += 1\n",
    "            \n",
    "            # 結果集計\n",
    "            if game.winner == 'A':\n",
    "                wins += 1\n",
    "            elif game.winner == 'B':\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        'wins': wins,\n",
    "        'draws': draws, \n",
    "        'losses': losses,\n",
    "        'win_rate': wins / n_games * 100\n",
    "    }\n",
    "\n",
    "# 性能評価\n",
    "results = evaluate_model(custom_ai, n_games=20)\n",
    "print(\"📊 性能評価結果:\")\n",
    "print(f\"勝利: {results['wins']}/20 ({results['win_rate']:.1f}%)\")\n",
    "print(f\"引分: {results['draws']}/20\")\n",
    "print(f\"敗北: {results['losses']}/20\")\n",
    "\n",
    "# ハイパーパラメータ調整のヒント\n",
    "print(\"\\n🔧 ハイパーパラメータ調整のヒント:\")\n",
    "print(\"1. 学習率: 0.001-0.01 (高すぎると不安定、低すぎると遅い)\")\n",
    "print(\"2. バッチサイズ: 8-32 (メモリとの兼ね合い)\")\n",
    "print(\"3. エピソード数: 1000-5000 (十分な学習のため)\")\n",
    "print(\"4. ε値: 0.1-0.5 開始、0.01まで減衰 (探索と活用のバランス)\")\n",
    "print(\"5. 量子回路層数: 2-5層 (表現力と計算コストのトレードオフ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 高度なカスタマイズ\n",
    "\n",
    "より高度な量子回路設計や学習手法を試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高度な量子回路設計\n",
    "class AdvancedQuantumCircuit:\n",
    "    \"\"\"\n",
    "    より複雑な量子回路設計\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    def amplitude_encoding_circuit(self, data, params):\n",
    "        \"\"\"\n",
    "        振幅エンコーディングを使用した回路\n",
    "        \"\"\"\n",
    "        # 正規化\n",
    "        norm = torch.norm(data)\n",
    "        if norm > 0:\n",
    "            data = data / norm\n",
    "        \n",
    "        # 振幅エンコーディング（簡略版）\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(data[i] * np.pi, wires=i)\n",
    "        \n",
    "        # Strongly Entangling Layers\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(self.n_qubits))\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def variational_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        変分量子回路\n",
    "        \"\"\"\n",
    "        # データエンコーディング\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "            qml.RZ(inputs[i], wires=i)\n",
    "        \n",
    "        # 変分層\n",
    "        for layer in range(len(params)):\n",
    "            # 回転ゲート\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[layer][i][0], wires=i)\n",
    "                qml.RY(params[layer][i][1], wires=i)\n",
    "                qml.RZ(params[layer][i][2], wires=i)\n",
    "            \n",
    "            # エンタングリング\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            if self.n_qubits > 2:\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "# 高度な学習手法\n",
    "def parameter_shift_training(model, data, target, lr=0.01):\n",
    "    \"\"\"\n",
    "    パラメータシフト法による勾配計算\n",
    "    \n",
    "    量子回路特有の勾配計算手法\n",
    "    \"\"\"\n",
    "    gradients = []\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param_grad = torch.zeros_like(param)\n",
    "            \n",
    "            for i in range(param.numel()):\n",
    "                # パラメータを+π/2シフト\n",
    "                param.data.view(-1)[i] += np.pi / 2\n",
    "                output_plus = model(data)\n",
    "                \n",
    "                # パラメータを-π/2シフト\n",
    "                param.data.view(-1)[i] -= np.pi\n",
    "                output_minus = model(data)\n",
    "                \n",
    "                # 元に戻す\n",
    "                param.data.view(-1)[i] += np.pi / 2\n",
    "                \n",
    "                # 勾配計算\n",
    "                grad = (output_plus - output_minus) / 2\n",
    "                param_grad.view(-1)[i] = grad.item()\n",
    "            \n",
    "            gradients.append(param_grad)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# 量子回路の可視化\n",
    "def visualize_quantum_circuit():\n",
    "    \"\"\"\n",
    "    量子回路の構造を可視化\n",
    "    \"\"\"\n",
    "    n_qubits = 4  # 可視化用に小さくする\n",
    "    dev = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def demo_circuit(params):\n",
    "        # データエンコーディング\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(params[i], wires=i)\n",
    "        \n",
    "        # エンタングリング\n",
    "        for i in range(n_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        \n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "    \n",
    "    # 回路図を生成\n",
    "    params = np.random.rand(n_qubits)\n",
    "    print(\"🔧 量子回路の構造:\")\n",
    "    print(qml.draw(demo_circuit)(params))\n",
    "\n",
    "visualize_quantum_circuit()\n",
    "\n",
    "print(\"\\n🎓 学習のポイント:\")\n",
    "print(\"1. 量子回路の深さ: 浅い回路から始めて徐々に複雑にする\")\n",
    "print(\"2. エンコーディング方法: Angle, Amplitude, IQPなど様々な手法を試す\")\n",
    "print(\"3. 測定方法: PauliZ, PauliX, PauliYの組み合わせで情報を抽出\")\n",
    "print(\"4. バリエーショナル層: 表現力と学習可能性のバランス\")\n",
    "print(\"5. ノイズ耐性: 実機では量子ノイズを考慮した設計が重要\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== WebUIとの双方向連携機能 =====\n",
    "\n",
    "class WebUIBridge:\n",
    "    \"\"\"\n",
    "    WebUIとJupyter Notebookの橋渡しクラス\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.status_file = \"jupyter_status.json\"\n",
    "        self.results_file = \"jupyter_results.json\"\n",
    "    \n",
    "    def send_progress_to_webui(self, progress_data):\n",
    "        \"\"\"\n",
    "        学習進捗をWebUIに送信\n",
    "        \"\"\"\n",
    "        progress_data['timestamp'] = datetime.now().isoformat()\n",
    "        progress_data['status'] = 'training'\n",
    "        \n",
    "        try:\n",
    "            with open(self.status_file, 'w') as f:\n",
    "                json.dump(progress_data, f, indent=2)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 進捗送信エラー: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def send_results_to_webui(self, results, config):\n",
    "        \"\"\"\n",
    "        学習結果をWebUIに送信\n",
    "        \"\"\"\n",
    "        webui_results = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'status': 'completed',\n",
    "            'config_used': config,\n",
    "            'performance': results['final_performance'],\n",
    "            'training_data': {\n",
    "                'total_episodes': len(results['rewards']),\n",
    "                'final_avg_reward': results['final_performance']['avg_reward'],\n",
    "                'convergence_achieved': results['final_performance']['final_win_rate'] > 60,\n",
    "                'loss_trend': 'decreasing' if len(results['losses']) > 10 and \n",
    "                             results['losses'][-1] < results['losses'][5] else 'stable'\n",
    "            },\n",
    "            'recommendations': self._generate_recommendations(results)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(self.results_file, 'w') as f:\n",
    "                json.dump(webui_results, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"✅ 結果をWebUIに送信: {self.results_file}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 結果送信エラー: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _generate_recommendations(self, results):\n",
    "        \"\"\"\n",
    "        学習結果に基づく推奨設定を生成\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        avg_reward = results['final_performance']['avg_reward']\n",
    "        win_rate = results['final_performance']['final_win_rate']\n",
    "        \n",
    "        if avg_reward < 20:\n",
    "            recommendations.append({\n",
    "                'type': 'learning_rate',\n",
    "                'message': '学習率を上げることを推奨します (0.01-0.05)',\n",
    "                'priority': 'high'\n",
    "            })\n",
    "        \n",
    "        if win_rate < 40:\n",
    "            recommendations.append({\n",
    "                'type': 'exploration',\n",
    "                'message': 'ε値を調整して探索を増やすことを推奨します',\n",
    "                'priority': 'medium'\n",
    "            })\n",
    "        \n",
    "        if len(results['losses']) > 0 and results['losses'][-1] > results['losses'][0]:\n",
    "            recommendations.append({\n",
    "                'type': 'overfitting',\n",
    "                'message': '過学習の可能性があります。正則化を強化してください',\n",
    "                'priority': 'high'\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# WebUI連携インスタンス作成\n",
    "webui_bridge = WebUIBridge()\n",
    "\n",
    "# 進捗コールバック関数\n",
    "def progress_callback(progress_data):\n",
    "    \"\"\"\n",
    "    学習進捗をWebUIに送信するコールバック\n",
    "    \"\"\"\n",
    "    webui_bridge.send_progress_to_webui(progress_data)\n",
    "    \n",
    "    # Jupyter内でも表示\n",
    "    print(f\"📊 Progress: Episode {progress_data['episode']}/{progress_data['total_episodes']} \"\n",
    "          f\"- Reward: {progress_data['avg_reward']:.1f}, \"\n",
    "          f\"Win Rate: {progress_data['win_rate']:.1f}%\")\n",
    "\n",
    "print(\"🌉 WebUI連携ブリッジ準備完了\")\n",
    "\n",
    "# ===== 高度な学習とモニタリング =====\n",
    "\n",
    "def advanced_training_with_monitoring(model, config):\n",
    "    \"\"\"\n",
    "    高度な学習監視機能付きトレーニング\n",
    "    \"\"\"\n",
    "    print(\"🚀 高度な学習・監視システム開始\")\n",
    "    \n",
    "    # 学習前の初期状態をWebUIに送信\n",
    "    initial_status = {\n",
    "        'episode': 0,\n",
    "        'total_episodes': config['hyperparameters']['epochs'],\n",
    "        'avg_reward': 0.0,\n",
    "        'avg_loss': 0.0,\n",
    "        'epsilon': 0.9,\n",
    "        'win_rate': 0.0,\n",
    "        'phase': 'initialization'\n",
    "    }\n",
    "    webui_bridge.send_progress_to_webui(initial_status)\n",
    "    \n",
    "    # 学習実行（プログレスコールバック付き）\n",
    "    results = webui_training_loop(model, config, progress_callback)\n",
    "    \n",
    "    # 最終結果をWebUIに送信\n",
    "    webui_bridge.send_results_to_webui(results, config)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# 学習実行とモニタリング\n",
    "print(\"🎯 高度な学習システムでWebUI連携テスト...\")\n",
    "advanced_results = advanced_training_with_monitoring(webui_ai, config)\n",
    "\n",
    "# 詳細分析と可視化\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# 1. 学習曲線\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.plot(advanced_results['rewards'])\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. 損失曲線\n",
    "plt.subplot(2, 4, 2)\n",
    "if advanced_results['losses']:\n",
    "    plt.plot(advanced_results['losses'])\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. 移動平均報酬\n",
    "plt.subplot(2, 4, 3)\n",
    "if len(advanced_results['rewards']) > 20:\n",
    "    moving_avg = np.convolve(advanced_results['rewards'], np.ones(20)/20, mode='valid')\n",
    "    plt.plot(moving_avg)\n",
    "    plt.title('Moving Average Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Avg Reward (20 episodes)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. 勝率推移\n",
    "plt.subplot(2, 4, 4)\n",
    "conv_data = advanced_results['convergence_data']\n",
    "if conv_data['episodes']:\n",
    "    plt.plot(conv_data['episodes'], conv_data['win_rate'])\n",
    "    plt.title('Win Rate')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Win Rate (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. 報酬分布\n",
    "plt.subplot(2, 4, 5)\n",
    "plt.hist(advanced_results['rewards'], bins=30, alpha=0.7)\n",
    "plt.title('Reward Distribution')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. 学習率vs性能\n",
    "plt.subplot(2, 4, 6)\n",
    "episodes = range(len(advanced_results['rewards']))\n",
    "rewards = advanced_results['rewards']\n",
    "if episodes and rewards:\n",
    "    # 学習段階別の性能\n",
    "    early = np.mean(rewards[:len(rewards)//3]) if len(rewards) > 3 else 0\n",
    "    mid = np.mean(rewards[len(rewards)//3:2*len(rewards)//3]) if len(rewards) > 3 else 0\n",
    "    late = np.mean(rewards[2*len(rewards)//3:]) if len(rewards) > 3 else 0\n",
    "    \n",
    "    plt.bar(['Early', 'Mid', 'Late'], [early, mid, late])\n",
    "    plt.title('Performance by Phase')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. 収束分析\n",
    "plt.subplot(2, 4, 7)\n",
    "if conv_data['losses']:\n",
    "    plt.plot(conv_data['episodes'], conv_data['losses'], 'r-', label='Loss')\n",
    "    plt.plot(conv_data['episodes'], conv_data['rewards'], 'b-', label='Reward')\n",
    "    plt.title('Convergence Analysis')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. 最終統計\n",
    "plt.subplot(2, 4, 8)\n",
    "final_stats = [\n",
    "    advanced_results['final_performance']['avg_reward'],\n",
    "    advanced_results['final_performance']['final_win_rate'],\n",
    "    len(advanced_results['rewards']),\n",
    "    len(advanced_results['losses'])\n",
    "]\n",
    "labels = ['Avg Reward', 'Win Rate', 'Episodes', 'Updates']\n",
    "plt.bar(labels, final_stats)\n",
    "plt.title('Final Statistics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\n🎊 WebUI連携学習完了!\")\n",
    "print(f\"📈 最終性能: 平均報酬 {advanced_results['final_performance']['avg_reward']:.2f}\")\n",
    "print(f\"🏆 勝率: {advanced_results['final_performance']['final_win_rate']:.1f}%\")\n",
    "print(f\"💾 結果ファイル: {webui_bridge.results_file}\")\n",
    "print(f\"📊 ステータスファイル: {webui_bridge.status_file}\")\n",
    "\n",
    "# 推奨設定の表示\n",
    "if 'training_data' in advanced_results:\n",
    "    print(f\"\\n💡 学習状況診断:\")\n",
    "    print(f\"   収束状況: {'✅ 良好' if advanced_results['final_performance']['final_win_rate'] > 60 else '⚠️ 要改善'}\")\n",
    "    print(f\"   学習安定性: {'✅ 安定' if len(advanced_results['losses']) > 0 else '⚠️ 不安定'}\")\n",
    "    print(f\"   推奨次回実行: {'長期学習' if advanced_results['final_performance']['avg_reward'] > 30 else '設定調整'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 WebUIとの双方向連携\n",
    "\n",
    "WebUIで設定した内容がJupyter Notebookに自動反映され、学習結果をWebUIに送り返すことができます。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 実践的な使い方\n",
    "\n",
    "### 基本的な学習フロー:\n",
    "\n",
    "1. **データ準備**: ゲーム状態を7チャンネル形式で表現\n",
    "2. **回路設計**: 問題に応じて量子回路を設計\n",
    "3. **学習実行**: DQNまたはカスタム学習ループで訓練\n",
    "4. **評価**: 対戦テストで性能を確認\n",
    "5. **調整**: ハイパーパラメータを最適化\n",
    "\n",
    "### パラメータ調整の指針:\n",
    "\n",
    "- **学習が不安定**: 学習率を下げる、バッチサイズを小さくする\n",
    "- **学習が遅い**: 学習率を上げる、エピソード数を増やす\n",
    "- **過学習**: 正則化を強める、回路を浅くする\n",
    "- **汎化性能が低い**: データ拡張、ドロップアウト追加\n",
    "\n",
    "### 次のステップ:\n",
    "\n",
    "1. 実際の量子デバイス（IBM Quantum、Rigetti）での実行\n",
    "2. より複雑なゲーム環境での評価\n",
    "3. 他の量子機械学習アルゴリズムとの比較\n",
    "4. ハイブリッド古典-量子アーキテクチャの探索"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
