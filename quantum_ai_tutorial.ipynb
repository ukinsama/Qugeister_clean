{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 量子ガイスターAI開発チュートリアル\n",
    "\n",
    "このノートブックでは、PennyLaneとPyTorchを使った量子機械学習によるガイスターAIの開発方法を学びます。\n",
    "\n",
    "## 📚 目次\n",
    "1. [環境セットアップ](#1-環境セットアップ)\n",
    "2. [基本的な量子回路](#2-基本的な量子回路)\n",
    "3. [ガイスターゲームエンジン](#3-ガイスターゲームエンジン)\n",
    "4. [量子AIアーキテクチャ](#4-量子aiアーキテクチャ)\n",
    "5. [学習プロセス](#5-学習プロセス)\n",
    "6. [性能評価と調整](#6-性能評価と調整)\n",
    "7. [高度なカスタマイズ](#7-高度なカスタマイズ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 環境セットアップ\n",
    "\n",
    "必要なライブラリをインポートします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# プロジェクトパスを追加\n",
    "sys.path.append(str(Path.cwd()))\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "print(\"📦 ライブラリ読み込み完了\")\n",
    "print(f\"🔥 PyTorch: {torch.__version__}\")\n",
    "print(f\"⚛️ PennyLane: {qml.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. 基本的な量子回路\n",
    "\n",
    "量子ガイスターAIの基盤となる量子回路を理解しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6量子ビット量子デバイスを作成\n",
    "n_qubits = 6\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def simple_quantum_circuit(inputs):\n",
    "    \"\"\"\n",
    "    シンプルな量子回路の例\n",
    "    \n",
    "    Args:\n",
    "        inputs: 入力パラメータ（6次元）\n",
    "    \n",
    "    Returns:\n",
    "        量子状態の期待値\n",
    "    \"\"\"\n",
    "    # 入力エンコーディング\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    \n",
    "    # エンタングリング層\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    \n",
    "    # 測定\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# テスト実行\n",
    "test_input = np.random.rand(6) * np.pi\n",
    "result = simple_quantum_circuit(test_input)\n",
    "print(f\"入力: {test_input[:3]:.3f}...\")\n",
    "print(f\"出力: {result[:3]:.3f}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ガイスターゲームエンジン\n",
    "\n",
    "ガイスターゲームの基本的な操作を理解しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qugeister import GeisterEngine\n",
    "\n",
    "# ゲームエンジンを初期化\n",
    "game = GeisterEngine()\n",
    "game.reset_game()\n",
    "\n",
    "print(\"🎮 ガイスターゲーム初期化完了\")\n",
    "print(f\"現在のプレイヤー: {game.current_player}\")\n",
    "print(f\"ゲーム状態: {'終了' if game.game_over else '進行中'}\")\n",
    "\n",
    "# 合法手を取得\n",
    "legal_moves = game.get_legal_moves('A')\n",
    "print(f\"プレイヤーAの合法手数: {len(legal_moves)}\")\n",
    "print(f\"最初の3手: {legal_moves[:3]}\")\n",
    "\n",
    "# ボード状態の可視化\n",
    "def visualize_board(game):\n",
    "    \"\"\"\n",
    "    ゲームボードを簡単に可視化\n",
    "    \"\"\"\n",
    "    board = game.get_board_state()\n",
    "    print(\"\\n📋 ボード状態 (6x6):\")\n",
    "    for row in board:\n",
    "        print(' '.join([f'{cell:2.0f}' for cell in row]))\n",
    "\n",
    "visualize_board(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 量子AIアーキテクチャ\n",
    "\n",
    "CNN風設計の量子AIアーキテクチャを構築します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 既存の量子AIクラスをインポート\n",
    "from test_qubits_6 import QuantumBattleAI_6Qubits\n",
    "\n",
    "class CustomQuantumAI(nn.Module):\n",
    "    \"\"\"\n",
    "    カスタマイズ可能な量子AI\n",
    "    \n",
    "    このクラスでは学習の各部分を細かく制御できます。\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # 量子デバイス\n",
    "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "        \n",
    "        # 前処理層（CNN風）\n",
    "        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 量子回路パラメータ\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(n_layers, n_qubits) * 0.1\n",
    "        )\n",
    "        \n",
    "        # 後処理層\n",
    "        self.fc1 = nn.Linear(n_qubits, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)  # Q値出力\n",
    "        \n",
    "    def quantum_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        カスタマイズ可能な量子回路\n",
    "        \"\"\"\n",
    "        # 入力エンコーディング\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(inputs[i], wires=i)\n",
    "        \n",
    "        # パラメータ化された層\n",
    "        for layer in range(self.n_layers):\n",
    "            # 回転ゲート\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RY(params[layer, i], wires=i)\n",
    "            \n",
    "            # エンタングリング\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            qml.CNOT(wires=[self.n_qubits - 1, 0])  # 循環結合\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def forward(self, game_state):\n",
    "        \"\"\"\n",
    "        フォワードパス\n",
    "        \"\"\"\n",
    "        batch_size = game_state.size(0)\n",
    "        \n",
    "        # CNN処理\n",
    "        x = torch.relu(self.conv1(game_state))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x).view(batch_size, -1)\n",
    "        \n",
    "        # 量子処理\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            quantum_input = torch.tanh(x[i]) * np.pi / 2\n",
    "            \n",
    "            # 量子回路をQNodeとして作成\n",
    "            @qml.qnode(self.dev)\n",
    "            def circuit(inputs, params):\n",
    "                return self.quantum_circuit(inputs, params)\n",
    "            \n",
    "            q_out = circuit(quantum_input, self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_tensor = torch.stack(quantum_outputs)\n",
    "        \n",
    "        # 後処理\n",
    "        x = torch.relu(self.fc1(quantum_tensor))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.output(x)\n",
    "        \n",
    "        return q_value\n",
    "\n",
    "# カスタムAIをテスト\n",
    "custom_ai = CustomQuantumAI(n_qubits=6, n_layers=3)\n",
    "print(f\"🤖 カスタム量子AI作成完了\")\n",
    "print(f\"パラメータ数: {sum(p.numel() for p in custom_ai.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. 学習プロセス\n",
    "\n",
    "量子AIの学習プロセスをステップバイステップで理解しましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_training_loop(model, episodes=100, batch_size=8):\n",
    "    \"\"\"\n",
    "    カスタマイズ可能な学習ループ\n",
    "    \n",
    "    この関数では学習の各ステップを詳細に制御できます。\n",
    "    \"\"\"\n",
    "    # 学習設定\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # 経験リプレイバッファー\n",
    "    replay_buffer = deque(maxlen=1000)\n",
    "    \n",
    "    # 学習統計\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"🎯 学習開始: {episodes}エピソード\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # ゲーム初期化\n",
    "        game = GeisterEngine()\n",
    "        game.reset_game()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        max_steps = 50  # エピソード長制限\n",
    "        \n",
    "        while not game.game_over and step_count < max_steps:\n",
    "            current_player = game.current_player\n",
    "            legal_moves = game.get_legal_moves(current_player)\n",
    "            \n",
    "            if not legal_moves:\n",
    "                break\n",
    "            \n",
    "            if current_player == 'A':  # AIの手番\n",
    "                # 状態をエンコード\n",
    "                state = game.get_board_state()\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # チャンネル次元追加\n",
    "                \n",
    "                # 7チャンネル形式に変換（簡略化）\n",
    "                state_7ch = torch.zeros(1, 7, 6, 6)\n",
    "                state_7ch[0, 0] = state_tensor[0]  # 基本ボード情報\n",
    "                \n",
    "                # ε-greedy行動選択\n",
    "                epsilon = max(0.01, 0.5 * (0.995 ** episode))\n",
    "                \n",
    "                if random.random() < epsilon:\n",
    "                    # ランダム行動\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:\n",
    "                    # AIによる行動選択（簡略化）\n",
    "                    action = random.choice(legal_moves)\n",
    "                \n",
    "                # 行動実行\n",
    "                game.make_move(current_player, action)\n",
    "                \n",
    "                # 報酬計算\n",
    "                if game.game_over:\n",
    "                    if game.winner == 'A':\n",
    "                        reward = 100  # 勝利\n",
    "                    elif game.winner == 'B':\n",
    "                        reward = -100  # 敗北\n",
    "                    else:\n",
    "                        reward = 0  # 引き分け\n",
    "                else:\n",
    "                    reward = 1  # 継続報酬\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "                # 経験をバッファーに保存\n",
    "                replay_buffer.append((state_7ch, action, reward))\n",
    "                \n",
    "            else:  # 相手（ランダム）の手番\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "            \n",
    "            step_count += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # バッチ学習\n",
    "        if len(replay_buffer) >= batch_size and episode % 10 == 0:\n",
    "            # ランダムサンプリング\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            \n",
    "            # バッチデータを準備\n",
    "            states = torch.cat([item[0] for item in batch])\n",
    "            rewards = torch.FloatTensor([item[2] for item in batch])\n",
    "            \n",
    "            # フォワードパス\n",
    "            optimizer.zero_grad()\n",
    "            q_values = model(states).squeeze()\n",
    "            \n",
    "            # 損失計算\n",
    "            loss = loss_fn(q_values, rewards)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # バックプロパゲーション\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 進捗表示\n",
    "        if episode % 20 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-20:]) if episode_rewards else 0\n",
    "            avg_loss = np.mean(losses[-10:]) if losses else 0\n",
    "            print(f\"Episode {episode}: 平均報酬={avg_reward:.1f}, 損失={avg_loss:.4f}\")\n",
    "    \n",
    "    return episode_rewards, losses\n",
    "\n",
    "# 短期学習テスト\n",
    "print(\"🧪 短期学習テスト開始...\")\n",
    "rewards, losses = custom_training_loop(custom_ai, episodes=50, batch_size=4)\n",
    "\n",
    "# 結果可視化\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards)\n",
    "plt.title('エピソード報酬')\n",
    "plt.xlabel('エピソード')\n",
    "plt.ylabel('報酬')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if losses:\n",
    "    plt.plot(losses)\n",
    "    plt.title('学習損失')\n",
    "    plt.xlabel('更新ステップ')\n",
    "    plt.ylabel('損失')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"✅ 学習完了: 平均報酬 = {np.mean(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. 性能評価と調整\n",
    "\n",
    "学習済みモデルの性能を評価し、ハイパーパラメータを調整します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, n_games=10):\n",
    "    \"\"\"\n",
    "    モデルの性能を評価\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    losses = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for game_idx in range(n_games):\n",
    "            game = GeisterEngine()\n",
    "            game.reset_game()\n",
    "            \n",
    "            step_count = 0\n",
    "            max_steps = 50\n",
    "            \n",
    "            while not game.game_over and step_count < max_steps:\n",
    "                current_player = game.current_player\n",
    "                legal_moves = game.get_legal_moves(current_player)\n",
    "                \n",
    "                if not legal_moves:\n",
    "                    break\n",
    "                \n",
    "                # 両プレイヤーともランダム（デモ用）\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "                step_count += 1\n",
    "            \n",
    "            # 結果集計\n",
    "            if game.winner == 'A':\n",
    "                wins += 1\n",
    "            elif game.winner == 'B':\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        'wins': wins,\n",
    "        'draws': draws, \n",
    "        'losses': losses,\n",
    "        'win_rate': wins / n_games * 100\n",
    "    }\n",
    "\n",
    "# 性能評価\n",
    "results = evaluate_model(custom_ai, n_games=20)\n",
    "print(\"📊 性能評価結果:\")\n",
    "print(f\"勝利: {results['wins']}/20 ({results['win_rate']:.1f}%)\")\n",
    "print(f\"引分: {results['draws']}/20\")\n",
    "print(f\"敗北: {results['losses']}/20\")\n",
    "\n",
    "# ハイパーパラメータ調整のヒント\n",
    "print(\"\\n🔧 ハイパーパラメータ調整のヒント:\")\n",
    "print(\"1. 学習率: 0.001-0.01 (高すぎると不安定、低すぎると遅い)\")\n",
    "print(\"2. バッチサイズ: 8-32 (メモリとの兼ね合い)\")\n",
    "print(\"3. エピソード数: 1000-5000 (十分な学習のため)\")\n",
    "print(\"4. ε値: 0.1-0.5 開始、0.01まで減衰 (探索と活用のバランス)\")\n",
    "print(\"5. 量子回路層数: 2-5層 (表現力と計算コストのトレードオフ)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. 高度なカスタマイズ\n",
    "\n",
    "より高度な量子回路設計や学習手法を試してみましょう。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 高度な量子回路設計\n",
    "class AdvancedQuantumCircuit:\n",
    "    \"\"\"\n",
    "    より複雑な量子回路設計\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    def amplitude_encoding_circuit(self, data, params):\n",
    "        \"\"\"\n",
    "        振幅エンコーディングを使用した回路\n",
    "        \"\"\"\n",
    "        # 正規化\n",
    "        norm = torch.norm(data)\n",
    "        if norm > 0:\n",
    "            data = data / norm\n",
    "        \n",
    "        # 振幅エンコーディング（簡略版）\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(data[i] * np.pi, wires=i)\n",
    "        \n",
    "        # Strongly Entangling Layers\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(self.n_qubits))\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def variational_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        変分量子回路\n",
    "        \"\"\"\n",
    "        # データエンコーディング\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "            qml.RZ(inputs[i], wires=i)\n",
    "        \n",
    "        # 変分層\n",
    "        for layer in range(len(params)):\n",
    "            # 回転ゲート\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[layer][i][0], wires=i)\n",
    "                qml.RY(params[layer][i][1], wires=i)\n",
    "                qml.RZ(params[layer][i][2], wires=i)\n",
    "            \n",
    "            # エンタングリング\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            if self.n_qubits > 2:\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "# 高度な学習手法\n",
    "def parameter_shift_training(model, data, target, lr=0.01):\n",
    "    \"\"\"\n",
    "    パラメータシフト法による勾配計算\n",
    "    \n",
    "    量子回路特有の勾配計算手法\n",
    "    \"\"\"\n",
    "    gradients = []\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param_grad = torch.zeros_like(param)\n",
    "            \n",
    "            for i in range(param.numel()):\n",
    "                # パラメータを+π/2シフト\n",
    "                param.data.view(-1)[i] += np.pi / 2\n",
    "                output_plus = model(data)\n",
    "                \n",
    "                # パラメータを-π/2シフト\n",
    "                param.data.view(-1)[i] -= np.pi\n",
    "                output_minus = model(data)\n",
    "                \n",
    "                # 元に戻す\n",
    "                param.data.view(-1)[i] += np.pi / 2\n",
    "                \n",
    "                # 勾配計算\n",
    "                grad = (output_plus - output_minus) / 2\n",
    "                param_grad.view(-1)[i] = grad.item()\n",
    "            \n",
    "            gradients.append(param_grad)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# 量子回路の可視化\n",
    "def visualize_quantum_circuit():\n",
    "    \"\"\"\n",
    "    量子回路の構造を可視化\n",
    "    \"\"\"\n",
    "    n_qubits = 4  # 可視化用に小さくする\n",
    "    dev = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def demo_circuit(params):\n",
    "        # データエンコーディング\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(params[i], wires=i)\n",
    "        \n",
    "        # エンタングリング\n",
    "        for i in range(n_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        \n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "    \n",
    "    # 回路図を生成\n",
    "    params = np.random.rand(n_qubits)\n",
    "    print(\"🔧 量子回路の構造:\")\n",
    "    print(qml.draw(demo_circuit)(params))\n",
    "\n",
    "visualize_quantum_circuit()\n",
    "\n",
    "print(\"\\n🎓 学習のポイント:\")\n",
    "print(\"1. 量子回路の深さ: 浅い回路から始めて徐々に複雑にする\")\n",
    "print(\"2. エンコーディング方法: Angle, Amplitude, IQPなど様々な手法を試す\")\n",
    "print(\"3. 測定方法: PauliZ, PauliX, PauliYの組み合わせで情報を抽出\")\n",
    "print(\"4. バリエーショナル層: 表現力と学習可能性のバランス\")\n",
    "print(\"5. ノイズ耐性: 実機では量子ノイズを考慮した設計が重要\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 実践的な使い方\n",
    "\n",
    "### 基本的な学習フロー:\n",
    "\n",
    "1. **データ準備**: ゲーム状態を7チャンネル形式で表現\n",
    "2. **回路設計**: 問題に応じて量子回路を設計\n",
    "3. **学習実行**: DQNまたはカスタム学習ループで訓練\n",
    "4. **評価**: 対戦テストで性能を確認\n",
    "5. **調整**: ハイパーパラメータを最適化\n",
    "\n",
    "### パラメータ調整の指針:\n",
    "\n",
    "- **学習が不安定**: 学習率を下げる、バッチサイズを小さくする\n",
    "- **学習が遅い**: 学習率を上げる、エピソード数を増やす\n",
    "- **過学習**: 正則化を強める、回路を浅くする\n",
    "- **汎化性能が低い**: データ拡張、ドロップアウト追加\n",
    "\n",
    "### 次のステップ:\n",
    "\n",
    "1. 実際の量子デバイス（IBM Quantum、Rigetti）での実行\n",
    "2. より複雑なゲーム環境での評価\n",
    "3. 他の量子機械学習アルゴリズムとの比較\n",
    "4. ハイブリッド古典-量子アーキテクチャの探索"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}