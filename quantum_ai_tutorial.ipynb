{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  é‡å­ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AIé–‹ç™ºãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã§ã¯ã€PennyLaneã¨PyTorchã‚’ä½¿ã£ãŸé‡å­æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AIã®é–‹ç™ºæ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“š ç›®æ¬¡\n",
    "1. [ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—](#1-ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—)\n",
    "2. [åŸºæœ¬çš„ãªé‡å­å›è·¯](#2-åŸºæœ¬çš„ãªé‡å­å›è·¯)\n",
    "3. [ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³](#3-ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³)\n",
    "4. [é‡å­AIã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£](#4-é‡å­aiã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£)\n",
    "5. [å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹](#5-å­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹)\n",
    "6. [æ€§èƒ½è©•ä¾¡ã¨èª¿æ•´](#6-æ€§èƒ½è©•ä¾¡ã¨èª¿æ•´)\n",
    "7. [é«˜åº¦ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º](#7-é«˜åº¦ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: quantum_geister_config_2025-09-23 (1).json\n",
      "ğŸ“… ç”Ÿæˆæ—¥æ™‚: 2025-09-23T07:48:01.440Z\n",
      "ğŸ§  å­¦ç¿’æ–¹æ³•: reinforcement\n",
      "âš›ï¸ é‡å­ãƒ“ãƒƒãƒˆæ•°: 4\n",
      "ğŸ“š ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: 1\n",
      "\n",
      "==================================================\n",
      "ğŸš€ WebUIé€£æºè¨­å®šå®Œäº†\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# ===== WebUIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ =====\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def load_latest_config():\n",
    "    \"\"\"\n",
    "    æœ€æ–°ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ã§èª­ã¿è¾¼ã‚€\n",
    "    \"\"\"\n",
    "    config_files = glob.glob(\"quantum_geister_config_*.json\")\n",
    "    if not config_files:\n",
    "        print(\"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚\")\n",
    "        print(\"ğŸ’¡ WebUIã§è¨­å®šã‚’ç”Ÿæˆã—ã¦ã‹ã‚‰å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "        return None\n",
    "    \n",
    "    # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—\n",
    "    latest_file = max(config_files, key=os.path.getctime)\n",
    "    \n",
    "    try:\n",
    "        with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "            config = json.load(f)\n",
    "        \n",
    "        print(f\"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†: {latest_file}\")\n",
    "        print(f\"ğŸ“… ç”Ÿæˆæ—¥æ™‚: {config['learning_config']['timestamp']}\")\n",
    "        print(f\"ğŸ§  å­¦ç¿’æ–¹æ³•: {config['learning_config']['method']}\")\n",
    "        print(f\"âš›ï¸ é‡å­ãƒ“ãƒƒãƒˆæ•°: {config['module_config']['quantum']['n_qubits']}\")\n",
    "        print(f\"ğŸ“š ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {config['module_config']['quantum']['n_layers']}\")\n",
    "        \n",
    "        return config\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "        return None\n",
    "\n",
    "def create_default_config():\n",
    "    \"\"\"\n",
    "    ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½œæˆï¼ˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒç„¡ã„å ´åˆï¼‰\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"learning_config\": {\n",
    "            \"method\": \"reinforcement\",\n",
    "            \"algorithm\": \"dqn\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"module_config\": {\n",
    "            \"quantum\": {\n",
    "                \"n_qubits\": 6,\n",
    "                \"n_layers\": 2,\n",
    "                \"embedding_type\": \"angle\",\n",
    "                \"entanglement\": \"linear\",\n",
    "                \"total_params\": 36\n",
    "            },\n",
    "            \"reward\": {\n",
    "                \"strategy\": \"balanced\"\n",
    "            },\n",
    "            \"qmap\": {\n",
    "                \"method\": \"dqn\",\n",
    "                \"state_dim\": 252,\n",
    "                \"action_dim\": 5\n",
    "            },\n",
    "            \"action_selection\": {\n",
    "                \"strategy\": \"epsilon\"\n",
    "            }\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 100,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "config = load_latest_config()\n",
    "if config is None:\n",
    "    print(\"ğŸ”§ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™\")\n",
    "    config = create_default_config()\n",
    "\n",
    "# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ã—ã¦è¨­å®š\n",
    "learning_config = config['learning_config']\n",
    "module_config = config['module_config']\n",
    "hyperparameters = config['hyperparameters']\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸš€ WebUIé€£æºè¨­å®šå®Œäº†\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msys\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹ã‚’è¿½åŠ \n",
    "sys.path.append(str(Path.cwd()))\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "print(\"ğŸ“¦ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªèª­ã¿è¾¼ã¿å®Œäº†\")\n",
    "print(f\"ğŸ”¥ PyTorch: {torch.__version__}\")\n",
    "print(f\"âš›ï¸ PennyLane: {qml.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. åŸºæœ¬çš„ãªé‡å­å›è·¯\n",
    "\n",
    "é‡å­ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AIã®åŸºç›¤ã¨ãªã‚‹é‡å­å›è·¯ã‚’ç†è§£ã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'qml' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 6é‡å­ãƒ“ãƒƒãƒˆé‡å­ãƒ‡ãƒã‚¤ã‚¹ã‚’ä½œæˆ\u001b[39;00m\n\u001b[0;32m      2\u001b[0m n_qubits \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m\n\u001b[1;32m----> 3\u001b[0m dev \u001b[38;5;241m=\u001b[39m \u001b[43mqml\u001b[49m\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault.qubit\u001b[39m\u001b[38;5;124m'\u001b[39m, wires\u001b[38;5;241m=\u001b[39mn_qubits)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;129m@qml\u001b[39m\u001b[38;5;241m.\u001b[39mqnode(dev)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21msimple_quantum_circuit\u001b[39m(inputs):\n\u001b[0;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m    ã‚·ãƒ³ãƒ—ãƒ«ãªé‡å­å›è·¯ã®ä¾‹\u001b[39;00m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;124;03m        é‡å­çŠ¶æ…‹ã®æœŸå¾…å€¤\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'qml' is not defined"
     ]
    }
   ],
   "source": [
    "# 6é‡å­ãƒ“ãƒƒãƒˆé‡å­ãƒ‡ãƒã‚¤ã‚¹ã‚’ä½œæˆ\n",
    "n_qubits = 6\n",
    "dev = qml.device('default.qubit', wires=n_qubits)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def simple_quantum_circuit(inputs):\n",
    "    \"\"\"\n",
    "    ã‚·ãƒ³ãƒ—ãƒ«ãªé‡å­å›è·¯ã®ä¾‹\n",
    "    \n",
    "    Args:\n",
    "        inputs: å…¥åŠ›ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ï¼ˆ6æ¬¡å…ƒï¼‰\n",
    "    \n",
    "    Returns:\n",
    "        é‡å­çŠ¶æ…‹ã®æœŸå¾…å€¤\n",
    "    \"\"\"\n",
    "    # å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "    for i in range(n_qubits):\n",
    "        qml.RY(inputs[i], wires=i)\n",
    "    \n",
    "    # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒªãƒ³ã‚°å±¤\n",
    "    for i in range(n_qubits - 1):\n",
    "        qml.CNOT(wires=[i, i + 1])\n",
    "    \n",
    "    # æ¸¬å®š\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_qubits)]\n",
    "\n",
    "# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ\n",
    "test_input = np.random.rand(6) * np.pi\n",
    "result = simple_quantum_circuit(test_input)\n",
    "print(f\"å…¥åŠ›: {test_input[:3]:.3f}...\")\n",
    "print(f\"å‡ºåŠ›: {result[:3]:.3f}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³\n",
    "\n",
    "ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ã®åŸºæœ¬çš„ãªæ“ä½œã‚’ç†è§£ã—ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'qugeister'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mqugeister\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m GeisterEngine\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–\u001b[39;00m\n\u001b[0;32m      4\u001b[0m game \u001b[38;5;241m=\u001b[39m GeisterEngine()\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'qugeister'"
     ]
    }
   ],
   "source": [
    "from qugeister import GeisterEngine\n",
    "\n",
    "# ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã‚’åˆæœŸåŒ–\n",
    "game = GeisterEngine()\n",
    "game.reset_game()\n",
    "\n",
    "print(\"ğŸ® ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ åˆæœŸåŒ–å®Œäº†\")\n",
    "print(f\"ç¾åœ¨ã®ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼: {game.current_player}\")\n",
    "print(f\"ã‚²ãƒ¼ãƒ çŠ¶æ…‹: {'çµ‚äº†' if game.game_over else 'é€²è¡Œä¸­'}\")\n",
    "\n",
    "# åˆæ³•æ‰‹ã‚’å–å¾—\n",
    "legal_moves = game.get_legal_moves('A')\n",
    "print(f\"ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼Aã®åˆæ³•æ‰‹æ•°: {len(legal_moves)}\")\n",
    "print(f\"æœ€åˆã®3æ‰‹: {legal_moves[:3]}\")\n",
    "\n",
    "# ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ã®å¯è¦–åŒ–\n",
    "def visualize_board(game):\n",
    "    \"\"\"\n",
    "    ã‚²ãƒ¼ãƒ ãƒœãƒ¼ãƒ‰ã‚’ç°¡å˜ã«å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    board = game.get_board_state()\n",
    "    print(\"\\nğŸ“‹ ãƒœãƒ¼ãƒ‰çŠ¶æ…‹ (6x6):\")\n",
    "    for row in board:\n",
    "        print(' '.join([f'{cell:2.0f}' for cell in row]))\n",
    "\n",
    "visualize_board(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# WebUIè¨­å®šã«åŸºã¥ãå‹•çš„é‡å­AIã‚¯ãƒ©ã‚¹\n",
    "class WebUIQuantumAI(nn.Module):\n",
    "    \"\"\"\n",
    "    WebUIã®è¨­å®šã«åŸºã¥ã„ã¦å‹•çš„ã«æ§‹ç¯‰ã•ã‚Œã‚‹é‡å­AI\n",
    "    \n",
    "    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§è‡ªå‹•æ§‹ç¯‰ã—ã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # è¨­å®šã‹ã‚‰é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "        quantum_config = config['module_config']['quantum']\n",
    "        self.n_qubits = quantum_config['n_qubits']\n",
    "        self.n_layers = quantum_config['n_layers']\n",
    "        self.embedding_type = quantum_config['embedding_type']\n",
    "        self.entanglement = quantum_config['entanglement']\n",
    "        \n",
    "        # Qå€¤ãƒãƒƒãƒ—è¨­å®š\n",
    "        qmap_config = config['module_config']['qmap']\n",
    "        self.action_dim = qmap_config['action_dim']\n",
    "        \n",
    "        print(f\"ğŸ—ï¸ é‡å­AIæ§‹ç¯‰ä¸­...\")\n",
    "        print(f\"   é‡å­ãƒ“ãƒƒãƒˆæ•°: {self.n_qubits}\")\n",
    "        print(f\"   ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {self.n_layers}\")\n",
    "        print(f\"   ã‚¨ãƒ³ãƒ™ãƒ‡ã‚£ãƒ³ã‚°: {self.embedding_type}\")\n",
    "        print(f\"   ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ: {self.entanglement}\")\n",
    "        print(f\"   è¡Œå‹•æ¬¡å…ƒ: {self.action_dim}\")\n",
    "        \n",
    "        # é‡å­ãƒ‡ãƒã‚¤ã‚¹\n",
    "        self.dev = qml.device('default.qubit', wires=self.n_qubits)\n",
    "        \n",
    "        # å‰å‡¦ç†å±¤ï¼ˆCNNé¢¨ï¼‰\n",
    "        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # é‡å­å›è·¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(self.n_layers, self.n_qubits, 3) * 0.1\n",
    "        )\n",
    "        \n",
    "        # å¾Œå‡¦ç†å±¤\n",
    "        self.fc1 = nn.Linear(self.n_qubits, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, self.action_dim)\n",
    "        \n",
    "        print(f\"âœ… é‡å­AIæ§‹ç¯‰å®Œäº† - ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in self.parameters())}\")\n",
    "    \n",
    "    def quantum_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        WebUIè¨­å®šã«åŸºã¥ãé‡å­å›è·¯\n",
    "        \"\"\"\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆè¨­å®šã«åŸºã¥ãï¼‰\n",
    "        for i in range(self.n_qubits):\n",
    "            if self.embedding_type == 'angle':\n",
    "                qml.RY(inputs[i], wires=i)\n",
    "            elif self.embedding_type == 'amplitude':\n",
    "                qml.RY(inputs[i] * np.pi / 2, wires=i)\n",
    "                qml.RZ(inputs[i] * np.pi / 4, wires=i)\n",
    "            else:  # iqp\n",
    "                qml.RX(inputs[i], wires=i)\n",
    "        \n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸå±¤\n",
    "        for layer in range(self.n_layers):\n",
    "            # å›è»¢ã‚²ãƒ¼ãƒˆ\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[layer, i, 0], wires=i)\n",
    "                qml.RY(params[layer, i, 1], wires=i)\n",
    "                qml.RZ(params[layer, i, 2], wires=i)\n",
    "            \n",
    "            # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆï¼ˆè¨­å®šã«åŸºã¥ãï¼‰\n",
    "            if self.entanglement == 'linear':\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "            elif self.entanglement == 'circular':\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])  # å¾ªç’°çµåˆ\n",
    "            elif self.entanglement == 'full':\n",
    "                for i in range(self.n_qubits):\n",
    "                    for j in range(i+1, self.n_qubits):\n",
    "                        qml.CZ(wires=[i, j])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def forward(self, game_state):\n",
    "        \"\"\"\n",
    "        ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
    "        \"\"\"\n",
    "        batch_size = game_state.size(0)\n",
    "        \n",
    "        # CNNå‡¦ç†\n",
    "        x = torch.relu(self.conv1(game_state))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x).view(batch_size, -1)\n",
    "        \n",
    "        # é‡å­å‡¦ç†\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            quantum_input = torch.tanh(x[i]) * np.pi / 2\n",
    "            \n",
    "            # é‡å­å›è·¯ã‚’QNodeã¨ã—ã¦ä½œæˆ\n",
    "            @qml.qnode(self.dev)\n",
    "            def circuit(inputs, params):\n",
    "                return self.quantum_circuit(inputs, params)\n",
    "            \n",
    "            q_out = circuit(quantum_input, self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_tensor = torch.stack(quantum_outputs)\n",
    "        \n",
    "        # å¾Œå‡¦ç†\n",
    "        x = torch.relu(self.fc1(quantum_tensor))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_values = self.output(x)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "# WebUIè¨­å®šã«åŸºã¥ã„ã¦AIã‚’ä½œæˆ\n",
    "webui_ai = WebUIQuantumAI(config)\n",
    "\n",
    "# è¨­å®šå†…å®¹ã®è¡¨ç¤º\n",
    "print(f\"\\nğŸ“‹ WebUIè¨­å®šã‚µãƒãƒªãƒ¼:\")\n",
    "print(f\"   å­¦ç¿’æ–¹æ³•: {learning_config['method']}\")\n",
    "print(f\"   ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ : {learning_config['algorithm']}\")\n",
    "print(f\"   å ±é…¬æˆ¦ç•¥: {module_config['reward']['strategy']}\")\n",
    "print(f\"   è¡Œå‹•é¸æŠ: {module_config['action_selection']['strategy']}\")\n",
    "print(f\"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {hyperparameters['batch_size']}\")\n",
    "print(f\"   å­¦ç¿’ç‡: {hyperparameters['learning_rate']}\")\n",
    "print(f\"   ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {hyperparameters['optimizer']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ—¢å­˜ã®é‡å­AIã‚¯ãƒ©ã‚¹ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ\n",
    "from test_qubits_6 import QuantumBattleAI_6Qubits\n",
    "\n",
    "class CustomQuantumAI(nn.Module):\n",
    "    \"\"\"\n",
    "    ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªé‡å­AI\n",
    "    \n",
    "    ã“ã®ã‚¯ãƒ©ã‚¹ã§ã¯å­¦ç¿’ã®å„éƒ¨åˆ†ã‚’ç´°ã‹ãåˆ¶å¾¡ã§ãã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6, n_layers=2):\n",
    "        super().__init__()\n",
    "        self.n_qubits = n_qubits\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # é‡å­ãƒ‡ãƒã‚¤ã‚¹\n",
    "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "        \n",
    "        # å‰å‡¦ç†å±¤ï¼ˆCNNé¢¨ï¼‰\n",
    "        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # é‡å­å›è·¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(n_layers, n_qubits) * 0.1\n",
    "        )\n",
    "        \n",
    "        # å¾Œå‡¦ç†å±¤\n",
    "        self.fc1 = nn.Linear(n_qubits, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, 1)  # Qå€¤å‡ºåŠ›\n",
    "        \n",
    "    def quantum_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªé‡å­å›è·¯\n",
    "        \"\"\"\n",
    "        # å…¥åŠ›ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(inputs[i], wires=i)\n",
    "        \n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸå±¤\n",
    "        for layer in range(self.n_layers):\n",
    "            # å›è»¢ã‚²ãƒ¼ãƒˆ\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RY(params[layer, i], wires=i)\n",
    "            \n",
    "            # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒªãƒ³ã‚°\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            qml.CNOT(wires=[self.n_qubits - 1, 0])  # å¾ªç’°çµåˆ\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def forward(self, game_state):\n",
    "        \"\"\"\n",
    "        ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
    "        \"\"\"\n",
    "        batch_size = game_state.size(0)\n",
    "        \n",
    "        # CNNå‡¦ç†\n",
    "        x = torch.relu(self.conv1(game_state))\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x).view(batch_size, -1)\n",
    "        \n",
    "        # é‡å­å‡¦ç†\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            quantum_input = torch.tanh(x[i]) * np.pi / 2\n",
    "            \n",
    "            # é‡å­å›è·¯ã‚’QNodeã¨ã—ã¦ä½œæˆ\n",
    "            @qml.qnode(self.dev)\n",
    "            def circuit(inputs, params):\n",
    "                return self.quantum_circuit(inputs, params)\n",
    "            \n",
    "            q_out = circuit(quantum_input, self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_tensor = torch.stack(quantum_outputs)\n",
    "        \n",
    "        # å¾Œå‡¦ç†\n",
    "        x = torch.relu(self.fc1(quantum_tensor))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_value = self.output(x)\n",
    "        \n",
    "        return q_value\n",
    "\n",
    "# ã‚«ã‚¹ã‚¿ãƒ AIã‚’ãƒ†ã‚¹ãƒˆ\n",
    "custom_ai = CustomQuantumAI(n_qubits=6, n_layers=3)\n",
    "print(f\"ğŸ¤– ã‚«ã‚¹ã‚¿ãƒ é‡å­AIä½œæˆå®Œäº†\")\n",
    "print(f\"ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {sum(p.numel() for p in custom_ai.parameters())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def webui_training_loop(model, config, progress_callback=None):\n",
    "    \"\"\"\n",
    "    WebUIè¨­å®šã«åŸºã¥ãå­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "    \n",
    "    è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚“ã§è‡ªå‹•å­¦ç¿’\n",
    "    \"\"\"\n",
    "    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’è¨­å®šã‹ã‚‰å–å¾—\n",
    "    hyperparams = config['hyperparameters']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    episodes = hyperparams['epochs']\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    optimizer_type = hyperparams['optimizer']\n",
    "    \n",
    "    # å ±é…¬æˆ¦ç•¥ã‚’å–å¾—\n",
    "    reward_strategy = config['module_config']['reward']['strategy']\n",
    "    action_strategy = config['module_config']['action_selection']['strategy']\n",
    "    \n",
    "    print(f\"ğŸ¯ WebUIè¨­å®šã«ã‚ˆã‚‹å­¦ç¿’é–‹å§‹\")\n",
    "    print(f\"   ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {episodes}\")\n",
    "    print(f\"   ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}\")\n",
    "    print(f\"   å­¦ç¿’ç‡: {learning_rate}\")\n",
    "    print(f\"   ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶: {optimizer_type}\")\n",
    "    print(f\"   å ±é…¬æˆ¦ç•¥: {reward_strategy}\")\n",
    "    print(f\"   è¡Œå‹•æˆ¦ç•¥: {action_strategy}\")\n",
    "    \n",
    "    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶è¨­å®š\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    else:  # rmsprop\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ãƒ¼\n",
    "    replay_buffer = deque(maxlen=10000)\n",
    "    \n",
    "    # å­¦ç¿’çµ±è¨ˆ\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    convergence_data = {\n",
    "        'episodes': [],\n",
    "        'rewards': [],\n",
    "        'losses': [],\n",
    "        'win_rate': []\n",
    "    }\n",
    "    \n",
    "    # å ±é…¬é–¢æ•°ã®è¨­å®š\n",
    "    def calculate_reward(game, player, reward_strategy):\n",
    "        if game.game_over:\n",
    "            if game.winner == player:\n",
    "                if reward_strategy == 'escape':\n",
    "                    return 150  # è„±å‡ºæˆ¦ç•¥ã§ã¯å‹åˆ©å ±é…¬ã‚’é«˜ã\n",
    "                elif reward_strategy == 'aggressive':\n",
    "                    return 120  # æ”»æ’ƒæˆ¦ç•¥\n",
    "                else:\n",
    "                    return 100  # æ¨™æº–å‹åˆ©å ±é…¬\n",
    "            elif game.winner and game.winner != player:\n",
    "                if reward_strategy == 'defensive':\n",
    "                    return -80  # å®ˆå‚™æˆ¦ç•¥ã§ã¯æ•—åŒ—ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’è»½ã\n",
    "                else:\n",
    "                    return -100\n",
    "            else:\n",
    "                return 0  # å¼•ãåˆ†ã‘\n",
    "        else:\n",
    "            if reward_strategy == 'balanced':\n",
    "                return 2  # ãƒãƒ©ãƒ³ã‚¹æˆ¦ç•¥ã§ã¯ç¶™ç¶šå ±é…¬ã‚’é«˜ã\n",
    "            else:\n",
    "                return 1\n",
    "    \n",
    "    # Îµ-greedyè¨­å®š\n",
    "    epsilon_start = 0.9 if action_strategy == 'epsilon' else 0.0\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.995\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # ã‚²ãƒ¼ãƒ åˆæœŸåŒ–\n",
    "        try:\n",
    "            from qugeister import GeisterEngine\n",
    "            game = GeisterEngine()\n",
    "            game.reset_game()\n",
    "        except ImportError:\n",
    "            # ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ãŒç„¡ã„å ´åˆã®ãƒ€ãƒŸãƒ¼å‡¦ç†\n",
    "            print(\"âš ï¸ ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ãƒ€ãƒŸãƒ¼å­¦ç¿’ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\")\n",
    "            \n",
    "            # ãƒ€ãƒŸãƒ¼ãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’\n",
    "            dummy_states = torch.randn(batch_size, 7, 6, 6)\n",
    "            dummy_targets = torch.randn(batch_size, model.action_dim)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(dummy_states)\n",
    "            loss = loss_fn(outputs, dummy_targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            episode_rewards.append(random.uniform(-10, 10))\n",
    "            losses.append(loss.item())\n",
    "            continue\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        max_steps = 100\n",
    "        \n",
    "        while not game.game_over and step_count < max_steps:\n",
    "            current_player = game.current_player\n",
    "            legal_moves = game.get_legal_moves(current_player)\n",
    "            \n",
    "            if not legal_moves:\n",
    "                break\n",
    "            \n",
    "            if current_player == 'A':  # AIã®æ‰‹ç•ª\n",
    "                # çŠ¶æ…‹ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "                state = game.get_board_state()\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "                \n",
    "                # 7ãƒãƒ£ãƒ³ãƒãƒ«å½¢å¼ã«å¤‰æ›\n",
    "                state_7ch = torch.zeros(1, 7, 6, 6)\n",
    "                state_7ch[0, 0] = state_tensor[0]\n",
    "                \n",
    "                # è¡Œå‹•é¸æŠï¼ˆè¨­å®šã«åŸºã¥ãï¼‰\n",
    "                epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
    "                \n",
    "                if action_strategy == 'epsilon' and random.random() < epsilon:\n",
    "                    action = random.choice(legal_moves)\n",
    "                elif action_strategy == 'boltzmann':\n",
    "                    # Boltzmannæ¢ç´¢ï¼ˆç°¡ç•¥ç‰ˆï¼‰\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:  # greedy or ucb\n",
    "                    action = random.choice(legal_moves)\n",
    "                \n",
    "                # è¡Œå‹•å®Ÿè¡Œ\n",
    "                game.make_move(current_player, action)\n",
    "                \n",
    "                # å ±é…¬è¨ˆç®—\n",
    "                reward = calculate_reward(game, current_player, reward_strategy)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # çµŒé¨“ã‚’ãƒãƒƒãƒ•ã‚¡ãƒ¼ã«ä¿å­˜\n",
    "                replay_buffer.append((state_7ch, action, reward))\n",
    "                \n",
    "            else:  # ç›¸æ‰‹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã®æ‰‹ç•ª\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "            \n",
    "            step_count += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # ãƒãƒƒãƒå­¦ç¿’\n",
    "        if len(replay_buffer) >= batch_size and episode % 5 == 0:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            \n",
    "            states = torch.cat([item[0] for item in batch])\n",
    "            rewards = torch.FloatTensor([item[2] for item in batch])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            q_values = model(states).mean(dim=1)  # å¹³å‡Qå€¤\n",
    "            loss = loss_fn(q_values, rewards)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "        \n",
    "        # çµ±è¨ˆåé›†\n",
    "        if episode % 20 == 0:\n",
    "            recent_rewards = episode_rewards[-20:] if episode_rewards else [0]\n",
    "            recent_losses = losses[-10:] if losses else [0]\n",
    "            \n",
    "            convergence_data['episodes'].append(episode)\n",
    "            convergence_data['rewards'].append(np.mean(recent_rewards))\n",
    "            convergence_data['losses'].append(np.mean(recent_losses) if recent_losses else 0)\n",
    "            convergence_data['win_rate'].append(\n",
    "                len([r for r in recent_rewards if r > 50]) / len(recent_rewards) * 100\n",
    "            )\n",
    "            \n",
    "            print(f\"Episode {episode}: å¹³å‡å ±é…¬={np.mean(recent_rewards):.1f}, \"\n",
    "                  f\"æå¤±={np.mean(recent_losses):.4f}, Îµ={epsilon:.3f}\")\n",
    "            \n",
    "            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼ˆWebUIã¨ã®é€£æºç”¨ï¼‰\n",
    "            if progress_callback:\n",
    "                progress_callback({\n",
    "                    'episode': episode,\n",
    "                    'total_episodes': episodes,\n",
    "                    'avg_reward': np.mean(recent_rewards),\n",
    "                    'avg_loss': np.mean(recent_losses) if recent_losses else 0,\n",
    "                    'epsilon': epsilon,\n",
    "                    'win_rate': convergence_data['win_rate'][-1]\n",
    "                })\n",
    "    \n",
    "    print(f\"âœ… å­¦ç¿’å®Œäº†: æœ€çµ‚å¹³å‡å ±é…¬ = {np.mean(episode_rewards[-20:]):.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'losses': losses,\n",
    "        'convergence_data': convergence_data,\n",
    "        'final_performance': {\n",
    "            'avg_reward': np.mean(episode_rewards[-20:]) if episode_rewards else 0,\n",
    "            'total_episodes': episodes,\n",
    "            'final_win_rate': convergence_data['win_rate'][-1] if convergence_data['win_rate'] else 0\n",
    "        }\n",
    "    }\n",
    "\n",
    "# WebUIè¨­å®šã«ã‚ˆã‚‹å­¦ç¿’å®Ÿè¡Œ\n",
    "print(\"ğŸ§ª WebUIè¨­å®šå­¦ç¿’ãƒ†ã‚¹ãƒˆé–‹å§‹...\")\n",
    "results = webui_training_loop(webui_ai, config)\n",
    "\n",
    "# çµæœå¯è¦–åŒ–\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(results['rewards'])\n",
    "plt.title('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬ (WebUIè¨­å®š)')\n",
    "plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')\n",
    "plt.ylabel('å ±é…¬')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "if results['losses']:\n",
    "    plt.plot(results['losses'])\n",
    "    plt.title('å­¦ç¿’æå¤±')\n",
    "    plt.xlabel('æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—')\n",
    "    plt.ylabel('æå¤±')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "conv_data = results['convergence_data']\n",
    "if conv_data['episodes']:\n",
    "    plt.plot(conv_data['episodes'], conv_data['win_rate'], 'g-', label='å‹ç‡')\n",
    "    plt.plot(conv_data['episodes'], conv_data['rewards'], 'b-', label='å¹³å‡å ±é…¬')\n",
    "    plt.title('åæŸåˆ†æ')\n",
    "    plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')\n",
    "    plt.ylabel('å€¤')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# æœ€çµ‚çµæœè¡¨ç¤º\n",
    "final_perf = results['final_performance']\n",
    "print(f\"\\nğŸ“Š æœ€çµ‚æ€§èƒ½çµæœ:\")\n",
    "print(f\"   å¹³å‡å ±é…¬: {final_perf['avg_reward']:.2f}\")\n",
    "print(f\"   å‹ç‡: {final_perf['final_win_rate']:.1f}%\")\n",
    "print(f\"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {final_perf['total_episodes']}\")\n",
    "\n",
    "# è¨­å®šä¿å­˜ï¼ˆå­¦ç¿’çµæœä»˜ãï¼‰\n",
    "config['training_results'] = results['final_performance']\n",
    "with open(f\"quantum_training_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\", 'w') as f:\n",
    "    json.dump(config, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"ğŸ’¾ å­¦ç¿’çµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_training_loop(model, episodes=100, batch_size=8):\n",
    "    \"\"\"\n",
    "    ã‚«ã‚¹ã‚¿ãƒã‚¤ã‚ºå¯èƒ½ãªå­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "    \n",
    "    ã“ã®é–¢æ•°ã§ã¯å­¦ç¿’ã®å„ã‚¹ãƒ†ãƒƒãƒ—ã‚’è©³ç´°ã«åˆ¶å¾¡ã§ãã¾ã™ã€‚\n",
    "    \"\"\"\n",
    "    # å­¦ç¿’è¨­å®š\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    loss_fn = nn.MSELoss()\n",
    "    \n",
    "    # çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ãƒ¼\n",
    "    replay_buffer = deque(maxlen=1000)\n",
    "    \n",
    "    # å­¦ç¿’çµ±è¨ˆ\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    \n",
    "    print(f\"ğŸ¯ å­¦ç¿’é–‹å§‹: {episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰\")\n",
    "    \n",
    "    for episode in range(episodes):\n",
    "        # ã‚²ãƒ¼ãƒ åˆæœŸåŒ–\n",
    "        game = GeisterEngine()\n",
    "        game.reset_game()\n",
    "        \n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        max_steps = 50  # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é•·åˆ¶é™\n",
    "        \n",
    "        while not game.game_over and step_count < max_steps:\n",
    "            current_player = game.current_player\n",
    "            legal_moves = game.get_legal_moves(current_player)\n",
    "            \n",
    "            if not legal_moves:\n",
    "                break\n",
    "            \n",
    "            if current_player == 'A':  # AIã®æ‰‹ç•ª\n",
    "                # çŠ¶æ…‹ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰\n",
    "                state = game.get_board_state()\n",
    "                state_tensor = torch.FloatTensor(state).unsqueeze(0)  # ãƒãƒ£ãƒ³ãƒãƒ«æ¬¡å…ƒè¿½åŠ \n",
    "                \n",
    "                # 7ãƒãƒ£ãƒ³ãƒãƒ«å½¢å¼ã«å¤‰æ›ï¼ˆç°¡ç•¥åŒ–ï¼‰\n",
    "                state_7ch = torch.zeros(1, 7, 6, 6)\n",
    "                state_7ch[0, 0] = state_tensor[0]  # åŸºæœ¬ãƒœãƒ¼ãƒ‰æƒ…å ±\n",
    "                \n",
    "                # Îµ-greedyè¡Œå‹•é¸æŠ\n",
    "                epsilon = max(0.01, 0.5 * (0.995 ** episode))\n",
    "                \n",
    "                if random.random() < epsilon:\n",
    "                    # ãƒ©ãƒ³ãƒ€ãƒ è¡Œå‹•\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:\n",
    "                    # AIã«ã‚ˆã‚‹è¡Œå‹•é¸æŠï¼ˆç°¡ç•¥åŒ–ï¼‰\n",
    "                    action = random.choice(legal_moves)\n",
    "                \n",
    "                # è¡Œå‹•å®Ÿè¡Œ\n",
    "                game.make_move(current_player, action)\n",
    "                \n",
    "                # å ±é…¬è¨ˆç®—\n",
    "                if game.game_over:\n",
    "                    if game.winner == 'A':\n",
    "                        reward = 100  # å‹åˆ©\n",
    "                    elif game.winner == 'B':\n",
    "                        reward = -100  # æ•—åŒ—\n",
    "                    else:\n",
    "                        reward = 0  # å¼•ãåˆ†ã‘\n",
    "                else:\n",
    "                    reward = 1  # ç¶™ç¶šå ±é…¬\n",
    "                \n",
    "                episode_reward += reward\n",
    "                \n",
    "                # çµŒé¨“ã‚’ãƒãƒƒãƒ•ã‚¡ãƒ¼ã«ä¿å­˜\n",
    "                replay_buffer.append((state_7ch, action, reward))\n",
    "                \n",
    "            else:  # ç›¸æ‰‹ï¼ˆãƒ©ãƒ³ãƒ€ãƒ ï¼‰ã®æ‰‹ç•ª\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "            \n",
    "            step_count += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # ãƒãƒƒãƒå­¦ç¿’\n",
    "        if len(replay_buffer) >= batch_size and episode % 10 == 0:\n",
    "            # ãƒ©ãƒ³ãƒ€ãƒ ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            \n",
    "            # ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿ã‚’æº–å‚™\n",
    "            states = torch.cat([item[0] for item in batch])\n",
    "            rewards = torch.FloatTensor([item[2] for item in batch])\n",
    "            \n",
    "            # ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\n",
    "            optimizer.zero_grad()\n",
    "            q_values = model(states).squeeze()\n",
    "            \n",
    "            # æå¤±è¨ˆç®—\n",
    "            loss = loss_fn(q_values, rewards)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # ãƒãƒƒã‚¯ãƒ—ãƒ­ãƒ‘ã‚²ãƒ¼ã‚·ãƒ§ãƒ³\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # é€²æ—è¡¨ç¤º\n",
    "        if episode % 20 == 0:\n",
    "            avg_reward = np.mean(episode_rewards[-20:]) if episode_rewards else 0\n",
    "            avg_loss = np.mean(losses[-10:]) if losses else 0\n",
    "            print(f\"Episode {episode}: å¹³å‡å ±é…¬={avg_reward:.1f}, æå¤±={avg_loss:.4f}\")\n",
    "    \n",
    "    return episode_rewards, losses\n",
    "\n",
    "# çŸ­æœŸå­¦ç¿’ãƒ†ã‚¹ãƒˆ\n",
    "print(\"ğŸ§ª çŸ­æœŸå­¦ç¿’ãƒ†ã‚¹ãƒˆé–‹å§‹...\")\n",
    "rewards, losses = custom_training_loop(custom_ai, episodes=50, batch_size=4)\n",
    "\n",
    "# çµæœå¯è¦–åŒ–\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards)\n",
    "plt.title('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å ±é…¬')\n",
    "plt.xlabel('ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰')\n",
    "plt.ylabel('å ±é…¬')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "if losses:\n",
    "    plt.plot(losses)\n",
    "    plt.title('å­¦ç¿’æå¤±')\n",
    "    plt.xlabel('æ›´æ–°ã‚¹ãƒ†ãƒƒãƒ—')\n",
    "    plt.ylabel('æå¤±')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"âœ… å­¦ç¿’å®Œäº†: å¹³å‡å ±é…¬ = {np.mean(rewards):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. æ€§èƒ½è©•ä¾¡ã¨èª¿æ•´\n",
    "\n",
    "å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡ã—ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, n_games=10):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®æ€§èƒ½ã‚’è©•ä¾¡\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    losses = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for game_idx in range(n_games):\n",
    "            game = GeisterEngine()\n",
    "            game.reset_game()\n",
    "            \n",
    "            step_count = 0\n",
    "            max_steps = 50\n",
    "            \n",
    "            while not game.game_over and step_count < max_steps:\n",
    "                current_player = game.current_player\n",
    "                legal_moves = game.get_legal_moves(current_player)\n",
    "                \n",
    "                if not legal_moves:\n",
    "                    break\n",
    "                \n",
    "                # ä¸¡ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã¨ã‚‚ãƒ©ãƒ³ãƒ€ãƒ ï¼ˆãƒ‡ãƒ¢ç”¨ï¼‰\n",
    "                action = random.choice(legal_moves)\n",
    "                game.make_move(current_player, action)\n",
    "                step_count += 1\n",
    "            \n",
    "            # çµæœé›†è¨ˆ\n",
    "            if game.winner == 'A':\n",
    "                wins += 1\n",
    "            elif game.winner == 'B':\n",
    "                losses += 1\n",
    "            else:\n",
    "                draws += 1\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    return {\n",
    "        'wins': wins,\n",
    "        'draws': draws, \n",
    "        'losses': losses,\n",
    "        'win_rate': wins / n_games * 100\n",
    "    }\n",
    "\n",
    "# æ€§èƒ½è©•ä¾¡\n",
    "results = evaluate_model(custom_ai, n_games=20)\n",
    "print(\"ğŸ“Š æ€§èƒ½è©•ä¾¡çµæœ:\")\n",
    "print(f\"å‹åˆ©: {results['wins']}/20 ({results['win_rate']:.1f}%)\")\n",
    "print(f\"å¼•åˆ†: {results['draws']}/20\")\n",
    "print(f\"æ•—åŒ—: {results['losses']}/20\")\n",
    "\n",
    "# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®ãƒ’ãƒ³ãƒˆ\n",
    "print(\"\\nğŸ”§ ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®ãƒ’ãƒ³ãƒˆ:\")\n",
    "print(\"1. å­¦ç¿’ç‡: 0.001-0.01 (é«˜ã™ãã‚‹ã¨ä¸å®‰å®šã€ä½ã™ãã‚‹ã¨é…ã„)\")\n",
    "print(\"2. ãƒãƒƒãƒã‚µã‚¤ã‚º: 8-32 (ãƒ¡ãƒ¢ãƒªã¨ã®å…¼ã­åˆã„)\")\n",
    "print(\"3. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: 1000-5000 (ååˆ†ãªå­¦ç¿’ã®ãŸã‚)\")\n",
    "print(\"4. Îµå€¤: 0.1-0.5 é–‹å§‹ã€0.01ã¾ã§æ¸›è¡° (æ¢ç´¢ã¨æ´»ç”¨ã®ãƒãƒ©ãƒ³ã‚¹)\")\n",
    "print(\"5. é‡å­å›è·¯å±¤æ•°: 2-5å±¤ (è¡¨ç¾åŠ›ã¨è¨ˆç®—ã‚³ã‚¹ãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. é«˜åº¦ãªã‚«ã‚¹ã‚¿ãƒã‚¤ã‚º\n",
    "\n",
    "ã‚ˆã‚Šé«˜åº¦ãªé‡å­å›è·¯è¨­è¨ˆã‚„å­¦ç¿’æ‰‹æ³•ã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# é«˜åº¦ãªé‡å­å›è·¯è¨­è¨ˆ\n",
    "class AdvancedQuantumCircuit:\n",
    "    \"\"\"\n",
    "    ã‚ˆã‚Šè¤‡é›‘ãªé‡å­å›è·¯è¨­è¨ˆ\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, n_qubits=6):\n",
    "        self.n_qubits = n_qubits\n",
    "        self.dev = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    def amplitude_encoding_circuit(self, data, params):\n",
    "        \"\"\"\n",
    "        æŒ¯å¹…ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ãŸå›è·¯\n",
    "        \"\"\"\n",
    "        # æ­£è¦åŒ–\n",
    "        norm = torch.norm(data)\n",
    "        if norm > 0:\n",
    "            data = data / norm\n",
    "        \n",
    "        # æŒ¯å¹…ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆç°¡ç•¥ç‰ˆï¼‰\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(data[i] * np.pi, wires=i)\n",
    "        \n",
    "        # Strongly Entangling Layers\n",
    "        qml.StronglyEntanglingLayers(params, wires=range(self.n_qubits))\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def variational_circuit(self, inputs, params):\n",
    "        \"\"\"\n",
    "        å¤‰åˆ†é‡å­å›è·¯\n",
    "        \"\"\"\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.Hadamard(wires=i)\n",
    "            qml.RZ(inputs[i], wires=i)\n",
    "        \n",
    "        # å¤‰åˆ†å±¤\n",
    "        for layer in range(len(params)):\n",
    "            # å›è»¢ã‚²ãƒ¼ãƒˆ\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[layer][i][0], wires=i)\n",
    "                qml.RY(params[layer][i][1], wires=i)\n",
    "                qml.RZ(params[layer][i][2], wires=i)\n",
    "            \n",
    "            # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒªãƒ³ã‚°\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "            if self.n_qubits > 2:\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "# é«˜åº¦ãªå­¦ç¿’æ‰‹æ³•\n",
    "def parameter_shift_training(model, data, target, lr=0.01):\n",
    "    \"\"\"\n",
    "    ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚·ãƒ•ãƒˆæ³•ã«ã‚ˆã‚‹å‹¾é…è¨ˆç®—\n",
    "    \n",
    "    é‡å­å›è·¯ç‰¹æœ‰ã®å‹¾é…è¨ˆç®—æ‰‹æ³•\n",
    "    \"\"\"\n",
    "    gradients = []\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        if param.requires_grad:\n",
    "            param_grad = torch.zeros_like(param)\n",
    "            \n",
    "            for i in range(param.numel()):\n",
    "                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’+Ï€/2ã‚·ãƒ•ãƒˆ\n",
    "                param.data.view(-1)[i] += np.pi / 2\n",
    "                output_plus = model(data)\n",
    "                \n",
    "                # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’-Ï€/2ã‚·ãƒ•ãƒˆ\n",
    "                param.data.view(-1)[i] -= np.pi\n",
    "                output_minus = model(data)\n",
    "                \n",
    "                # å…ƒã«æˆ»ã™\n",
    "                param.data.view(-1)[i] += np.pi / 2\n",
    "                \n",
    "                # å‹¾é…è¨ˆç®—\n",
    "                grad = (output_plus - output_minus) / 2\n",
    "                param_grad.view(-1)[i] = grad.item()\n",
    "            \n",
    "            gradients.append(param_grad)\n",
    "    \n",
    "    return gradients\n",
    "\n",
    "# é‡å­å›è·¯ã®å¯è¦–åŒ–\n",
    "def visualize_quantum_circuit():\n",
    "    \"\"\"\n",
    "    é‡å­å›è·¯ã®æ§‹é€ ã‚’å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    n_qubits = 4  # å¯è¦–åŒ–ç”¨ã«å°ã•ãã™ã‚‹\n",
    "    dev = qml.device('default.qubit', wires=n_qubits)\n",
    "    \n",
    "    @qml.qnode(dev)\n",
    "    def demo_circuit(params):\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "        for i in range(n_qubits):\n",
    "            qml.RY(params[i], wires=i)\n",
    "        \n",
    "        # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒªãƒ³ã‚°\n",
    "        for i in range(n_qubits - 1):\n",
    "            qml.CNOT(wires=[i, i + 1])\n",
    "        \n",
    "        return qml.expval(qml.PauliZ(0))\n",
    "    \n",
    "    # å›è·¯å›³ã‚’ç”Ÿæˆ\n",
    "    params = np.random.rand(n_qubits)\n",
    "    print(\"ğŸ”§ é‡å­å›è·¯ã®æ§‹é€ :\")\n",
    "    print(qml.draw(demo_circuit)(params))\n",
    "\n",
    "visualize_quantum_circuit()\n",
    "\n",
    "print(\"\\nğŸ“ å­¦ç¿’ã®ãƒã‚¤ãƒ³ãƒˆ:\")\n",
    "print(\"1. é‡å­å›è·¯ã®æ·±ã•: æµ…ã„å›è·¯ã‹ã‚‰å§‹ã‚ã¦å¾ã€…ã«è¤‡é›‘ã«ã™ã‚‹\")\n",
    "print(\"2. ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°æ–¹æ³•: Angle, Amplitude, IQPãªã©æ§˜ã€…ãªæ‰‹æ³•ã‚’è©¦ã™\")\n",
    "print(\"3. æ¸¬å®šæ–¹æ³•: PauliZ, PauliX, PauliYã®çµ„ã¿åˆã‚ã›ã§æƒ…å ±ã‚’æŠ½å‡º\")\n",
    "print(\"4. ãƒãƒªã‚¨ãƒ¼ã‚·ãƒ§ãƒŠãƒ«å±¤: è¡¨ç¾åŠ›ã¨å­¦ç¿’å¯èƒ½æ€§ã®ãƒãƒ©ãƒ³ã‚¹\")\n",
    "print(\"5. ãƒã‚¤ã‚ºè€æ€§: å®Ÿæ©Ÿã§ã¯é‡å­ãƒã‚¤ã‚ºã‚’è€ƒæ…®ã—ãŸè¨­è¨ˆãŒé‡è¦\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== WebUIã¨ã®åŒæ–¹å‘é€£æºæ©Ÿèƒ½ =====\n",
    "\n",
    "class WebUIBridge:\n",
    "    \"\"\"\n",
    "    WebUIã¨Jupyter Notebookã®æ©‹æ¸¡ã—ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.status_file = \"jupyter_status.json\"\n",
    "        self.results_file = \"jupyter_results.json\"\n",
    "    \n",
    "    def send_progress_to_webui(self, progress_data):\n",
    "        \"\"\"\n",
    "        å­¦ç¿’é€²æ—ã‚’WebUIã«é€ä¿¡\n",
    "        \"\"\"\n",
    "        progress_data['timestamp'] = datetime.now().isoformat()\n",
    "        progress_data['status'] = 'training'\n",
    "        \n",
    "        try:\n",
    "            with open(self.status_file, 'w') as f:\n",
    "                json.dump(progress_data, f, indent=2)\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ é€²æ—é€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def send_results_to_webui(self, results, config):\n",
    "        \"\"\"\n",
    "        å­¦ç¿’çµæœã‚’WebUIã«é€ä¿¡\n",
    "        \"\"\"\n",
    "        webui_results = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'status': 'completed',\n",
    "            'config_used': config,\n",
    "            'performance': results['final_performance'],\n",
    "            'training_data': {\n",
    "                'total_episodes': len(results['rewards']),\n",
    "                'final_avg_reward': results['final_performance']['avg_reward'],\n",
    "                'convergence_achieved': results['final_performance']['final_win_rate'] > 60,\n",
    "                'loss_trend': 'decreasing' if len(results['losses']) > 10 and \n",
    "                             results['losses'][-1] < results['losses'][5] else 'stable'\n",
    "            },\n",
    "            'recommendations': self._generate_recommendations(results)\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            with open(self.results_file, 'w') as f:\n",
    "                json.dump(webui_results, f, indent=2, ensure_ascii=False)\n",
    "            print(f\"âœ… çµæœã‚’WebUIã«é€ä¿¡: {self.results_file}\")\n",
    "            return True\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ çµæœé€ä¿¡ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def _generate_recommendations(self, results):\n",
    "        \"\"\"\n",
    "        å­¦ç¿’çµæœã«åŸºã¥ãæ¨å¥¨è¨­å®šã‚’ç”Ÿæˆ\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        avg_reward = results['final_performance']['avg_reward']\n",
    "        win_rate = results['final_performance']['final_win_rate']\n",
    "        \n",
    "        if avg_reward < 20:\n",
    "            recommendations.append({\n",
    "                'type': 'learning_rate',\n",
    "                'message': 'å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™ (0.01-0.05)',\n",
    "                'priority': 'high'\n",
    "            })\n",
    "        \n",
    "        if win_rate < 40:\n",
    "            recommendations.append({\n",
    "                'type': 'exploration',\n",
    "                'message': 'Îµå€¤ã‚’èª¿æ•´ã—ã¦æ¢ç´¢ã‚’å¢—ã‚„ã™ã“ã¨ã‚’æ¨å¥¨ã—ã¾ã™',\n",
    "                'priority': 'medium'\n",
    "            })\n",
    "        \n",
    "        if len(results['losses']) > 0 and results['losses'][-1] > results['losses'][0]:\n",
    "            recommendations.append({\n",
    "                'type': 'overfitting',\n",
    "                'message': 'éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æ­£å‰‡åŒ–ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„',\n",
    "                'priority': 'high'\n",
    "            })\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "# WebUIé€£æºã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ä½œæˆ\n",
    "webui_bridge = WebUIBridge()\n",
    "\n",
    "# é€²æ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯é–¢æ•°\n",
    "def progress_callback(progress_data):\n",
    "    \"\"\"\n",
    "    å­¦ç¿’é€²æ—ã‚’WebUIã«é€ä¿¡ã™ã‚‹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯\n",
    "    \"\"\"\n",
    "    webui_bridge.send_progress_to_webui(progress_data)\n",
    "    \n",
    "    # Jupyterå†…ã§ã‚‚è¡¨ç¤º\n",
    "    print(f\"ğŸ“Š Progress: Episode {progress_data['episode']}/{progress_data['total_episodes']} \"\n",
    "          f\"- Reward: {progress_data['avg_reward']:.1f}, \"\n",
    "          f\"Win Rate: {progress_data['win_rate']:.1f}%\")\n",
    "\n",
    "print(\"ğŸŒ‰ WebUIé€£æºãƒ–ãƒªãƒƒã‚¸æº–å‚™å®Œäº†\")\n",
    "\n",
    "# ===== é«˜åº¦ãªå­¦ç¿’ã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚° =====\n",
    "\n",
    "def advanced_training_with_monitoring(model, config):\n",
    "    \"\"\"\n",
    "    é«˜åº¦ãªå­¦ç¿’ç›£è¦–æ©Ÿèƒ½ä»˜ããƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ é«˜åº¦ãªå­¦ç¿’ãƒ»ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ é–‹å§‹\")\n",
    "    \n",
    "    # å­¦ç¿’å‰ã®åˆæœŸçŠ¶æ…‹ã‚’WebUIã«é€ä¿¡\n",
    "    initial_status = {\n",
    "        'episode': 0,\n",
    "        'total_episodes': config['hyperparameters']['epochs'],\n",
    "        'avg_reward': 0.0,\n",
    "        'avg_loss': 0.0,\n",
    "        'epsilon': 0.9,\n",
    "        'win_rate': 0.0,\n",
    "        'phase': 'initialization'\n",
    "    }\n",
    "    webui_bridge.send_progress_to_webui(initial_status)\n",
    "    \n",
    "    # å­¦ç¿’å®Ÿè¡Œï¼ˆãƒ—ãƒ­ã‚°ãƒ¬ã‚¹ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ä»˜ãï¼‰\n",
    "    results = webui_training_loop(model, config, progress_callback)\n",
    "    \n",
    "    # æœ€çµ‚çµæœã‚’WebUIã«é€ä¿¡\n",
    "    webui_bridge.send_results_to_webui(results, config)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# å­¦ç¿’å®Ÿè¡Œã¨ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°\n",
    "print(\"ğŸ¯ é«˜åº¦ãªå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã§WebUIé€£æºãƒ†ã‚¹ãƒˆ...\")\n",
    "advanced_results = advanced_training_with_monitoring(webui_ai, config)\n",
    "\n",
    "# è©³ç´°åˆ†æã¨å¯è¦–åŒ–\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "# 1. å­¦ç¿’æ›²ç·š\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.plot(advanced_results['rewards'])\n",
    "plt.title('Learning Curve')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Reward')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 2. æå¤±æ›²ç·š\n",
    "plt.subplot(2, 4, 2)\n",
    "if advanced_results['losses']:\n",
    "    plt.plot(advanced_results['losses'])\n",
    "    plt.title('Loss Curve')\n",
    "    plt.xlabel('Training Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. ç§»å‹•å¹³å‡å ±é…¬\n",
    "plt.subplot(2, 4, 3)\n",
    "if len(advanced_results['rewards']) > 20:\n",
    "    moving_avg = np.convolve(advanced_results['rewards'], np.ones(20)/20, mode='valid')\n",
    "    plt.plot(moving_avg)\n",
    "    plt.title('Moving Average Reward')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Avg Reward (20 episodes)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 4. å‹ç‡æ¨ç§»\n",
    "plt.subplot(2, 4, 4)\n",
    "conv_data = advanced_results['convergence_data']\n",
    "if conv_data['episodes']:\n",
    "    plt.plot(conv_data['episodes'], conv_data['win_rate'])\n",
    "    plt.title('Win Rate')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Win Rate (%)')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 5. å ±é…¬åˆ†å¸ƒ\n",
    "plt.subplot(2, 4, 5)\n",
    "plt.hist(advanced_results['rewards'], bins=30, alpha=0.7)\n",
    "plt.title('Reward Distribution')\n",
    "plt.xlabel('Reward')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 6. å­¦ç¿’ç‡vsæ€§èƒ½\n",
    "plt.subplot(2, 4, 6)\n",
    "episodes = range(len(advanced_results['rewards']))\n",
    "rewards = advanced_results['rewards']\n",
    "if episodes and rewards:\n",
    "    # å­¦ç¿’æ®µéšåˆ¥ã®æ€§èƒ½\n",
    "    early = np.mean(rewards[:len(rewards)//3]) if len(rewards) > 3 else 0\n",
    "    mid = np.mean(rewards[len(rewards)//3:2*len(rewards)//3]) if len(rewards) > 3 else 0\n",
    "    late = np.mean(rewards[2*len(rewards)//3:]) if len(rewards) > 3 else 0\n",
    "    \n",
    "    plt.bar(['Early', 'Mid', 'Late'], [early, mid, late])\n",
    "    plt.title('Performance by Phase')\n",
    "    plt.ylabel('Average Reward')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 7. åæŸåˆ†æ\n",
    "plt.subplot(2, 4, 7)\n",
    "if conv_data['losses']:\n",
    "    plt.plot(conv_data['episodes'], conv_data['losses'], 'r-', label='Loss')\n",
    "    plt.plot(conv_data['episodes'], conv_data['rewards'], 'b-', label='Reward')\n",
    "    plt.title('Convergence Analysis')\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Value')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "\n",
    "# 8. æœ€çµ‚çµ±è¨ˆ\n",
    "plt.subplot(2, 4, 8)\n",
    "final_stats = [\n",
    "    advanced_results['final_performance']['avg_reward'],\n",
    "    advanced_results['final_performance']['final_win_rate'],\n",
    "    len(advanced_results['rewards']),\n",
    "    len(advanced_results['losses'])\n",
    "]\n",
    "labels = ['Avg Reward', 'Win Rate', 'Episodes', 'Updates']\n",
    "plt.bar(labels, final_stats)\n",
    "plt.title('Final Statistics')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸŠ WebUIé€£æºå­¦ç¿’å®Œäº†!\")\n",
    "print(f\"ğŸ“ˆ æœ€çµ‚æ€§èƒ½: å¹³å‡å ±é…¬ {advanced_results['final_performance']['avg_reward']:.2f}\")\n",
    "print(f\"ğŸ† å‹ç‡: {advanced_results['final_performance']['final_win_rate']:.1f}%\")\n",
    "print(f\"ğŸ’¾ çµæœãƒ•ã‚¡ã‚¤ãƒ«: {webui_bridge.results_file}\")\n",
    "print(f\"ğŸ“Š ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ãƒ•ã‚¡ã‚¤ãƒ«: {webui_bridge.status_file}\")\n",
    "\n",
    "# æ¨å¥¨è¨­å®šã®è¡¨ç¤º\n",
    "if 'training_data' in advanced_results:\n",
    "    print(f\"\\nğŸ’¡ å­¦ç¿’çŠ¶æ³è¨ºæ–­:\")\n",
    "    print(f\"   åæŸçŠ¶æ³: {'âœ… è‰¯å¥½' if advanced_results['final_performance']['final_win_rate'] > 60 else 'âš ï¸ è¦æ”¹å–„'}\")\n",
    "    print(f\"   å­¦ç¿’å®‰å®šæ€§: {'âœ… å®‰å®š' if len(advanced_results['losses']) > 0 else 'âš ï¸ ä¸å®‰å®š'}\")\n",
    "    print(f\"   æ¨å¥¨æ¬¡å›å®Ÿè¡Œ: {'é•·æœŸå­¦ç¿’' if advanced_results['final_performance']['avg_reward'] > 30 else 'è¨­å®šèª¿æ•´'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ”„ WebUIã¨ã®åŒæ–¹å‘é€£æº\n",
    "\n",
    "WebUIã§è¨­å®šã—ãŸå†…å®¹ãŒJupyter Notebookã«è‡ªå‹•åæ˜ ã•ã‚Œã€å­¦ç¿’çµæœã‚’WebUIã«é€ã‚Šè¿”ã™ã“ã¨ãŒã§ãã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ¯ å®Ÿè·µçš„ãªä½¿ã„æ–¹\n",
    "\n",
    "### åŸºæœ¬çš„ãªå­¦ç¿’ãƒ•ãƒ­ãƒ¼:\n",
    "\n",
    "1. **ãƒ‡ãƒ¼ã‚¿æº–å‚™**: ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã‚’7ãƒãƒ£ãƒ³ãƒãƒ«å½¢å¼ã§è¡¨ç¾\n",
    "2. **å›è·¯è¨­è¨ˆ**: å•é¡Œã«å¿œã˜ã¦é‡å­å›è·¯ã‚’è¨­è¨ˆ\n",
    "3. **å­¦ç¿’å®Ÿè¡Œ**: DQNã¾ãŸã¯ã‚«ã‚¹ã‚¿ãƒ å­¦ç¿’ãƒ«ãƒ¼ãƒ—ã§è¨“ç·´\n",
    "4. **è©•ä¾¡**: å¯¾æˆ¦ãƒ†ã‚¹ãƒˆã§æ€§èƒ½ã‚’ç¢ºèª\n",
    "5. **èª¿æ•´**: ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’æœ€é©åŒ–\n",
    "\n",
    "### ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´ã®æŒ‡é‡:\n",
    "\n",
    "- **å­¦ç¿’ãŒä¸å®‰å®š**: å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹ã€ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å°ã•ãã™ã‚‹\n",
    "- **å­¦ç¿’ãŒé…ã„**: å­¦ç¿’ç‡ã‚’ä¸Šã’ã‚‹ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã‚’å¢—ã‚„ã™\n",
    "- **éå­¦ç¿’**: æ­£å‰‡åŒ–ã‚’å¼·ã‚ã‚‹ã€å›è·¯ã‚’æµ…ãã™ã‚‹\n",
    "- **æ±åŒ–æ€§èƒ½ãŒä½ã„**: ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã€ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆè¿½åŠ \n",
    "\n",
    "### æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:\n",
    "\n",
    "1. å®Ÿéš›ã®é‡å­ãƒ‡ãƒã‚¤ã‚¹ï¼ˆIBM Quantumã€Rigettiï¼‰ã§ã®å®Ÿè¡Œ\n",
    "2. ã‚ˆã‚Šè¤‡é›‘ãªã‚²ãƒ¼ãƒ ç’°å¢ƒã§ã®è©•ä¾¡\n",
    "3. ä»–ã®é‡å­æ©Ÿæ¢°å­¦ç¿’ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã¨ã®æ¯”è¼ƒ\n",
    "4. ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰å¤å…¸-é‡å­ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®æ¢ç´¢"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
