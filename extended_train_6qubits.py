#!/usr/bin/env python3
"""
6é‡å­ãƒ“ãƒƒãƒˆAI CNNé¢¨è¨­è¨ˆã§ã®é•·æœŸå­¦ç¿’ï¼ˆ3000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰
"""

import sys
from pathlib import Path
import torch
import time

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã®ãƒ‘ã‚¹ã‚’è¿½åŠ 
sys.path.append(str(Path(__file__).parent))
sys.path.append(str(Path(__file__).parent / "src"))

from test_qubits_6 import QuantumBattleAI_6Qubits, train_model

print('ğŸ§  6é‡å­ãƒ“ãƒƒãƒˆAI CNNé¢¨è¨­è¨ˆã§ã®é•·æœŸå­¦ç¿’')
print('=' * 60)
print('ğŸ“Š è¨­å®š: 3000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å­¦ç¿’')

# é•·æœŸå­¦ç¿’è¨­å®š
hyperparameters = {
    'epochs': 3000,         # é•·æœŸå­¦ç¿’
    'batch_size': 8,        
    'learning_rate': 0.0005,  # å°‘ã—ä½ã‚ã«è¨­å®š
    'optimizer': 'adam',
    'scheduler': 'cosine',
    'replay_buffer_size': 2000,  # ãƒãƒƒãƒ•ã‚¡ãƒ¼ã‚µã‚¤ã‚ºå¢—åŠ 
    'epsilon': 0.5,         # æ¢ç´¢ã‚’å¤šã‚ã«
    'epsilon_decay': 0.9995,  # ã‚†ã£ãã‚Šã¨æ¸›è¡°
    'l2_regularization': 1e-4,
    'gamma': 0.99,          # é•·æœŸå ±é…¬é‡è¦–
    'target_update': 20     # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ›´æ–°é »åº¦
}

start_time = time.time()

try:
    # AIåˆæœŸåŒ–
    ai = QuantumBattleAI_6Qubits(name='6qubits_extended', player_id='A')
    print('âœ… AIåˆæœŸåŒ–å®Œäº†')
    
    # ãƒ¢ãƒ‡ãƒ«è¨­å®šã‚’å–å¾—
    config = ai.config
    
    print(f'ğŸ¯ é•·æœŸå­¦ç¿’é–‹å§‹ï¼ˆ{hyperparameters["epochs"]}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰...')
    print('é€²è¡ŒçŠ¶æ³ã¯1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«è¡¨ç¤ºã•ã‚Œã¾ã™')
    
    # å­¦ç¿’å®Ÿè¡Œ
    trained_model, win_rate = train_model(ai, config, hyperparameters)
    
    elapsed_time = time.time() - start_time
    print(f'âœ… å­¦ç¿’å®Œäº†ï¼')
    print(f'ğŸ“Š æœ€çµ‚å‹ç‡: {win_rate:.2f}%')
    print(f'â±ï¸ å­¦ç¿’æ™‚é–“: {elapsed_time:.1f}ç§’ ({elapsed_time/60:.1f}åˆ†)')
    
    # å­¦ç¿’å¾Œãƒ†ã‚¹ãƒˆ
    from qugeister import GeisterEngine
    game = GeisterEngine()
    game.reset_game()
    legal_moves = game.get_legal_moves('A')
    move = trained_model.get_move(game, legal_moves)
    
    print(f'ğŸ® å­¦ç¿’å¾Œã®æ‰‹é¸æŠ: {move}')
    print(f'âœ“ æœ‰åŠ¹æ€§: {move in legal_moves}')
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    model_filename = 'trained_6qubits_extended_3000ep.pth'
    torch.save({
        'model_state_dict': trained_model.state_dict(),
        'config': config,
        'hyperparameters': hyperparameters,
        'final_win_rate': win_rate,
        'training_episodes': hyperparameters['epochs'],
        'training_time': elapsed_time
    }, model_filename)
    print(f'ğŸ’¾ å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†: {model_filename}')
    
    # æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆ
    print('\nğŸ† æ€§èƒ½æ¯”è¼ƒãƒ†ã‚¹ãƒˆé–‹å§‹...')
    from test_trained_models import test_trained_model
    
    # å¾“æ¥ãƒ¢ãƒ‡ãƒ«ã¨ã®æ¯”è¼ƒ
    print('çŸ­æœŸå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆ30ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰ã¨ã®æ¯”è¼ƒ:')
    short_win_rate = test_trained_model(QuantumBattleAI_6Qubits, 'trained_6qubits_cnn.pth', '6_short')
    
    print(f'é•·æœŸå­¦ç¿’ãƒ¢ãƒ‡ãƒ«ï¼ˆ3000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰ã®ãƒ†ã‚¹ãƒˆ:')
    long_win_rate = test_trained_model(QuantumBattleAI_6Qubits, model_filename, '6_extended')
    
    print(f'\nğŸ“ˆ æ”¹å–„çµæœ:')
    print(f'çŸ­æœŸå­¦ç¿’: {short_win_rate:.1f}% â†’ é•·æœŸå­¦ç¿’: {long_win_rate:.1f}%')
    improvement = long_win_rate - short_win_rate
    print(f'æ”¹å–„åº¦: {improvement:+.1f}%')
    
except Exception as e:
    print(f'âŒ ã‚¨ãƒ©ãƒ¼: {e}')
    import traceback
    traceback.print_exc()

print('\nâœ… é•·æœŸå­¦ç¿’ãƒ—ãƒ­ã‚»ã‚¹å®Œäº†')