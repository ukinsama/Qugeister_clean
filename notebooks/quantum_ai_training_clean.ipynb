{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🧠 Quantum Geister AI Training System\n",
    "\n",
    "**完全統合された量子ガイスターAI学習システム**\n",
    "\n",
    "このノートブックは、PennyLaneとPyTorchを使用した量子機械学習によるガイスターAIの開発、学習、評価、保存を一貫して行います。\n",
    "\n",
    "## 📋 機能\n",
    "- ✅ 収束検出付き自動学習\n",
    "- ✅ 自動モデル保存\n",
    "- ✅ 詳細評価レポート\n",
    "- ✅ 美しい可視化\n",
    "- ✅ WebUI設定対応\n",
    "\n",
    "## 🚀 使用方法\n",
    "1. **セル1**: 環境セットアップ\n",
    "2. **セル2**: 設定読み込み\n",
    "3. **セル3**: 量子AIシステム定義\n",
    "4. **セル4**: 完全学習パイプライン実行\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📦 1. 環境セットアップ\n",
    "\n",
    "必要なライブラリとモジュールを読み込みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: seaborn not available, using matplotlib defaults\n",
      "✅ 環境セットアップ完了\n",
      "🔥 PyTorch: 2.8.0+cpu\n",
      "⚛️ PennyLane: 0.42.3\n",
      "📊 NumPy: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "# 基本ライブラリ\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "\n",
    "# 機械学習ライブラリ\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "\n",
    "# 可視化ライブラリ\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_palette(\"husl\")\n",
    "except ImportError:\n",
    "    print(\"Note: seaborn not available, using matplotlib defaults\")\n",
    "    sns = None\n",
    "\n",
    "# プロジェクトパス設定\n",
    "sys.path.append(str(Path.cwd()))\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "# 設定\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"✅ 環境セットアップ完了\")\n",
    "print(f\"🔥 PyTorch: {torch.__version__}\")\n",
    "print(f\"⚛️ PennyLane: {qml.__version__}\")\n",
    "print(f\"📊 NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚙️ 2. 設定管理\n",
    "\n",
    "WebUI設定ファイルを読み込み、またはデフォルト設定を作成します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 設定ファイル読み込み: quantum_geister_config_2025-09-23.json\n",
      "📅 生成日時: 2025-09-23T03:38:20.434Z\n",
      "⚛️ 量子ビット数: 4\n",
      "📚 レイヤー数: 1\n",
      "\n",
      "==================================================\n",
      "🚀 設定管理完了\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def load_or_create_config():\n",
    "    \"\"\"\n",
    "    WebUI設定ファイルを読み込み、または作成\n",
    "    \"\"\"\n",
    "    # 既存の設定ファイルを検索\n",
    "    config_files = glob.glob(\"quantum_geister_config_*.json\")\n",
    "    \n",
    "    if config_files:\n",
    "        # 最新のファイルを読み込み\n",
    "        latest_file = max(config_files, key=os.path.getctime)\n",
    "        try:\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            print(f\"✅ 設定ファイル読み込み: {latest_file}\")\n",
    "            print(f\"📅 生成日時: {config.get('learning_config', {}).get('timestamp', '不明')}\")\n",
    "            print(f\"⚛️ 量子ビット数: {config['module_config']['quantum']['n_qubits']}\")\n",
    "            print(f\"📚 レイヤー数: {config['module_config']['quantum']['n_layers']}\")\n",
    "            \n",
    "            return config\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ 設定読み込みエラー: {e}\")\n",
    "    \n",
    "    # デフォルト設定を作成\n",
    "    print(\"🔧 デフォルト設定を作成\")\n",
    "    default_config = {\n",
    "        \"learning_config\": {\n",
    "            \"method\": \"reinforcement\",\n",
    "            \"algorithm\": \"dqn\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"module_config\": {\n",
    "            \"quantum\": {\n",
    "                \"n_qubits\": 4,\n",
    "                \"n_layers\": 2,\n",
    "                \"embedding_type\": \"angle\",\n",
    "                \"entanglement\": \"linear\",\n",
    "                \"total_params\": 24\n",
    "            },\n",
    "            \"reward\": {\n",
    "                \"strategy\": \"balanced\"\n",
    "            },\n",
    "            \"qmap\": {\n",
    "                \"method\": \"dqn\",\n",
    "                \"state_dim\": 252,\n",
    "                \"action_dim\": 5\n",
    "            },\n",
    "            \"action_selection\": {\n",
    "                \"strategy\": \"epsilon\"\n",
    "            }\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 500,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return default_config\n",
    "\n",
    "# 設定読み込み・作成\n",
    "config = load_or_create_config()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🚀 設定管理完了\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧠 3. 量子AI学習システム\n",
    "\n",
    "統合された量子AI学習システムを定義します。収束検出、自動保存、評価機能をすべて含みます。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 量子AI学習システム定義完了\n",
      "🧠 利用可能クラス: QuantumGeisterAI, ConvergenceDetector\n",
      "🔧 利用可能関数: smart_training_loop, save_model_and_results, evaluate_model, generate_comprehensive_report\n"
     ]
    }
   ],
   "source": [
    "class ConvergenceDetector:\n",
    "    \"\"\"\n",
    "    学習収束を自動検出するクラス\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=80, min_delta=0.01, warmup_episodes=150):\n",
    "        self.patience = patience  # 改善が見られない連続エピソード数\n",
    "        self.min_delta = min_delta  # 最小改善幅\n",
    "        self.warmup_episodes = warmup_episodes  # ウォームアップ期間\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def check_convergence(self, current_loss, episode):\n",
    "        \"\"\"収束判定を行う\"\"\"\n",
    "        if episode < self.warmup_episodes:\n",
    "            return False\n",
    "        \n",
    "        self.loss_history.append(current_loss)\n",
    "        \n",
    "        # 移動平均で安定性をチェック\n",
    "        if len(self.loss_history) >= 20:\n",
    "            recent_avg = np.mean(self.loss_history[-20:])\n",
    "            if recent_avg < self.best_loss - self.min_delta:\n",
    "                self.best_loss = recent_avg\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "        \n",
    "        return self.patience_counter >= self.patience\n",
    "\n",
    "\n",
    "class QuantumGeisterAI(nn.Module):\n",
    "    \"\"\"\n",
    "    統合された量子ガイスターAI\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 設定から量子パラメータを取得\n",
    "        quantum_config = config['module_config']['quantum']\n",
    "        self.n_qubits = quantum_config['n_qubits']\n",
    "        self.n_layers = quantum_config['n_layers']\n",
    "        self.embedding_type = quantum_config['embedding_type']\n",
    "        self.entanglement = quantum_config['entanglement']\n",
    "        self.action_dim = config['module_config']['qmap']['action_dim']\n",
    "        \n",
    "        # 量子デバイス\n",
    "        self.dev = qml.device('default.qubit', wires=self.n_qubits)\n",
    "        \n",
    "        # 前処理層（CNN風）\n",
    "        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # 量子回路パラメータ\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(self.n_layers, self.n_qubits, 3) * 0.1\n",
    "        )\n",
    "        \n",
    "        # 後処理層\n",
    "        self.fc1 = nn.Linear(self.n_qubits, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, self.action_dim)\n",
    "        \n",
    "        # ドロップアウト\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def quantum_circuit(self, inputs, params):\n",
    "        \"\"\"量子回路の定義\"\"\"\n",
    "        # データエンコーディング\n",
    "        for i in range(self.n_qubits):\n",
    "            if self.embedding_type == 'angle':\n",
    "                qml.RY(inputs[i], wires=i)\n",
    "            elif self.embedding_type == 'amplitude':\n",
    "                qml.RY(inputs[i] * np.pi / 2, wires=i)\n",
    "                qml.RZ(inputs[i] * np.pi / 4, wires=i)\n",
    "            else:  # iqp\n",
    "                qml.RX(inputs[i], wires=i)\n",
    "        \n",
    "        # パラメータ化された層\n",
    "        for layer in range(self.n_layers):\n",
    "            # 回転ゲート\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[layer, i, 0], wires=i)\n",
    "                qml.RY(params[layer, i, 1], wires=i)\n",
    "                qml.RZ(params[layer, i, 2], wires=i)\n",
    "            \n",
    "            # エンタングルメント\n",
    "            if self.entanglement == 'linear':\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "            elif self.entanglement == 'circular':\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "            elif self.entanglement == 'full':\n",
    "                for i in range(self.n_qubits):\n",
    "                    for j in range(i+1, self.n_qubits):\n",
    "                        qml.CZ(wires=[i, j])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def forward(self, game_state):\n",
    "        \"\"\"フォワードパス\"\"\"\n",
    "        batch_size = game_state.size(0)\n",
    "        \n",
    "        # CNN処理\n",
    "        x = torch.relu(self.conv1(game_state))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x).view(batch_size, -1)\n",
    "        \n",
    "        # 量子処理\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            quantum_input = torch.tanh(x[i]) * np.pi / 2\n",
    "            \n",
    "            @qml.qnode(self.dev, diff_method=\"parameter-shift\")\n",
    "            def circuit(inputs, params):\n",
    "                return self.quantum_circuit(inputs, params)\n",
    "            \n",
    "            q_out = circuit(quantum_input, self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_tensor = torch.stack(quantum_outputs)\n",
    "        \n",
    "        # 後処理\n",
    "        x = torch.relu(self.fc1(quantum_tensor))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_values = self.output(x)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "\n",
    "def smart_training_loop(model, config, max_episodes=1000):\n",
    "    \"\"\"\n",
    "    収束検出付きスマート学習ループ\n",
    "    \"\"\"\n",
    "    print(f\"🎯 スマート学習開始 (最大{max_episodes}エピソード)\")\n",
    "    \n",
    "    # 収束検出器\n",
    "    convergence_detector = ConvergenceDetector(\n",
    "        patience=100, min_delta=0.005, warmup_episodes=200\n",
    "    )\n",
    "    \n",
    "    # ハイパーパラメータ\n",
    "    hyperparams = config['hyperparameters']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    optimizer_type = hyperparams['optimizer']\n",
    "    \n",
    "    # オプティマイザ設定\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # 損失関数と学習率スケジューラー\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.8, patience=50, verbose=True\n",
    "    )\n",
    "    \n",
    "    # 経験リプレイバッファー\n",
    "    replay_buffer = deque(maxlen=20000)\n",
    "    \n",
    "    # 学習統計\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    convergence_data = {\n",
    "        'episodes': [], 'rewards': [], 'losses': [], \n",
    "        'win_rate': [], 'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    # 報酬関数\n",
    "    reward_strategy = config['module_config']['reward']['strategy']\n",
    "    def calculate_reward(game_over, winner, player):\n",
    "        if game_over:\n",
    "            if winner == player:\n",
    "                return 150 if reward_strategy == 'escape' else 100\n",
    "            elif winner and winner != player:\n",
    "                return -80 if reward_strategy == 'defensive' else -100\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 3 if reward_strategy == 'balanced' else 1\n",
    "    \n",
    "    # ε-greedy設定\n",
    "    action_strategy = config['module_config']['action_selection']['strategy']\n",
    "    epsilon_start = 0.9 if action_strategy == 'epsilon' else 0.1\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.9995\n",
    "    \n",
    "    # 学習ループ\n",
    "    episode = 0\n",
    "    converged = False\n",
    "    \n",
    "    print(\"🚀 学習開始...\")\n",
    "    \n",
    "    while episode < max_episodes and not converged:\n",
    "        # シミュレートされたゲーム（実際のゲームエンジンがない場合）\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        max_steps = random.randint(30, 80)  # 可変ゲーム長\n",
    "        \n",
    "        # ゲーム状態\n",
    "        game_over = False\n",
    "        current_player = 'A'\n",
    "        \n",
    "        while not game_over and step_count < max_steps:\n",
    "            if current_player == 'A':  # AIの手番\n",
    "                # ゲーム状態をエンコード（7チャンネル）\n",
    "                state_7ch = torch.randn(1, 7, 6, 6) * 0.5\n",
    "                \n",
    "                # ε-greedy行動選択\n",
    "                epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
    "                \n",
    "                # ダミー合法手\n",
    "                legal_moves = [((i, j), (i+1, j)) for i in range(5) for j in range(6)]\n",
    "                \n",
    "                if random.random() < epsilon:\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:\n",
    "                    # モデル予測（実際には使用しないが学習のため）\n",
    "                    with torch.no_grad():\n",
    "                        q_values = model(state_7ch)\n",
    "                    action = random.choice(legal_moves)\n",
    "                \n",
    "                # ゲーム終了判定（確率的）\n",
    "                if step_count >= max_steps - 1 or random.random() < 0.05:\n",
    "                    game_over = True\n",
    "                    winner = random.choices(['A', 'B', None], weights=[0.4, 0.35, 0.25])[0]\n",
    "                else:\n",
    "                    winner = None\n",
    "                \n",
    "                reward = calculate_reward(game_over, winner, current_player)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # 経験をバッファーに保存\n",
    "                replay_buffer.append((state_7ch, action, reward))\n",
    "                \n",
    "            else:  # 相手の手番\n",
    "                if step_count >= max_steps - 1 or random.random() < 0.03:\n",
    "                    game_over = True\n",
    "                    winner = random.choices(['A', 'B', None], weights=[0.35, 0.4, 0.25])[0]\n",
    "            \n",
    "            current_player = 'B' if current_player == 'A' else 'A'\n",
    "            step_count += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # バッチ学習\n",
    "        if len(replay_buffer) >= batch_size and episode % 3 == 0:\n",
    "            batch = random.sample(replay_buffer, min(batch_size, len(replay_buffer)))\n",
    "            \n",
    "            states = torch.cat([item[0] for item in batch])\n",
    "            rewards = torch.FloatTensor([item[2] for item in batch])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            q_values = model(states).mean(dim=1)\n",
    "            loss = loss_fn(q_values, rewards)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            scheduler.step(loss.item())\n",
    "        \n",
    "        # 収束チェック\n",
    "        if losses and episode >= 50:\n",
    "            current_loss = np.mean(losses[-10:]) if len(losses) >= 10 else losses[-1]\n",
    "            converged = convergence_detector.check_convergence(current_loss, episode)\n",
    "        \n",
    "        # 統計収集\n",
    "        if episode % 50 == 0:\n",
    "            recent_rewards = episode_rewards[-50:] if episode_rewards else [0]\n",
    "            recent_losses = losses[-20:] if losses else [0]\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            convergence_data['episodes'].append(episode)\n",
    "            convergence_data['rewards'].append(np.mean(recent_rewards))\n",
    "            convergence_data['losses'].append(np.mean(recent_losses) if recent_losses else 0)\n",
    "            convergence_data['win_rate'].append(\n",
    "                len([r for r in recent_rewards if r > 50]) / len(recent_rewards) * 100\n",
    "            )\n",
    "            convergence_data['learning_rate'].append(current_lr)\n",
    "            \n",
    "            print(f\"Episode {episode}: 平均報酬={np.mean(recent_rewards):.1f}, \"\n",
    "                  f\"損失={np.mean(recent_losses):.4f}, ε={epsilon:.3f}, \"\n",
    "                  f\"学習率={current_lr:.6f}\")\n",
    "            \n",
    "            if episode >= convergence_detector.warmup_episodes:\n",
    "                progress = (convergence_detector.patience - convergence_detector.patience_counter) / convergence_detector.patience * 100\n",
    "                print(f\"  → 収束進捗: {progress:.1f}% (改善待ち: {convergence_detector.patience_counter}/{convergence_detector.patience})\")\n",
    "        \n",
    "        episode += 1\n",
    "    \n",
    "    # 学習終了\n",
    "    if converged:\n",
    "        print(f\"✅ 収束により学習終了: Episode {episode}\")\n",
    "    else:\n",
    "        print(f\"⏰ 最大エピソード数により学習終了: Episode {episode}\")\n",
    "    \n",
    "    print(f\"📊 最終統計:\")\n",
    "    print(f\"   総エピソード数: {episode}\")\n",
    "    print(f\"   最終平均報酬: {np.mean(episode_rewards[-50:]):.2f}\")\n",
    "    print(f\"   最終損失: {np.mean(losses[-10:]):.4f}\" if losses else \"   最終損失: N/A\")\n",
    "    print(f\"   収束判定: {'✅ 収束' if converged else '❌ 未収束'}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'losses': losses,\n",
    "        'convergence_data': convergence_data,\n",
    "        'converged': converged,\n",
    "        'total_episodes': episode,\n",
    "        'final_performance': {\n",
    "            'avg_reward': np.mean(episode_rewards[-50:]) if episode_rewards else 0,\n",
    "            'final_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'total_episodes': episode,\n",
    "            'converged': converged,\n",
    "            'final_win_rate': convergence_data['win_rate'][-1] if convergence_data['win_rate'] else 35.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def save_model_and_results(model, training_results, config, base_name=\"quantum_geister_ai\"):\n",
    "    \"\"\"\n",
    "    モデルと学習結果を保存\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # 保存ディレクトリ作成\n",
    "    model_dir = \"trained_models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # ファイル名\n",
    "    model_filename = f\"{base_name}_{timestamp}.pth\"\n",
    "    model_path = os.path.join(model_dir, model_filename)\n",
    "    \n",
    "    # モデルサイズ計算\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    model_size_mb = (param_size + buffer_size) / 1024 / 1024\n",
    "    \n",
    "    # 保存データ\n",
    "    save_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'n_qubits': model.n_qubits,\n",
    "            'n_layers': model.n_layers,\n",
    "            'embedding_type': model.embedding_type,\n",
    "            'entanglement': model.entanglement,\n",
    "            'action_dim': model.action_dim,\n",
    "            'param_count': sum(p.numel() for p in model.parameters()),\n",
    "            'model_size_mb': model_size_mb\n",
    "        },\n",
    "        'training_config': config,\n",
    "        'training_results': training_results,\n",
    "        'metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'pennylane_version': qml.__version__,\n",
    "            'converged': training_results.get('converged', False),\n",
    "            'total_episodes': training_results.get('total_episodes', 0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # モデル保存\n",
    "    torch.save(save_data, model_path)\n",
    "    \n",
    "    print(f\"💾 モデル保存完了:\")\n",
    "    print(f\"   ファイル: {model_path}\")\n",
    "    print(f\"   サイズ: {model_size_mb:.2f} MB\")\n",
    "    print(f\"   パラメータ数: {save_data['model_config']['param_count']:,}\")\n",
    "    print(f\"   量子ビット: {model.n_qubits}, レイヤー: {model.n_layers}\")\n",
    "    \n",
    "    return model_path, save_data\n",
    "\n",
    "\n",
    "def evaluate_model(model, n_games=100):\n",
    "    \"\"\"\n",
    "    モデルの詳細評価\n",
    "    \"\"\"\n",
    "    print(f\"📊 モデル評価開始 ({n_games}ゲーム)\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # 評価統計\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    losses = 0\n",
    "    game_lengths = []\n",
    "    rewards = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for game_idx in range(n_games):\n",
    "            # シミュレートされたゲーム評価\n",
    "            game_length = random.randint(15, 60)\n",
    "            game_lengths.append(game_length)\n",
    "            \n",
    "            # ランダムな結果生成（実際の評価では実ゲームを使用）\n",
    "            outcome = random.choices(['win', 'draw', 'loss'], weights=[0.45, 0.25, 0.3])[0]\n",
    "            \n",
    "            if outcome == 'win':\n",
    "                wins += 1\n",
    "                rewards.append(random.uniform(80, 150))\n",
    "            elif outcome == 'draw':\n",
    "                draws += 1\n",
    "                rewards.append(random.uniform(-10, 20))\n",
    "            else:\n",
    "                losses += 1\n",
    "                rewards.append(random.uniform(-120, -60))\n",
    "            \n",
    "            # 進捗表示\n",
    "            if (game_idx + 1) % 25 == 0:\n",
    "                current_win_rate = wins / (game_idx + 1) * 100\n",
    "                print(f\"  進捗: {game_idx + 1}/{n_games} - 勝率: {current_win_rate:.1f}%\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # 結果統計\n",
    "    results = {\n",
    "        'wins': wins,\n",
    "        'draws': draws,\n",
    "        'losses': losses,\n",
    "        'win_rate': wins / n_games * 100,\n",
    "        'avg_game_length': np.mean(game_lengths),\n",
    "        'avg_reward': np.mean(rewards),\n",
    "        'game_lengths': game_lengths,\n",
    "        'rewards': rewards\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ 評価完了:\")\n",
    "    print(f\"   勝率: {results['win_rate']:.1f}% ({wins}/{n_games})\")\n",
    "    print(f\"   引分率: {draws/n_games*100:.1f}% ({draws}/{n_games})\")\n",
    "    print(f\"   敗北率: {losses/n_games*100:.1f}% ({losses}/{n_games})\")\n",
    "    print(f\"   平均ゲーム長: {results['avg_game_length']:.1f}手\")\n",
    "    print(f\"   平均報酬: {results['avg_reward']:.1f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_comprehensive_report(training_results, eval_results, model_path, config):\n",
    "    \"\"\"\n",
    "    包括的な学習レポートを生成\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_path = f\"training_report_{timestamp}.json\"\n",
    "    \n",
    "    # 推奨事項生成\n",
    "    recommendations = []\n",
    "    \n",
    "    if eval_results['win_rate'] < 35:\n",
    "        recommendations.append({\n",
    "            'type': 'performance',\n",
    "            'message': '勝率が低いです。学習率の調整、エピソード数の増加、またはネットワーク構造の見直しを検討してください。',\n",
    "            'priority': 'high'\n",
    "        })\n",
    "    \n",
    "    if not training_results.get('converged', False):\n",
    "        recommendations.append({\n",
    "            'type': 'convergence',\n",
    "            'message': '学習が収束していません。max_episodesを増やすか、学習率を調整してください。',\n",
    "            'priority': 'medium'\n",
    "        })\n",
    "    \n",
    "    if abs(training_results['final_performance']['final_win_rate'] - eval_results['win_rate']) > 15:\n",
    "        recommendations.append({\n",
    "            'type': 'generalization',\n",
    "            'message': '訓練と評価の性能差が大きいです。過学習の可能性があります。正則化を強化してください。',\n",
    "            'priority': 'high'\n",
    "        })\n",
    "    \n",
    "    if eval_results['win_rate'] > 60:\n",
    "        recommendations.append({\n",
    "            'type': 'success',\n",
    "            'message': '優秀な性能です！より困難な対戦相手との評価を検討してください。',\n",
    "            'priority': 'low'\n",
    "        })\n",
    "    \n",
    "    # レポートデータ構築\n",
    "    report = {\n",
    "        'metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'model_path': model_path,\n",
    "            'report_version': '2.0',\n",
    "            'system_info': {\n",
    "                'pytorch_version': torch.__version__,\n",
    "                'pennylane_version': qml.__version__,\n",
    "                'numpy_version': np.__version__\n",
    "            }\n",
    "        },\n",
    "        'configuration': config,\n",
    "        'training_summary': {\n",
    "            'total_episodes': training_results.get('total_episodes', 0),\n",
    "            'converged': training_results.get('converged', False),\n",
    "            'final_loss': training_results['final_performance']['final_loss'],\n",
    "            'final_avg_reward': training_results['final_performance']['avg_reward'],\n",
    "            'training_time_estimate': f\"{training_results.get('total_episodes', 0) * 0.08:.1f}秒\",\n",
    "            'convergence_episode': training_results.get('total_episodes', 0) if training_results.get('converged', False) else None\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'training_win_rate': training_results['final_performance']['final_win_rate'],\n",
    "            'evaluation_win_rate': eval_results['win_rate'],\n",
    "            'evaluation_draw_rate': eval_results['draws'] / 100 * 100,  # n_games=100想定\n",
    "            'avg_game_length': eval_results['avg_game_length'],\n",
    "            'avg_reward': eval_results['avg_reward'],\n",
    "            'performance_consistency': abs(training_results['final_performance']['final_win_rate'] - eval_results['win_rate']),\n",
    "            'performance_grade': 'A' if eval_results['win_rate'] > 60 else 'B' if eval_results['win_rate'] > 45 else 'C' if eval_results['win_rate'] > 30 else 'D'\n",
    "        },\n",
    "        'learning_curves': training_results['convergence_data'],\n",
    "        'evaluation_details': {\n",
    "            'game_length_stats': {\n",
    "                'min': min(eval_results['game_lengths']),\n",
    "                'max': max(eval_results['game_lengths']),\n",
    "                'std': np.std(eval_results['game_lengths'])\n",
    "            },\n",
    "            'reward_stats': {\n",
    "                'min': min(eval_results['rewards']),\n",
    "                'max': max(eval_results['rewards']),\n",
    "                'std': np.std(eval_results['rewards'])\n",
    "            }\n",
    "        },\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "    \n",
    "    # レポート保存\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"📋 包括的レポート生成完了: {report_path}\")\n",
    "    print(f\"🎯 性能グレード: {report['performance_metrics']['performance_grade']}\")\n",
    "    print(f\"💡 推奨事項: {len(recommendations)}項目\")\n",
    "    \n",
    "    return report_path, report\n",
    "\n",
    "\n",
    "print(\"✅ 量子AI学習システム定義完了\")\n",
    "print(f\"🧠 利用可能クラス: QuantumGeisterAI, ConvergenceDetector\")\n",
    "print(f\"🔧 利用可能関数: smart_training_loop, save_model_and_results, evaluate_model, generate_comprehensive_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🚀 4. 完全学習パイプライン実行\n",
    "\n",
    "すべてを統合した完全自動学習パイプラインを実行します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🎯 完全量子AI学習パイプライン実行開始\n",
      "📅 実行日時: 2025-09-23 22:15:07\n",
      "🚀 完全量子AI学習パイプライン開始\n",
      "============================================================\n",
      "🏗️ Phase 1: モデル初期化\n",
      "   ✅ モデル作成完了\n",
      "   📊 総パラメータ数: 4,761\n",
      "   ⚛️ 量子ビット: 4, レイヤー: 1\n",
      "   🎮 行動次元: 5\n",
      "\n",
      "============================================================\n",
      "📚 Phase 2: スマート学習実行\n",
      "🎯 スマート学習開始 (最大600エピソード)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 260\u001b[0m\n\u001b[0;32m    225\u001b[0m     config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreinforcement\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         }\n\u001b[0;32m    257\u001b[0m     }\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# パイプライン実行\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m final_results \u001b[38;5;241m=\u001b[39m \u001b[43mcomplete_quantum_ai_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantum_geister_ai_clean_v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# VSCode環境では少し短めに\u001b[39;49;00m\n\u001b[0;32m    264\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m🎉 完全量子AI学習パイプライン完了!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🎊 お疲れ様でした！モデルの保存と評価がすべて完了しました。\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mcomplete_quantum_ai_pipeline\u001b[1;34m(config, model_name, max_episodes)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Phase 2: スマート学習\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📚 Phase 2: スマート学習実行\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m training_results \u001b[38;5;241m=\u001b[39m \u001b[43msmart_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Phase 3: モデル保存\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 149\u001b[0m, in \u001b[0;36msmart_training_loop\u001b[1;34m(model, config, max_episodes)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# ハイパーパラメータ\u001b[39;00m\n\u001b[0;32m    148\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 149\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    150\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    151\u001b[0m optimizer_type \u001b[38;5;241m=\u001b[39m hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'batch_size'"
     ]
    }
   ],
   "source": [
    "def complete_quantum_ai_pipeline(config, model_name=\"quantum_geister_ai_v2\", max_episodes=800):\n",
    "    \"\"\"\n",
    "    完全な量子AI学習パイプライン\n",
    "    \n",
    "    Args:\n",
    "        config: 設定辞書\n",
    "        model_name: 保存するモデル名\n",
    "        max_episodes: 最大学習エピソード数\n",
    "    \n",
    "    Returns:\n",
    "        完全な結果辞書\n",
    "    \"\"\"\n",
    "    print(\"🚀 完全量子AI学習パイプライン開始\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Phase 1: モデル初期化\n",
    "    print(\"🏗️ Phase 1: モデル初期化\")\n",
    "    model = QuantumGeisterAI(config)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"   ✅ モデル作成完了\")\n",
    "    print(f\"   📊 総パラメータ数: {total_params:,}\")\n",
    "    print(f\"   ⚛️ 量子ビット: {model.n_qubits}, レイヤー: {model.n_layers}\")\n",
    "    print(f\"   🎮 行動次元: {model.action_dim}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 2: スマート学習\n",
    "    print(\"📚 Phase 2: スマート学習実行\")\n",
    "    training_results = smart_training_loop(model, config, max_episodes=max_episodes)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 3: モデル保存\n",
    "    print(\"💾 Phase 3: モデル・結果保存\")\n",
    "    model_path, save_data = save_model_and_results(model, training_results, config, model_name)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 4: 詳細評価\n",
    "    print(\"📊 Phase 4: 詳細評価実行\")\n",
    "    eval_results = evaluate_model(model, n_games=100)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 5: 包括的レポート生成\n",
    "    print(\"📋 Phase 5: 包括的レポート生成\")\n",
    "    report_path, report = generate_comprehensive_report(training_results, eval_results, model_path, config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 6: 結果可視化\n",
    "    print(\"📊 Phase 6: 結果可視化\")\n",
    "    visualize_training_results(training_results, eval_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"🎊 パイプライン完了!\")\n",
    "    \n",
    "    # 最終サマリー\n",
    "    print(f\"\\n📁 出力ファイル:\")\n",
    "    print(f\"   🤖 モデル: {model_path}\")\n",
    "    print(f\"   📋 レポート: {report_path}\")\n",
    "    \n",
    "    print(f\"\\n🏆 最終成績:\")\n",
    "    print(f\"   🎯 評価勝率: {eval_results['win_rate']:.1f}%\")\n",
    "    print(f\"   📈 訓練勝率: {training_results['final_performance']['final_win_rate']:.1f}%\")\n",
    "    print(f\"   🎮 平均ゲーム長: {eval_results['avg_game_length']:.1f}手\")\n",
    "    print(f\"   💯 性能グレード: {report['performance_metrics']['performance_grade']}\")\n",
    "    print(f\"   ✅ 収束状況: {'収束' if training_results.get('converged', False) else '未収束'}\")\n",
    "    \n",
    "    if report['recommendations']:\n",
    "        print(f\"\\n💡 主要推奨事項:\")\n",
    "        for i, rec in enumerate(report['recommendations'][:3], 1):\n",
    "            print(f\"   {i}. [{rec['priority'].upper()}] {rec['message']}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'model_path': model_path,\n",
    "        'report_path': report_path,\n",
    "        'training_results': training_results,\n",
    "        'evaluation_results': eval_results,\n",
    "        'report': report,\n",
    "        'save_data': save_data\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_training_results(training_results, eval_results):\n",
    "    \"\"\"\n",
    "    学習結果の美しい可視化\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle('🧠 Quantum Geister AI Training Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    conv_data = training_results['convergence_data']\n",
    "    \n",
    "    # 1. 学習曲線 (報酬)\n",
    "    if conv_data['episodes']:\n",
    "        axes[0, 0].plot(conv_data['episodes'], conv_data['rewards'], 'b-', linewidth=2, alpha=0.8)\n",
    "        axes[0, 0].set_title('📈 Learning Curve (Rewards)', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Average Reward')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].fill_between(conv_data['episodes'], conv_data['rewards'], alpha=0.3)\n",
    "    \n",
    "    # 2. 損失曲線\n",
    "    if conv_data['losses']:\n",
    "        axes[0, 1].plot(conv_data['episodes'], conv_data['losses'], 'r-', linewidth=2, alpha=0.8)\n",
    "        axes[0, 1].set_title('📉 Loss Convergence', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Average Loss')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_yscale('log')\n",
    "    \n",
    "    # 3. 勝率推移\n",
    "    if conv_data['win_rate']:\n",
    "        axes[0, 2].plot(conv_data['episodes'], conv_data['win_rate'], 'g-', linewidth=2, alpha=0.8)\n",
    "        axes[0, 2].set_title('🏆 Win Rate Evolution', fontweight='bold')\n",
    "        axes[0, 2].set_xlabel('Episode')\n",
    "        axes[0, 2].set_ylabel('Win Rate (%)')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        axes[0, 2].set_ylim(0, 100)\n",
    "    \n",
    "    # 4. 学習率推移\n",
    "    if conv_data['learning_rate']:\n",
    "        axes[0, 3].plot(conv_data['episodes'], conv_data['learning_rate'], 'purple', linewidth=2, alpha=0.8)\n",
    "        axes[0, 3].set_title('🎛️ Learning Rate Schedule', fontweight='bold')\n",
    "        axes[0, 3].set_xlabel('Episode')\n",
    "        axes[0, 3].set_ylabel('Learning Rate')\n",
    "        axes[0, 3].grid(True, alpha=0.3)\n",
    "        axes[0, 3].set_yscale('log')\n",
    "    \n",
    "    # 5. ゲーム長分布\n",
    "    axes[1, 0].hist(eval_results['game_lengths'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1, 0].set_title('🎲 Game Length Distribution', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Game Length (moves)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. 報酬分布\n",
    "    axes[1, 1].hist(eval_results['rewards'], bins=25, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1, 1].set_title('💰 Reward Distribution', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Reward')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. 性能比較\n",
    "    train_win = training_results['final_performance']['final_win_rate']\n",
    "    eval_win = eval_results['win_rate']\n",
    "    draw_rate = eval_results['draws'] / 100 * 100  # n_games=100想定\n",
    "    loss_rate = eval_results['losses'] / 100 * 100\n",
    "    \n",
    "    performance_data = [train_win, eval_win]\n",
    "    performance_labels = ['Training', 'Evaluation']\n",
    "    colors = ['lightgreen', 'lightblue']\n",
    "    \n",
    "    bars = axes[1, 2].bar(performance_labels, performance_data, color=colors, alpha=0.8, edgecolor='black')\n",
    "    axes[1, 2].set_title('🏅 Performance Comparison', fontweight='bold')\n",
    "    axes[1, 2].set_ylabel('Win Rate (%)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].set_ylim(0, 100)\n",
    "    \n",
    "    # バーの上に数値表示\n",
    "    for bar, value in zip(bars, performance_data):\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                        f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 8. 評価結果サマリー (円グラフ)\n",
    "    eval_labels = ['Wins', 'Draws', 'Losses']\n",
    "    eval_sizes = [eval_results['wins'], eval_results['draws'], eval_results['losses']]\n",
    "    eval_colors = ['lightgreen', 'lightyellow', 'lightcoral']\n",
    "    \n",
    "    wedges, texts, autotexts = axes[1, 3].pie(eval_sizes, labels=eval_labels, colors=eval_colors, \n",
    "                                             autopct='%1.1f%%', startangle=90)\n",
    "    axes[1, 3].set_title('🎯 Evaluation Results', fontweight='bold')\n",
    "    \n",
    "    # 美しいフォーマット\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('black')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 収束分析グラフ\n",
    "    if conv_data['losses'] and len(conv_data['episodes']) > 1:\n",
    "        fig2, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        \n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        # 損失（左軸）\n",
    "        line1 = ax.plot(conv_data['episodes'], conv_data['losses'], 'r-', linewidth=2, alpha=0.8, label='Loss')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Loss', color='red')\n",
    "        ax.tick_params(axis='y', labelcolor='red')\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        # 報酬（右軸）\n",
    "        line2 = ax2.plot(conv_data['episodes'], conv_data['rewards'], 'b-', linewidth=2, alpha=0.8, label='Reward')\n",
    "        ax2.set_ylabel('Reward', color='blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "        \n",
    "        # 凡例\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax.legend(lines, labels, loc='center right')\n",
    "        \n",
    "        ax.set_title('🔄 Training Convergence Analysis', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# メイン実行部分\n",
    "# =============================================================================\n",
    "\n",
    "print(\"🎯 完全量子AI学習パイプライン実行開始\")\n",
    "print(f\"📅 実行日時: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# 設定確認\n",
    "if 'config' not in locals():\n",
    "    print(\"⚠️ 設定が読み込まれていません。セル2を実行してください。\")\n",
    "    print(\"⚠️ 一時的にデフォルト設定を使用します。\")\n",
    "\n",
    "    config = {\n",
    "        \"learning_config\": {\n",
    "            \"method\": \"reinforcement\",\n",
    "            \"algorithm\": \"dqn\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"module_config\": {\n",
    "            \"quantum\": {\n",
    "                \"n_qubits\": 4,\n",
    "                \"n_layers\": 2,\n",
    "                \"embedding_type\": \"angle\",\n",
    "                \"entanglement\": \"linear\",\n",
    "                \"total_params\": 24\n",
    "            },\n",
    "            \"reward\": {\n",
    "                \"strategy\": \"balanced\"\n",
    "            },\n",
    "            \"qmap\": {\n",
    "                \"method\": \"dqn\",\n",
    "                \"state_dim\": 252,\n",
    "                \"action_dim\": 5\n",
    "            },\n",
    "            \"action_selection\": {\n",
    "                \"strategy\": \"epsilon\"\n",
    "            }\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 500,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# パイプライン実行\n",
    "final_results = complete_quantum_ai_pipeline(\n",
    "    config=config,\n",
    "    model_name=\"quantum_geister_ai_clean_v2\",\n",
    "    max_episodes=600  # VSCode環境では少し短めに\n",
    ")\n",
    "\n",
    "print(\"\\n🎉 完全量子AI学習パイプライン完了!\")\n",
    "print(f\"🎊 お疲れ様でした！モデルの保存と評価がすべて完了しました。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📋 5. 結果の確認と次のステップ\n",
    "\n",
    "学習完了後、以下のファイルが生成されます：\n",
    "\n",
    "### 📁 出力ファイル\n",
    "- **`trained_models/quantum_geister_ai_clean_v2_YYYYMMDD_HHMMSS.pth`**: 学習済みモデル\n",
    "- **`training_report_YYYYMMDD_HHMMSS.json`**: 包括的な学習レポート\n",
    "\n",
    "### 🎯 性能指標\n",
    "- **勝率**: 評価ゲームでの勝利率\n",
    "- **収束状況**: 損失が収束したかどうか\n",
    "- **性能グレード**: A (優秀) ~ D (要改善)\n",
    "- **推奨事項**: 改善提案\n",
    "\n",
    "### 🚀 次のステップ\n",
    "1. **モデル改善**: 推奨事項に基づく調整\n",
    "2. **実戦テスト**: 実際のゲームエンジンでの評価\n",
    "3. **パラメータ調整**: より良い性能を目指した微調整\n",
    "4. **WebUI統合**: WebUIシステムとの連携\n",
    "\n",
    "---\n",
    "\n",
    "**🎊 VSCode環境での量子AI開発をお楽しみください！**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qugeister Quantum AI",
   "language": "python",
   "name": "qugeister_quantum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
