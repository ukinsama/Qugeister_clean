{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 本格CQCNN量子強化学習実験\n",
    "## JSON Config完全対応版 - 収束まで継続学習\n",
    "\n",
    "- **設定ファイル**: quantum_cqcnn_config_2025-09-24.json\n",
    "- **量子構成**: 4Q1L (4量子ビット, 1レイヤー)\n",
    "- **収束閾値**: 0.95 (Balance)\n",
    "- **最大エピソード**: 50,000\n",
    "- **アーキテクチャ**: 完全CQCNN (Frontend CNN → Quantum → Backend CNN)\n",
    "\n",
    "### 実験目標\n",
    "1. 0.95収束閾値での学習収束確認\n",
    "2. Ultra-strict実験(0.995)との比較分析\n",
    "3. 初期状態依存性の影響評価\n",
    "4. 量子効果の定量的測定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 量子強化学習実験環境 ===\n",
      "PyTorch version: 2.8.0+cpu\n",
      "PennyLane version: 0.42.3\n",
      "NumPy version: 2.3.3\n",
      "Random seed: 42\n",
      "Device: CPU\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# === 環境設定とライブラリ導入 ===\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 乱数シード設定（再現性確保）\n",
    "EXPERIMENT_SEED = 42\n",
    "random.seed(EXPERIMENT_SEED)\n",
    "np.random.seed(EXPERIMENT_SEED)\n",
    "torch.manual_seed(EXPERIMENT_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"=== 量子強化学習実験環境 ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PennyLane version: {qml.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 実験設定サマリー ===\n",
      "Algorithm: anti_draw_quantum_spatial_36d\n",
      "Quantum: 4Q2L\n",
      "Embedding: angle\n",
      "Entanglement: full\n",
      "State Dimension: 322\n",
      "Action Space: 36D\n",
      "Batch Size: 64\n",
      "Learning Rate: 0.001\n",
      "Epochs: 10000\n",
      "Epsilon: 0.6 → 0.15 (decay: 0.9995)\n",
      "Replay Buffer: 4000\n",
      "Target Update: every 100 episodes\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# === JSON設定読み込み ===\n",
    "config_path = \"quantum_anti_draw_config_2025-09-26.json\"\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"=== 実験設定サマリー ===\")\n",
    "print(f\"Algorithm: {config['learning_config']['algorithm']}\")\n",
    "print(f\"Quantum: {config['module_config']['quantum']['n_qubits']}Q{config['module_config']['quantum']['n_layers']}L\")\n",
    "print(f\"Embedding: {config['module_config']['quantum']['embedding_type']}\")\n",
    "print(f\"Entanglement: {config['module_config']['quantum']['entanglement']}\")\n",
    "print(f\"State Dimension: {config['module_config']['quantum']['state_dimension']}\")\n",
    "print(f\"Action Space: {config['module_config']['qmap']['action_dim']}D\")\n",
    "print(f\"Batch Size: {config['hyperparameters']['batch_size']}\")\n",
    "print(f\"Learning Rate: {config['hyperparameters']['learning_rate']}\")\n",
    "print(f\"Epochs: {config['hyperparameters']['epochs']}\")\n",
    "print(f\"Epsilon: {config['hyperparameters']['epsilon']} → {config['hyperparameters']['epsilon_min']} (decay: {config['hyperparameters']['epsilon_decay']})\")\n",
    "print(f\"Replay Buffer: {config['hyperparameters']['replay_buffer_size']}\")\n",
    "print(f\"Target Update: every {config['hyperparameters']['target_update_freq']} episodes\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# 設定値を変数に展開\n",
    "N_QUBITS = config['module_config']['quantum']['n_qubits']\n",
    "N_LAYERS = config['module_config']['quantum']['n_layers']\n",
    "STATE_DIM = 322  # config['module_config']['quantum']['state_dimension']\n",
    "ACTION_DIM = config['module_config']['qmap']['action_dim']\n",
    "BATCH_SIZE = config['hyperparameters']['batch_size']\n",
    "LEARNING_RATE = config['hyperparameters']['learning_rate']\n",
    "MAX_EPISODES = config['hyperparameters']['epochs']\n",
    "EPSILON_START = config['hyperparameters']['epsilon']\n",
    "EPSILON_DECAY = config['hyperparameters']['epsilon_decay']\n",
    "EPSILON_MIN = config['hyperparameters']['epsilon_min']\n",
    "BUFFER_SIZE = config['hyperparameters']['replay_buffer_size']\n",
    "TARGET_UPDATE = config['hyperparameters']['target_update_freq']\n",
    "GAMMA = config['hyperparameters']['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完全版ガイスター環境定義完了\n",
      "ガイスター環境初期化完了 - 報酬戦略: ultra_aggressive_anti_draw\n",
      "状態ベクトル形状: (322,)\n",
      "有効手数: 8\n"
     ]
    }
   ],
   "source": [
    "# === 完全版ガイスター環境 ===\n",
    "class GeisterEnvironment:\n",
    "    \"\"\"完全なガイスターゲーム環境（JSON設定対応）\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.board_size = 6\n",
    "        self.forced_p2_setup_id = None\n",
    "        self.p2_backrow_cells = [(0, 1), (0, 2), (0, 3), (0, 4),\n",
    "                                (1, 1), (1, 2), (1, 3), (1, 4)]\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "        # 報酬設定（JSON設定から読み込み）\n",
    "        reward_config = config['module_config']['reward']\n",
    "        self.capture_good_reward = reward_config['capture_good_reward']\n",
    "        self.capture_bad_penalty = reward_config['capture_bad_penalty']\n",
    "        self.escape_reward = reward_config['escape_reward']\n",
    "        self.captured_good_penalty = reward_config['captured_good_penalty']\n",
    "        self.captured_bad_reward = reward_config['captured_bad_reward']\n",
    "        self.position_rewards = reward_config['position_rewards']\n",
    "        \n",
    "        print(f\"ガイスター環境初期化完了 - 報酬戦略: {reward_config['strategy']}\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"ゲーム状態をリセット\"\"\"\n",
    "        self.board = np.zeros((6, 6), dtype=int)\n",
    "        self.turn = 0\n",
    "        self.current_player = 1\n",
    "        self.game_over = False\n",
    "        self.winner = None\n",
    "        self.captured_pieces = {'player_1': [], 'player_2': []}\n",
    "        self.move_history = []\n",
    "\n",
    "        # 配置設定（JSON設定から読み込み）\n",
    "        placement_config = self.config['module_config']['placement']\n",
    "        \n",
    "        if placement_config['type'] == 'custom':\n",
    "            # カスタム配置（JSON指定）\n",
    "            my_pieces = placement_config['my_pieces_config']\n",
    "            \n",
    "            # プレイヤー1（下側）\n",
    "            for i, row in enumerate(my_pieces):\n",
    "                for j, piece in enumerate(row):\n",
    "                    if piece != 0:\n",
    "                        self.board[4 + i][j] = piece\n",
    "\n",
    "            # ===== ここから P2（上側）の配置 =====\n",
    "            # 善=+2, 悪=-2 を前提にしています（必要ならあなたの符号に合わせて変更）\n",
    "            GOOD, BAD = 2, -2\n",
    "\n",
    "            # 8マス（2x4）の順序は __init__ で定義した p2_backrow_cells を使用\n",
    "            # もし未定義なら定義しておく\n",
    "            if not hasattr(self, 'p2_backrow_cells'):\n",
    "                self.p2_backrow_cells = [(0, 1), (0, 2), (0, 3), (0, 4),\n",
    "                                        (1, 1), (1, 2), (1, 3), (1, 4)]\n",
    "\n",
    "            # ---- 8C4 の「k番目の組合せ」を復元する関数（辞書順）----\n",
    "            from math import comb\n",
    "            def kth_comb_8_4(k: int):\n",
    "                \"\"\"0<=k<70 を 8要素から4要素を選ぶ辞書順のk番目に対応させ、昇順インデックス(4つ)を返す\"\"\"\n",
    "                res = []\n",
    "                n, r = 8, 4\n",
    "                x = 0\n",
    "                for i in range(r, 0, -1):\n",
    "                    for v in range(x, n):\n",
    "                        c = comb(n - v - 1, i - 1)\n",
    "                        if k < c:\n",
    "                            res.append(v)\n",
    "                            x = v + 1\n",
    "                            break\n",
    "                        k -= c\n",
    "                return res\n",
    "\n",
    "            # ---- P2配置の決定：強制IDがあれば適用、なければランダム ----\n",
    "            if getattr(self, 'forced_p2_setup_id', None) is not None:\n",
    "                sid = int(self.forced_p2_setup_id) % 70\n",
    "                red_idx = kth_comb_8_4(sid)          # 赤（悪）の位置(0..7)を取得\n",
    "                # まず全て善にしてから、赤インデックスの所だけ悪に置き換え\n",
    "                for idx, (r, c) in enumerate(self.p2_backrow_cells):\n",
    "                    self.board[r][c] = GOOD\n",
    "                for idx in red_idx:\n",
    "                    r, c = self.p2_backrow_cells[idx]\n",
    "                    self.board[r][c] = BAD\n",
    "            else:\n",
    "                # 従来のランダム配置（善4、悪4）を維持\n",
    "                pieces = [GOOD, GOOD, GOOD, GOOD, BAD, BAD, BAD, BAD]\n",
    "                random.shuffle(pieces)\n",
    "                for i, (r, c) in enumerate(self.p2_backrow_cells):\n",
    "                    self.board[r][c] = pieces[i]\n",
    "\n",
    "            # ---- 実際に用いられた P2 配置のID（0..69）を計算して保持 ----\n",
    "            # 赤（悪=-2）が置かれたインデックスを 0..7 で収集\n",
    "            red_positions = []\n",
    "            for idx, (r, c) in enumerate(self.p2_backrow_cells):\n",
    "                if self.board[r][c] == BAD:\n",
    "                    red_positions.append(idx)\n",
    "            red_positions.sort()\n",
    "\n",
    "            # rank（=組合せの辞書順ランク）を計算： 8C4 = 70\n",
    "            rank = 0\n",
    "            last = -1\n",
    "            for i, rr in enumerate(red_positions):\n",
    "                start = last + 1\n",
    "                for x in range(start, rr):\n",
    "                    rank += comb(8 - (x + 1), 4 - (i + 1))\n",
    "                last = rr\n",
    "            self.p2_setup_id = int(rank)   # ← これを後でモニタリングに使います\n",
    "\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"322次元状態ベクトル生成（7チャンネル + 配置ID埋め込み）\"\"\"\n",
    "        state = np.zeros(252 + 70)  # 252 + 70 = 322次元\n",
    "\n",
    "        # 既存の盤面エンコーディング（252次元）\n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                base_idx = (i * 6 + j) * 7\n",
    "                value = self.board[i][j]\n",
    "\n",
    "                # 7チャンネル: 空, P1善, P1悪, P2善, P2悪, プレイヤー, ターン\n",
    "                if value == 0:     # 空\n",
    "                    state[base_idx] = 1\n",
    "                elif value == 1:   # プレイヤー1善玉\n",
    "                    state[base_idx + 1] = 1\n",
    "                elif value == -1:  # プレイヤー1悪玉\n",
    "                    state[base_idx + 2] = 1\n",
    "                elif value == 2:   # プレイヤー2善玉\n",
    "                    state[base_idx + 3] = 1\n",
    "                elif value == -2:  # プレイヤー2悪玉\n",
    "                    state[base_idx + 4] = 1\n",
    "\n",
    "                # 追加情報\n",
    "                state[base_idx + 5] = 1 if self.current_player == 1 else 0\n",
    "                state[base_idx + 6] = min(self.turn / 100.0, 1.0)  # 正規化ターン\n",
    "\n",
    "        # ★ NEW: P2配置ID埋め込み（One-Hot エンコーディング）\n",
    "        if hasattr(self, 'p2_setup_id') and self.p2_setup_id is not None:\n",
    "            setup_idx = int(self.p2_setup_id) % 70\n",
    "            state[252 + setup_idx] = 1.0  # One-hotで配置IDを表現\n",
    "\n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, player=None):\n",
    "        \"\"\"有効な手を 36 次元（toセル基準の index）で返す。\n",
    "        同一 toセル への複数候補は優先度で 1 つに絞る。\n",
    "        \"\"\"\n",
    "        if player is None:\n",
    "            player = self.current_player\n",
    "\n",
    "        piece_values = [1, -1] if player == 1 else [2, -2]\n",
    "        # to-index -> (priority, move) で良い方を残す\n",
    "        best = {}  # { idx: (prio, (idx,\"move\",(from_i,from_j),(to_i,to_j))) }\n",
    "\n",
    "        def priority(piece, target, from_i, to_i):\n",
    "            # 高いほど良い：捕獲 > 前進 > その他\n",
    "            # （必要なら中央/敵陣入りで微加点）\n",
    "            p = 0\n",
    "            if target != 0:      p += 100\n",
    "            if (piece == 1 and to_i < from_i) or (piece == 2 and to_i > from_i): p += 3\n",
    "            if 2 <= to_i <= 3:   p += 1  # 中央\n",
    "            return p\n",
    "\n",
    "        for from_i in range(6):\n",
    "            for from_j in range(6):\n",
    "                if self.board[from_i][from_j] in piece_values:\n",
    "                    for di, dj in [(0,1),(0,-1),(1,0),(-1,0)]:\n",
    "                        to_i, to_j = from_i + di, from_j + dj\n",
    "                        if 0 <= to_i < 6 and 0 <= to_j < 6:\n",
    "                            target = self.board[to_i][to_j]\n",
    "                            # 自駒は不可／空 or 敵駒は可\n",
    "                            if (target == 0 or\n",
    "                                (player == 1 and abs(target) == 2) or\n",
    "                                (player == 2 and abs(target) == 1)):\n",
    "                                idx = to_i * 6 + to_j  # ★ 固定の toセルID（0..35）\n",
    "                                pr  = priority(self.board[from_i][from_j], target, from_i, to_i)\n",
    "                                mv  = (idx, \"move\", (from_i, from_j), (to_i, to_j))\n",
    "                                if (idx not in best) or (pr > best[idx][0]):\n",
    "                                    best[idx] = (pr, mv)\n",
    "\n",
    "        # 優先度で絞り込んだ手を取り出し\n",
    "        valid_moves = [t[1] for t in best.values()]\n",
    "        # 並びを安定化（index昇順）\n",
    "        valid_moves.sort(key=lambda m: m[0])\n",
    "        return valid_moves\n",
    "\n",
    "    def make_move(self, move):\n",
    "        \"\"\"手を実行し、報酬を計算\"\"\"\n",
    "        move_index, direction, from_pos, to_pos = move\n",
    "        from_i, from_j = from_pos\n",
    "        to_i, to_j = to_pos\n",
    "        \n",
    "        piece = self.board[from_i][from_j]\n",
    "        target = self.board[to_i][to_j]\n",
    "        \n",
    "        # 移動記録\n",
    "        self.move_history.append({\n",
    "            'turn': self.turn,\n",
    "            'player': self.current_player,\n",
    "            'move': move,\n",
    "            'piece': piece,\n",
    "            'captured': target if target != 0 else None\n",
    "        })\n",
    "        \n",
    "        # 駒を移動\n",
    "        self.board[from_i][from_j] = 0\n",
    "        self.board[to_i][to_j] = piece\n",
    "        \n",
    "        # 報酬計算\n",
    "        reward = self._calculate_reward(piece, target, from_pos, to_pos)\n",
    "        \n",
    "        # 勝利条件チェック\n",
    "        done = self._check_win_condition(piece, to_pos, target)\n",
    "        \n",
    "        # ターン進行\n",
    "        self.turn += 1\n",
    "        \n",
    "        # 最大ターン数チェック\n",
    "        if self.turn >= 100:  # 長期戦対応\n",
    "            self.game_over = True\n",
    "            self.winner = None\n",
    "            done = True\n",
    "        \n",
    "        if not done:\n",
    "            self.current_player = 2 if self.current_player == 1 else 1\n",
    "        \n",
    "        return self.get_state(), reward, done, {\n",
    "            'captured': target,\n",
    "            'winner': self.winner,\n",
    "            'turn': self.turn\n",
    "        }\n",
    "    \n",
    "    def _calculate_reward(self, piece, target, from_pos, to_pos):\n",
    "        \"\"\"詳細報酬計算（JSON設定準拠）\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # 駒捕獲報酬\n",
    "        if target != 0:\n",
    "            if target > 0:  # 善玉捕獲\n",
    "                reward += self.capture_good_reward\n",
    "            else:  # 悪玉捕獲\n",
    "                reward += self.capture_bad_penalty\n",
    "              # ★ NEW: Draw回避報酬\n",
    "\n",
    "        if hasattr(self, 'recent_draw_rate') and self.recent_draw_rate > 0.7:\n",
    "            # 攻撃的行動への追加ボーナス\n",
    "            if target != 0:  # 駒を取る行動\n",
    "                reward += 8.0\n",
    "\n",
    "            # 前進への追加ボーナス\n",
    "            from_i, from_j = from_pos\n",
    "            to_i, to_j = to_pos\n",
    "            if self.current_player == 1 and to_i < from_i:  # P1前進\n",
    "                reward += 1.5\n",
    "            elif self.current_player == 2 and to_i > from_i:  # P2前進\n",
    "                reward += 1.5\n",
    "        \n",
    "        # 位置的報酬\n",
    "        from_i, from_j = from_pos\n",
    "        to_i, to_j = to_pos\n",
    "        \n",
    "        # 前進報酬\n",
    "        if abs(piece) == 1 and piece > 0:  # 善玉の場合\n",
    "            if self.current_player == 1 and to_i < from_i:  # 前進\n",
    "                reward += self.position_rewards['advance_toward_escape']\n",
    "            elif self.current_player == 2 and to_i > from_i:  # 前進\n",
    "                reward += self.position_rewards['advance_toward_escape']\n",
    "        \n",
    "        # 中央制御\n",
    "        if 2 <= to_i <= 3 and 2 <= to_j <= 3:\n",
    "            reward += self.position_rewards['center_control']\n",
    "        \n",
    "        # 相手陣地進入\n",
    "        if ((self.current_player == 1 and to_i <= 2) or \n",
    "            (self.current_player == 2 and to_i >= 3)):\n",
    "            reward += self.position_rewards['opponent_territory']\n",
    "        \n",
    "          # ★ NEW: 長期戦ペナルティ（Draw抑制）\n",
    "        if hasattr(self, 'turn') and self.turn > 150:\n",
    "            turn_penalty = -(self.turn - 150) * 0.05  # 150ターン超過でペナルティ\n",
    "            reward += turn_penalty\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _check_win_condition(self, piece, to_pos, captured):\n",
    "        \"\"\"勝利条件判定\"\"\"\n",
    "        to_i, to_j = to_pos\n",
    "        \n",
    "        # 脱出勝利\n",
    "        if abs(piece) == 1 and piece > 0:  # 善玉\n",
    "            if ((self.current_player == 1 and to_i == 0 and to_j in [0, 5]) or\n",
    "                (self.current_player == 2 and to_i == 5 and to_j in [0, 5])):\n",
    "                self.game_over = True\n",
    "                self.winner = self.current_player\n",
    "                return True\n",
    "        \n",
    "        # 善玉全捕獲勝利\n",
    "        if captured is not None and captured > 0:\n",
    "            opponent_good_count = 0\n",
    "            search_value = 2 if self.current_player == 1 else 1\n",
    "            \n",
    "            for i in range(6):\n",
    "                for j in range(6):\n",
    "                    if self.board[i][j] == search_value:\n",
    "                        opponent_good_count += 1\n",
    "            \n",
    "            if opponent_good_count == 0:\n",
    "                self.game_over = True\n",
    "                self.winner = self.current_player\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "print(\"完全版ガイスター環境定義完了\")\n",
    "\n",
    "# 環境テスト\n",
    "test_env = GeisterEnvironment(config)\n",
    "test_state = test_env.reset()\n",
    "print(f\"状態ベクトル形状: {test_state.shape}\")\n",
    "print(f\"有効手数: {len(test_env.get_valid_moves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完全版CQCNN定義完了\n",
      "CQCNN初期化: 322D → 4Q2L → 36D\n",
      "Frontend: 4 Linear layers\n",
      "Quantum: 4 qubits, 2 layers, angle embedding\n",
      "Backend: 5 Linear layers\n",
      "\n",
      "モデル構造テスト中...\n",
      "入力形状: torch.Size([4, 322])\n",
      "出力形状: torch.Size([4, 36])\n",
      "出力範囲: [-1.273, 0.902]\n",
      "\n",
      "パラメータ数:\n",
      "  Classical: 82,536\n",
      "  Quantum: 16\n",
      "  Total: 82,552\n",
      "\n",
      "CQCNNテスト完了!\n"
     ]
    }
   ],
   "source": [
    "# === 完全版CQCNN実装 ===\n",
    "class CQCNN(nn.Module):\n",
    "    \"\"\"完全なClassical-Quantum Convolutional Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 設定読み込み\n",
    "        arch_config = config['architecture']\n",
    "        quantum_config = config['module_config']['quantum']\n",
    "        \n",
    "        self.n_qubits = quantum_config['n_qubits']\n",
    "        self.n_layers = quantum_config['n_layers']\n",
    "        self.state_dim = 322  # quantum_config['state_dimension']  # Fixed for config ID embedding\n",
    "        self.action_dim = config['module_config']['qmap']['action_dim']\n",
    "        \n",
    "        print(f\"CQCNN初期化: {self.state_dim}D → {self.n_qubits}Q{self.n_layers}L → {self.action_dim}D\")\n",
    "        \n",
    "        # Frontend CNN (322 → 4) - Updated for config ID embedding\n",
    "        frontend_config = arch_config['frontend_cnn']['layers'].copy()\n",
    "        frontend_config[0]['in_features'] = 322  # Update first layer to accept 322 dims\n",
    "        self.frontend_layers = self._build_layers(frontend_config)\n",
    "        \n",
    "        # Quantum Section\n",
    "        self.dev = qml.device(\n",
    "            arch_config['quantum_section']['device'], \n",
    "            wires=self.n_qubits\n",
    "        )\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(self.n_layers, self.n_qubits, 2) * 0.1\n",
    "        )\n",
    "        self.quantum_node = qml.QNode(\n",
    "            self._quantum_circuit, \n",
    "            self.dev, \n",
    "            interface='torch'\n",
    "        )\n",
    "        \n",
    "        # Backend CNN (4 → 36)\n",
    "        self.backend_layers = self._build_layers(arch_config['backend_cnn']['layers'])\n",
    "        \n",
    "        print(f\"Frontend: {len([l for l in self.frontend_layers if isinstance(l, nn.Linear)])} Linear layers\")\n",
    "        print(f\"Quantum: {self.n_qubits} qubits, {self.n_layers} layers, {quantum_config['embedding_type']} embedding\")\n",
    "        print(f\"Backend: {len([l for l in self.backend_layers if isinstance(l, nn.Linear)])} Linear layers\")\n",
    "\n",
    "    def _build_layers(self, layer_configs):\n",
    "        \"\"\"JSON設定からレイヤー構築\"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        for layer_config in layer_configs:\n",
    "            layer_type = layer_config['type']\n",
    "            \n",
    "            if layer_type == 'linear':\n",
    "                layers.append(nn.Linear(\n",
    "                    layer_config['in_features'],\n",
    "                    layer_config['out_features']\n",
    "                ))\n",
    "            elif layer_type == 'batch_norm':\n",
    "                layers.append(nn.BatchNorm1d(layer_config['num_features']))\n",
    "            elif layer_type == 'layer_norm':\n",
    "                layers.append(nn.LayerNorm(layer_config['normalized_shape']))\n",
    "            elif layer_type == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif layer_type == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif layer_type == 'dropout':\n",
    "                layers.append(nn.Dropout(layer_config['p']))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _quantum_circuit(self, features, params):\n",
    "        \"\"\"量子回路（JSON設定準拠）\"\"\"\n",
    "        # Angle embedding\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(features[i], wires=i)\n",
    "        \n",
    "        # Variational layers\n",
    "        for layer in range(self.n_layers):\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RY(params[layer, i, 0], wires=i)\n",
    "                qml.RZ(params[layer, i, 1], wires=i)\n",
    "            \n",
    "            # Full entanglement (updated from linear)\n",
    "            for i in range(self.n_qubits):\n",
    "                for j in range(i + 1, self.n_qubits):\n",
    "                    qml.CNOT(wires=[i, j])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"順伝播処理\"\"\"\n",
    "        batch_size = state.shape[0]\n",
    "        \n",
    "        # Frontend processing\n",
    "        state_flat = state.view(batch_size, -1)  # Flatten\n",
    "        frontend_out = self.frontend_layers(state_flat)  # (batch, 4)\n",
    "        \n",
    "        # Quantum processing (batch対応)\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            q_out = self.quantum_node(frontend_out[i], self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_out = torch.stack(quantum_outputs)  # (batch, 4)\n",
    "        \n",
    "        # Backend processing\n",
    "        output = self.backend_layers(quantum_out)  # (batch, 36)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"完全版CQCNN定義完了\")\n",
    "\n",
    "# モデル初期化テスト\n",
    "test_model = CQCNN(config)\n",
    "test_input = torch.randn(4, STATE_DIM)  # バッチサイズ4\n",
    "\n",
    "print(\"\\nモデル構造テスト中...\")\n",
    "with torch.no_grad():\n",
    "    test_output = test_model(test_input)\n",
    "    print(f\"入力形状: {test_input.shape}\")\n",
    "    print(f\"出力形状: {test_output.shape}\")\n",
    "    print(f\"出力範囲: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "    \n",
    "    # パラメータ数計算\n",
    "    total_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "    quantum_params = test_model.quantum_params.numel()\n",
    "    classical_params = total_params - quantum_params\n",
    "    \n",
    "    print(f\"\\nパラメータ数:\")\n",
    "    print(f\"  Classical: {classical_params:,}\")\n",
    "    print(f\"  Quantum: {quantum_params:,}\")\n",
    "    print(f\"  Total: {total_params:,}\")\n",
    "\n",
    "print(\"\\nCQCNNテスト完了!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 収束検出（バランス収束） ===\n",
    "class ConvergenceDetector:\n",
    "    \"\"\"\n",
    "    直近 window 局の P1/P2 勝数のバランスが balance_threshold 以上を\n",
    "    patience 回連続で満たしたら収束とみなす。\n",
    "    \"\"\"\n",
    "    def __init__(self, balance_threshold=0.95, patience=50, min_games=1000, window=100):\n",
    "        self.balance_threshold = balance_threshold\n",
    "        self.patience = patience\n",
    "        self.min_games = min_games\n",
    "        self.window = window\n",
    "        self.consecutive_good = 0  # 内部状態: 連続達成回数\n",
    "\n",
    "    def check_convergence(self, game_results, episode):\n",
    "        total = len(game_results)\n",
    "        # まだゲーム数が少ない間は判定しない\n",
    "        if total < max(self.min_games, self.window):\n",
    "            return (False, 0.0,\n",
    "                    {\"consecutive_good\": self.consecutive_good,\n",
    "                     \"recent_window\": min(self.window, total),\n",
    "                     \"enough_games\": False})\n",
    "\n",
    "        recent = game_results[-self.window:]\n",
    "        w1 = sum(1 for r in recent if r['winner'] == 1)\n",
    "        w2 = sum(1 for r in recent if r['winner'] == 2)\n",
    "\n",
    "        if max(w1, w2) > 0:\n",
    "            balance = min(w1, w2) / max(w1, w2)\n",
    "        else:\n",
    "            balance = 1.0  # 全部引き分け\n",
    "\n",
    "        if balance >= self.balance_threshold:\n",
    "            self.consecutive_good += 1\n",
    "        else:\n",
    "            self.consecutive_good = 0\n",
    "\n",
    "        converged = (self.consecutive_good >= self.patience)\n",
    "        metrics = {\n",
    "            \"consecutive_good\": self.consecutive_good,\n",
    "            \"recent_window\": self.window,\n",
    "            \"wins_1\": w1,\n",
    "            \"wins_2\": w2,\n",
    "            \"enough_games\": True\n",
    "        }\n",
    "        return converged, balance, metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "完全版CQCNN実験クラス定義完了（ε増加メカニズム強化版）\n"
     ]
    }
   ],
   "source": [
    "import copy  # NEW: スナップショット用\n",
    "# === メイン実験クラス ===\n",
    "class CQCNNExperiment:\n",
    "    \"\"\"完全版CQCNN自己対戦実験（早期停止を止まりやすく調整）\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = GeisterEnvironment(config)\n",
    "\n",
    "        # ===== 配置スケジューリング & モニタ初期化 =====\n",
    "        self.setup_stats = {sid: {'w':0, 'd':0, 'l':0, 'n':0} for sid in range(70)}\n",
    "        self.min_coverage_per_sid = 40     # ★ 50→40 に緩和\n",
    "        self.ucb_alpha = 0.8\n",
    "        self.ucb_top_k = 8\n",
    "        self.role_mirror_prob = 0.15\n",
    "        self.metrics_csv_path = 'training_metrics.csv'\n",
    "        with open(self.metrics_csv_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('episode,mean_wr,worst_decile,variance,coverage_min,coverage_ok\\n')\n",
    "        # ==============================================\n",
    "\n",
    "        # モデル初期化\n",
    "        self.cqcnn_1 = CQCNN(config)\n",
    "        self.cqcnn_2 = CQCNN(config)\n",
    "\n",
    "        # P2へ微小ノイズ（多様性）\n",
    "        with torch.no_grad():\n",
    "            for param in self.cqcnn_2.parameters():\n",
    "                param.add_(torch.randn_like(param) * 0.01)\n",
    "\n",
    "        # オプティマイザー\n",
    "        self.optimizer_1 = optim.Adam(self.cqcnn_1.parameters(), lr=LEARNING_RATE)\n",
    "        self.optimizer_2 = optim.Adam(self.cqcnn_2.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "        # 学習関連\n",
    "        self.criterion = nn.SmoothL1Loss(beta=1.0)\n",
    "        self.replay_buffer_1 = deque(maxlen=BUFFER_SIZE)\n",
    "        self.replay_buffer_2 = deque(maxlen=BUFFER_SIZE)\n",
    "\n",
    "        # 統計記録\n",
    "        self.game_results = []\n",
    "        self.losses_1 = []\n",
    "        self.losses_2 = []\n",
    "        self.epsilon_history = []\n",
    "        self.training_metrics = []\n",
    "\n",
    "        # 収束検出（従来のバランス判定）\n",
    "        self.convergence_detector = ConvergenceDetector(\n",
    "            balance_threshold=0.95, patience=50, min_games=1000\n",
    "        )\n",
    "\n",
    "        # ★ 無改善停止用の状態\n",
    "        self.best_metrics = {'worst10': 0.0, 'mean_wr': 0.0}\n",
    "        self.no_improve_streak = 0\n",
    "        self.NOIMP_PATIENCE = 2    # ご指定\n",
    "        self.NOIMP_DELTA = 0.002   # ご指定\n",
    "\n",
    "        # その他\n",
    "        self.start_time = time.time()\n",
    "        self.episode_times = []\n",
    "\n",
    "        print(\"\\n=== CQCNN実験初期化完了 ===\")\n",
    "        print(f\"モデル1パラメータ: {sum(p.numel() for p in self.cqcnn_1.parameters()):,}\")\n",
    "        print(f\"モデル2パラメータ: {sum(p.numel() for p in self.cqcnn_2.parameters()):,}\")\n",
    "        print(f\"リプレイバッファサイズ: {BUFFER_SIZE:,}\")\n",
    "        print(f\"バッチサイズ: {BATCH_SIZE}\")\n",
    "        print(f\"最大エピソード: {MAX_EPISODES:,}\")\n",
    "        # 現在の表示を更新\n",
    "        print(f\"状態次元: 322 (盤面252 + 配置ID70)\")\n",
    "        print(f\"P2配置識別: 有効\")\n",
    "\n",
    "    # ===== 配置モニタ/スケジューラ =====\n",
    "    def _record_setup_result(self, setup_id: int, winner: int | None):\n",
    "        s = self.setup_stats[setup_id]\n",
    "        if winner is None: s['d'] += 1\n",
    "        elif winner == 1:  s['w'] += 1\n",
    "        else:              s['l'] += 1\n",
    "        s['n'] += 1\n",
    "    # ===== 適応的学習メカニズム =====\n",
    "\n",
    "    def _calculate_adaptive_epsilon(self, episode):\n",
    "        \"\"\"適応的エプシロン計算（Draw回避版・強化）\"\"\"\n",
    "        base_epsilon = max(EPSILON_MIN, EPSILON_START * (EPSILON_DECAY ** episode))\n",
    "\n",
    "        # 直近50ゲームのDraw率を計算（より頻繁にチェック）\n",
    "        if len(self.game_results) >= 100:\n",
    "            recent = self.game_results[-100:]\n",
    "            draw_rate = sum(1 for r in recent if r['winner'] is None) / len(recent)\n",
    "\n",
    "            # Draw率が0.9以上で探索を強化（閾値を下げる）\n",
    "            if draw_rate > 0.95:\n",
    "                # より積極的な探索ブースト\n",
    "                exploration_boost = min(0.2, (draw_rate - 0.5) * 2.0)  \n",
    "                adaptive_epsilon = min(0.50, base_epsilon + exploration_boost)\n",
    "                \n",
    "                # より頻繁に報告（500エピソードごと）\n",
    "                if episode % 500 == 0:\n",
    "                    print(f\"[AdaptiveΕ] ep={episode}, draw_rate={draw_rate:.3f}, \"\n",
    "                        f\"ε={base_epsilon:.4f}→{adaptive_epsilon:.4f} (+{exploration_boost:.3f})\")\n",
    "                return adaptive_epsilon\n",
    "\n",
    "            # Draw率が低い場合でも最低限のベースεを維持\n",
    "            enhanced_base = max(base_epsilon, 0.15)  # 最低εを0.15に設定\n",
    "            if episode % 500 == 0 and enhanced_base > base_epsilon:\n",
    "                print(f\"[BaseΕ] ep={episode}, ε_enhanced={enhanced_base:.4f} (base: {base_epsilon:.4f})\")\n",
    "            return enhanced_base\n",
    "\n",
    "        # 初期段階では高いεを維持\n",
    "        return max(base_epsilon, 0.3)\n",
    "    \n",
    "    def _inject_quantum_noise_if_needed(self, episode):\n",
    "        \"\"\"量子パラメータノイズ注入（局所解脱出・頻度増加）\"\"\"\n",
    "        # 頻度を250エピソードごとに増加\n",
    "        if episode % 300 == 0 and len(self.game_results) >= 50:\n",
    "            recent_draws = sum(1 for r in self.game_results[-50:] if r['winner'] is None)\n",
    "            draw_rate = recent_draws / 50\n",
    "\n",
    "            # 閾値を0.5に下げる\n",
    "            if draw_rate > 0.7:\n",
    "                # ノイズ強度を増加\n",
    "                noise_scale = min(0.05, (draw_rate - 0.7) * 0.2)\n",
    "                with torch.no_grad():\n",
    "                    # Player 1\n",
    "                    noise1 = torch.randn_like(self.cqcnn_1.quantum_params) * noise_scale\n",
    "                    self.cqcnn_1.quantum_params.add_(noise1)\n",
    "                    # Player 2\n",
    "                    noise2 = torch.randn_like(self.cqcnn_2.quantum_params) * noise_scale\n",
    "                    self.cqcnn_2.quantum_params.add_(noise2)\n",
    "                print(f\"[QuantumNoise] ep={episode}, draw_rate={draw_rate:.3f}, \"\n",
    "                    f\"injected_noise={noise_scale:.4f}\")\n",
    "\n",
    "    def _save_checkpoint(self, episode, reason, metrics=None):\n",
    "        \"\"\"学習状態を保存（weights + optimizer + 進捗メタ）\"\"\"\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        payload = {\n",
    "            \"episode\": int(episode),\n",
    "            \"reason\": str(reason),\n",
    "            \"p1_state\": self.cqcnn_1.state_dict(),\n",
    "            \"p2_state\": self.cqcnn_2.state_dict(),\n",
    "            \"opt1_state\": self.optimizer_1.state_dict(),\n",
    "            \"opt2_state\": self.optimizer_2.state_dict(),\n",
    "            \"setup_stats\": self.setup_stats,\n",
    "            \"config\": self.config,\n",
    "            \"losses_1\": self.losses_1[-200:],  # 末尾だけで十分\n",
    "            \"losses_2\": self.losses_2[-200:],\n",
    "            \"epsilon\": self.epsilon_history[-1] if self.epsilon_history else None,\n",
    "            \"metrics\": metrics or {},\n",
    "        }\n",
    "        path = f\"checkpoints/ep{episode:06d}_{reason}.pth\"\n",
    "        torch.save(payload, path)\n",
    "        print(f\"[save] checkpoint -> {path}\")\n",
    "\n",
    "\n",
    "    def _compute_setup_metrics(self):\n",
    "        wrs, counts = [], []\n",
    "        for sid in range(70):\n",
    "            s = self.setup_stats[sid]; n = max(1, s['n'])\n",
    "            wr = (s['w'] + 0.5*s['d']) / n\n",
    "            wrs.append(wr); counts.append(s['n'])\n",
    "        wrs = np.array(wrs); counts = np.array(counts)\n",
    "        mean_wr = float(wrs.mean())\n",
    "        worst_decile = float(np.sort(wrs)[:max(1, 70//10)].mean())\n",
    "        variance = float(wrs.var())\n",
    "        coverage_ok = bool(counts.min() >= self.min_coverage_per_sid)\n",
    "        return dict(mean_wr=mean_wr, worst_decile=worst_decile,\n",
    "                    variance=variance, coverage_ok=coverage_ok,\n",
    "                    coverage_min=int(counts.min()))\n",
    "\n",
    "    def _pick_setup_min_coverage(self):\n",
    "        need = [sid for sid in range(70) if self.setup_stats[sid]['n'] < self.min_coverage_per_sid]\n",
    "        return random.choice(need) if need else None\n",
    "\n",
    "    def _pick_setup_ucb(self):\n",
    "        import math\n",
    "        total = 1 + sum(self.setup_stats[sid]['n'] for sid in range(70))\n",
    "        scores = []\n",
    "        for sid in range(70):\n",
    "            s = self.setup_stats[sid]; n = s['n']\n",
    "            wr = (s['w'] + 0.5*s['d']) / max(1, n)\n",
    "            exploit = 1.0 - wr\n",
    "            explore = self.ucb_alpha * math.sqrt(math.log(total) / (1 + n))\n",
    "            scores.append((exploit + explore, sid))\n",
    "        scores.sort(reverse=True)\n",
    "        cand = [sid for _, sid in scores[:self.ucb_top_k]]\n",
    "        return random.choice(cand)\n",
    "\n",
    "    def choose_p2_setup_id(self):\n",
    "        sid = self._pick_setup_min_coverage()\n",
    "        if sid is not None: return sid\n",
    "        return self._pick_setup_ucb()\n",
    "\n",
    "    # ===== アクション選択 =====\n",
    "    def select_action(self, model, state, valid_moves, epsilon):\n",
    "        # ε-greedy\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(valid_moves)\n",
    "\n",
    "        # ★ バッチサイズ1の推論では BN/Dropout を eval にする\n",
    "        was_training = model.training\n",
    "        model.eval()\n",
    "        try:\n",
    "            dev = next(model.parameters()).device\n",
    "            with torch.inference_mode():\n",
    "                state_tensor = torch.as_tensor(state, dtype=torch.float32, device=dev).view(1, -1)\n",
    "                q_values = model(state_tensor).squeeze(0)  # (36,)\n",
    "        finally:\n",
    "            if was_training:\n",
    "                model.train()\n",
    "\n",
    "        # 有効手だけで argmax（安定）\n",
    "        valid_idx = torch.as_tensor([m[0] for m in valid_moves], device=dev, dtype=torch.long)\n",
    "        best_local = torch.argmax(q_values.index_select(0, valid_idx)).item()\n",
    "        return valid_moves[best_local]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ===== 1ゲーム実行 =====\n",
    "    def play_game(self, episode):\n",
    "        \"\"\"1ゲームを実行（改良版：適応的ε + Draw回避報酬 + 量子ノイズ注入）\"\"\"\n",
    "        self.env.forced_p2_setup_id = self.choose_p2_setup_id()\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "\n",
    "        game_states_1 = []\n",
    "        game_states_2 = []\n",
    "\n",
    "        # ★ 改良1: 適応的エプシロン計算\n",
    "        epsilon = self._calculate_adaptive_epsilon(episode)\n",
    "\n",
    "        # ★ 改良2: 量子ノイズ注入（定期実行）\n",
    "        self._inject_quantum_noise_if_needed(episode)\n",
    "\n",
    "        # ★ 改良3: 環境にDraw率情報を伝達\n",
    "        if len(self.game_results) >= 50:  # 50ゲームに短縮\n",
    "            recent_draws = sum(1 for r in self.game_results[-50:] if r['winner'] is None)\n",
    "            self.env.recent_draw_rate = recent_draws / 50\n",
    "        else:\n",
    "            self.env.recent_draw_rate = 0.0\n",
    "\n",
    "        while not done:\n",
    "            valid_moves = self.env.get_valid_moves()\n",
    "            if not valid_moves:\n",
    "                self.env.game_over = True\n",
    "                self.env.winner = None\n",
    "                break\n",
    "\n",
    "            current_state = state.copy()\n",
    "            current_player = self.env.current_player\n",
    "\n",
    "            if current_player == 1:\n",
    "                chosen_move = self.select_action(self.cqcnn_1, current_state, valid_moves, epsilon)\n",
    "            else:\n",
    "                chosen_move = self.select_action(self.cqcnn_2, current_state, valid_moves, epsilon)\n",
    "\n",
    "            next_state, reward, done, info = self.env.make_move(chosen_move)\n",
    "\n",
    "            # move_index はタプル先頭を使用（0..35）\n",
    "            if isinstance(chosen_move, (tuple, list)) and len(chosen_move) >= 1:\n",
    "                move_index = int(chosen_move[0])\n",
    "            else:\n",
    "                move_index = int(chosen_move)\n",
    "            experience = (current_state, move_index, reward, next_state, done)\n",
    "\n",
    "            if current_player == 1:\n",
    "                game_states_1.append(experience)\n",
    "            else:\n",
    "                game_states_2.append(experience)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "        # 結果\n",
    "        result = {\n",
    "            'episode': episode,\n",
    "            'winner': self.env.winner,\n",
    "            'turns': self.env.turn,\n",
    "            'player_1_moves': len(game_states_1),\n",
    "            'player_2_moves': len(game_states_2)\n",
    "        }\n",
    "\n",
    "        # 終局報酬\n",
    "        final_reward_1 = 1.0 if self.env.winner == 1 else (-1.0 if self.env.winner == 2 else 0.0)\n",
    "        final_reward_2 = 1.0 if self.env.winner == 2 else (-1.0 if self.env.winner == 1 else 0.0)\n",
    "\n",
    "        # sid 記録 & 戦績集計\n",
    "        sid = getattr(self.env, 'p2_setup_id', None)\n",
    "        if sid is not None:\n",
    "            self._record_setup_result(sid, self.env.winner)\n",
    "\n",
    "        # リプレイへ（sid付き6タプル）\n",
    "        for state, action, reward, next_state, done in game_states_1:\n",
    "            final_exp = (state, action, reward + final_reward_1, next_state, done, sid)\n",
    "            self.replay_buffer_1.append(final_exp)\n",
    "        for state, action, reward, next_state, done in game_states_2:\n",
    "            final_exp = (state, action, reward + final_reward_2, next_state, done, sid)\n",
    "            self.replay_buffer_2.append(final_exp)\n",
    "\n",
    "        return result, epsilon\n",
    "\n",
    "    # ===== 学習（既存のまま／細部だけ整備） =====\n",
    "    def train_model(self, model, optimizer, replay_buffer, losses_list):\n",
    "        \"\"\"モデル学習\"\"\"\n",
    "        if len(replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # バッチサンプリング（sid付き6タプル/旧5タプル両対応）\n",
    "        batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "        # 後方互換で展開\n",
    "        states, actions, rewards, next_states, dones = [], [], [], [], []\n",
    "        for exp in batch:\n",
    "            if len(exp) == 6:\n",
    "                s, a, r, ns, d, sid = exp\n",
    "            else:\n",
    "                s, a, r, ns, d = exp\n",
    "            states.append(s); actions.append(a); rewards.append(r); next_states.append(ns); dones.append(d)\n",
    "\n",
    "        states      = torch.FloatTensor(states)\n",
    "        actions     = torch.LongTensor(actions)\n",
    "        rewards     = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)   # 状態はそのまま\n",
    "        dones       = torch.BoolTensor(dones)\n",
    "\n",
    "        # 現在のQ値\n",
    "        current_q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "\n",
    "        # ターゲットQ値（簡単なTD学習）\n",
    "        with torch.no_grad():\n",
    "            next_q_values = model(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (GAMMA * next_q_values * ~dones)\n",
    "            target_q_values = target_q_values.clamp(-10.0, 10.0) # 過大更新防止\n",
    "        # 損失計算と更新\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)  # ★追加\n",
    "        optimizer.step()\n",
    "\n",
    "        losses_list.append(loss.item())\n",
    "\n",
    "    # ===== 実験ループ（300ep監視＋無改善停止） =====\n",
    "    def run_experiment(self):\n",
    "        \"\"\"メイン実験実行（監視300ep、緩めしきい値、無改善停止つき）\"\"\"\n",
    "        print(f\"\\n=== 実験開始: 収束まで最大{MAX_EPISODES:,}エピソード ===\")\n",
    "        print(f\"収束条件: Balance ≥ 0.95, {self.convergence_detector.patience}回連続\")\n",
    "\n",
    "        # ★ しきい値（既存）\n",
    "        WORST10_THR    = 0.46\n",
    "        VAR_THR        = 0.012\n",
    "        SETUP_PATIENCE = 1\n",
    "\n",
    "        # ★ 追加: εガード＆最小エピソード\n",
    "        EPS_GUARD = 0.15                 # ← これ未満になってから止める\n",
    "        MIN_EPISODES_BEFORE_SETUP_STOP = 2500\n",
    "\n",
    "        setup_ok_streak = 0\n",
    "\n",
    "        for episode in range(MAX_EPISODES):\n",
    "            episode_start = time.time()\n",
    "\n",
    "            # 1ゲーム\n",
    "            result, epsilon = self.play_game(episode)\n",
    "            self.game_results.append(result)\n",
    "            self.epsilon_history.append(epsilon)\n",
    "\n",
    "            # 学習（5epごと）\n",
    "            if episode % 5 == 0:\n",
    "                self.train_model(self.cqcnn_1, self.optimizer_1, self.replay_buffer_1, self.losses_1)\n",
    "                self.train_model(self.cqcnn_2, self.optimizer_2, self.replay_buffer_2, self.losses_2)\n",
    "\n",
    "            # 計時\n",
    "            episode_time = time.time() - episode_start\n",
    "            self.episode_times.append(episode_time)\n",
    "\n",
    "            # 進捗 + バランス収束（100epごと）\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                self._print_progress(episode)\n",
    "                converged, balance, metrics = self.convergence_detector.check_convergence(\n",
    "                    self.game_results, episode\n",
    "                )\n",
    "                if converged:\n",
    "                    print(f\"\\n🎉 収束達成! Episode {episode+1}\")\n",
    "                    print(f\"Final Balance: {balance:.4f}\")\n",
    "                    print(f\"Consecutive Good: {metrics['consecutive_good']}\")\n",
    "                    self._save_checkpoint(episode+1, reason=\"balance\", metrics={\"balance\": balance, **metrics})\n",
    "                    break\n",
    "\n",
    "            # ★ 配置メトリクス監視＆早期停止（300epごと）\n",
    "            if (episode + 1) % 300 == 0:\n",
    "                # 追加: ガード閾値（クラス属性があればそれを使う）\n",
    "                EPS_GUARD = getattr(self, 'EPS_GUARD', 0.15)                      # εがこの値以下になるまで停止させない\n",
    "                MIN_EPISODES_BEFORE_SETUP_STOP = getattr(self, 'MIN_EPISODES_BEFORE_SETUP_STOP', 2500)  # この局数までは停止させない\n",
    "                current_eps = self.epsilon_history[-1] if self.epsilon_history else EPSILON_START\n",
    "\n",
    "                m = self._compute_setup_metrics()\n",
    "                print(f\"[monitor] ep={episode+1} mean={m['mean_wr']:.3f} \"\n",
    "                    f\"worst10%={m['worst_decile']:.3f} var={m['variance']:.4f} \"\n",
    "                    f\"cov_min={m['coverage_min']} cov_ok={m['coverage_ok']}\")\n",
    "                with open(self.metrics_csv_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"{episode+1},{m['mean_wr']:.6f},{m['worst_decile']:.6f},\"\n",
    "                            f\"{m['variance']:.6f},{m['coverage_min']},{int(m['coverage_ok'])}\\n\")\n",
    "\n",
    "                # 条件成立→streak加算（SETUP_PATIENCE=1 なので 1回で発火）\n",
    "                if m[\"coverage_ok\"] and m[\"worst_decile\"] >= WORST10_THR and m[\"variance\"] <= VAR_THR:\n",
    "                    setup_ok_streak += 1\n",
    "                    print(f\"[setup-ok] streak {setup_ok_streak}/{SETUP_PATIENCE}\")\n",
    "                else:\n",
    "                    setup_ok_streak = 0\n",
    "\n",
    "                # --- setup 停止（ε & 最小ep ガード付き）---\n",
    "                if setup_ok_streak >= SETUP_PATIENCE:\n",
    "                    if (current_eps > EPS_GUARD) or ((episode + 1) < MIN_EPISODES_BEFORE_SETUP_STOP):\n",
    "                        print(f\"[setup-ok] blocked by guard: eps={current_eps:.3f} \"\n",
    "                            f\"min_ep={episode+1}/{MIN_EPISODES_BEFORE_SETUP_STOP}\")\n",
    "                        setup_ok_streak = 1  # streakは維持（リセットしない）\n",
    "                    else:\n",
    "                        print(f\"\\n🛑 EarlyStop/setup at ep={episode+1} | \"\n",
    "                            f\"mean={m['mean_wr']:.3f}, worst10%={m['worst_decile']:.3f}, var={m['variance']:.4f}\")\n",
    "                        self._save_checkpoint(episode+1, reason=\"setup\", metrics=m)\n",
    "                        break\n",
    "\n",
    "                # ★ 無改善停止（coverage_okが前提）\n",
    "                improved = False\n",
    "                if m[\"worst_decile\"] > self.best_metrics['worst10'] + self.NOIMP_DELTA:\n",
    "                    self.best_metrics['worst10'] = m[\"worst_decile\"]; improved = True\n",
    "                if m[\"mean_wr\"] > self.best_metrics['mean_wr'] + self.NOIMP_DELTA:\n",
    "                    self.best_metrics['mean_wr'] = m[\"mean_wr\"]; improved = True\n",
    "\n",
    "                if m[\"coverage_ok\"]:\n",
    "                    self.no_improve_streak = 0 if improved else (self.no_improve_streak + 1)\n",
    "\n",
    "                    # --- 無改善停止（ε & 最小ep ガード付き）---\n",
    "                    if self.no_improve_streak >= self.NOIMP_PATIENCE:\n",
    "                        if (current_eps > EPS_GUARD) or ((episode + 1) < MIN_EPISODES_BEFORE_SETUP_STOP):\n",
    "                            print(f\"[no-improve] blocked by guard: eps={current_eps:.3f} \"\n",
    "                                f\"min_ep={episode+1}/{MIN_EPISODES_BEFORE_SETUP_STOP}\")\n",
    "                            self.no_improve_streak = 1\n",
    "                        else:\n",
    "                            print(f\"\\n🛑 EarlyStop/no-improve at ep={episode+1} | \"\n",
    "                                f\"worst10={m['worst_decile']:.3f}, mean={m['mean_wr']:.3f}, var={m['variance']:.4f}\")\n",
    "                            self._save_checkpoint(episode+1, reason=\"no-improve\", metrics=m)\n",
    "                            break\n",
    "                else:\n",
    "                    self.no_improve_streak = 0\n",
    "\n",
    "            # 既存の早期収束（1000epごと）\n",
    "            if (episode + 1) % 1000 == 0:\n",
    "                converged, balance, metrics = self.convergence_detector.check_convergence(\n",
    "                    self.game_results, episode\n",
    "                )\n",
    "                if converged:\n",
    "                    print(f\"\\n🎉 収束達成! Episode {episode+1}\")\n",
    "                    self._save_checkpoint(episode+1, reason=\"balance_1000\", metrics={\"balance\": balance, **metrics})\n",
    "                    break\n",
    "\n",
    "        total_time = time.time() - self.start_time\n",
    "\n",
    "        print(f\"\\n=== 実験完了 ===\")\n",
    "        print(f\"Total Episodes: {len(self.game_results):,}\")\n",
    "        print(f\"Total Time: {total_time:.1f}s ({total_time/3600:.1f}h)\")\n",
    "        print(f\"Average Episode Time: {np.mean(self.episode_times):.3f}s\")\n",
    "\n",
    "        # 最終収束チェック\n",
    "        final_converged, final_balance, final_metrics = self.convergence_detector.check_convergence(\n",
    "            self.game_results, len(self.game_results)-1\n",
    "        )\n",
    "        return len(self.game_results), total_time, {\n",
    "            'converged': final_converged,\n",
    "            'balance': final_balance,\n",
    "            'metrics': final_metrics,\n",
    "            'total_games': len(self.game_results)\n",
    "        }\n",
    "\n",
    "    def _print_progress(self, episode):\n",
    "        \"\"\"進捗表示\"\"\"\n",
    "        recent_window = min(100, len(self.game_results))\n",
    "        recent_results = self.game_results[-recent_window:]\n",
    "\n",
    "        wins_1 = sum(1 for r in recent_results if r['winner'] == 1)\n",
    "        wins_2 = sum(1 for r in recent_results if r['winner'] == 2)\n",
    "        draws  = sum(1 for r in recent_results if r['winner'] is None)\n",
    "\n",
    "        win_rate_1 = wins_1 / recent_window if recent_window else 0.0\n",
    "        win_rate_2 = wins_2 / recent_window if recent_window else 0.0\n",
    "        draw_rate  = draws  / recent_window if recent_window else 0.0\n",
    "\n",
    "        balance = min(wins_1, wins_2) / max(wins_1, wins_2) if max(wins_1, wins_2) > 0 else 1.0\n",
    "        avg_turns = np.mean([r['turns'] for r in recent_results]) if recent_results else 0.0\n",
    "\n",
    "        current_epsilon = self.epsilon_history[-1] if self.epsilon_history else EPSILON_START\n",
    "        avg_loss_1 = np.mean(self.losses_1[-50:]) if len(self.losses_1) >= 50 else 0\n",
    "        avg_loss_2 = np.mean(self.losses_2[-50:]) if len(self.losses_2) >= 50 else 0\n",
    "\n",
    "        elapsed_time = time.time() - self.start_time\n",
    "\n",
    "        print(f\"Episode {episode+1:5d} | \"\n",
    "              f\"P1={win_rate_1:.3f} P2={win_rate_2:.3f} D={draw_rate:.3f} | \"\n",
    "              f\"Balance={balance:.4f} | \"\n",
    "              f\"Turns={avg_turns:.1f} | \"\n",
    "              f\"ε={current_epsilon:.4f} | \"\n",
    "              f\"Loss={avg_loss_1:.4f}/{avg_loss_2:.4f} | \"\n",
    "              f\"Time={elapsed_time:.0f}s\")\n",
    "\n",
    "print(\"完全版CQCNN実験クラス定義完了（ε増加メカニズム強化版）\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 CQCNN量子強化学習実験を開始します\n",
      "設定: quantum_anti_draw_config_2025-09-26.json\n",
      "最大エピソード: 10,000\n",
      "目標: Balance ≥ 0.95, 50回連続達成で収束\n",
      "ガイスター環境初期化完了 - 報酬戦略: ultra_aggressive_anti_draw\n",
      "CQCNN初期化: 322D → 4Q2L → 36D\n",
      "Frontend: 4 Linear layers\n",
      "Quantum: 4 qubits, 2 layers, angle embedding\n",
      "Backend: 5 Linear layers\n",
      "CQCNN初期化: 322D → 4Q2L → 36D\n",
      "Frontend: 4 Linear layers\n",
      "Quantum: 4 qubits, 2 layers, angle embedding\n",
      "Backend: 5 Linear layers\n",
      "\n",
      "=== CQCNN実験初期化完了 ===\n",
      "モデル1パラメータ: 82,552\n",
      "モデル2パラメータ: 82,552\n",
      "リプレイバッファサイズ: 4,000\n",
      "バッチサイズ: 64\n",
      "最大エピソード: 10,000\n",
      "状態次元: 322 (盤面252 + 配置ID70)\n",
      "P2配置識別: 有効\n",
      "\n",
      "=== 実験開始: 収束まで最大10,000エピソード ===\n",
      "収束条件: Balance ≥ 0.95, 50回連続\n",
      "Episode   100 | P1=0.070 P2=0.020 D=0.910 | Balance=0.2857 | Turns=96.9 | ε=0.5710 | Loss=0.0000/0.0000 | Time=18s\n",
      "Episode   200 | P1=0.060 P2=0.000 D=0.940 | Balance=0.0000 | Turns=97.8 | ε=0.5432 | Loss=0.0000/0.0000 | Time=38s\n",
      "Episode   300 | P1=0.090 P2=0.000 D=0.910 | Balance=0.0000 | Turns=97.8 | ε=0.5167 | Loss=2.6421/2.0623 | Time=59s\n",
      "[monitor] ep=300 mean=0.529 worst10%=0.464 var=0.0050 cov_min=1 cov_ok=False\n",
      "[QuantumNoise] ep=300, draw_rate=0.880, injected_noise=0.0360\n",
      "Episode   400 | P1=0.050 P2=0.010 D=0.940 | Balance=0.2000 | Turns=98.7 | ε=0.4915 | Loss=2.7778/2.1322 | Time=80s\n",
      "Episode   500 | P1=0.090 P2=0.010 D=0.900 | Balance=0.1111 | Turns=98.0 | ε=0.4675 | Loss=2.9099/2.0526 | Time=101s\n",
      "Episode   600 | P1=0.050 P2=0.000 D=0.950 | Balance=0.0000 | Turns=98.3 | ε=0.4447 | Loss=2.7898/1.9165 | Time=124s\n",
      "[monitor] ep=600 mean=0.531 worst10%=0.490 var=0.0022 cov_min=2 cov_ok=False\n",
      "[QuantumNoise] ep=600, draw_rate=0.960, injected_noise=0.0500\n",
      "Episode   700 | P1=0.030 P2=0.000 D=0.970 | Balance=0.0000 | Turns=99.4 | ε=0.5000 | Loss=2.5189/1.7845 | Time=145s\n",
      "Episode   800 | P1=0.020 P2=0.010 D=0.970 | Balance=0.5000 | Turns=99.5 | ε=0.5000 | Loss=2.0701/1.6240 | Time=167s\n",
      "Episode   900 | P1=0.060 P2=0.000 D=0.940 | Balance=0.0000 | Turns=98.1 | ε=0.3827 | Loss=1.5211/1.4182 | Time=190s\n",
      "[monitor] ep=900 mean=0.527 worst10%=0.489 var=0.0014 cov_min=5 cov_ok=False\n",
      "[QuantumNoise] ep=900, draw_rate=0.960, injected_noise=0.0500\n",
      "Episode  1000 | P1=0.060 P2=0.020 D=0.920 | Balance=0.3333 | Turns=98.5 | ε=0.3641 | Loss=0.9317/1.0405 | Time=213s\n",
      "Episode  1100 | P1=0.130 P2=0.010 D=0.860 | Balance=0.0769 | Turns=96.3 | ε=0.3463 | Loss=0.5320/0.6962 | Time=236s\n",
      "Episode  1200 | P1=0.070 P2=0.020 D=0.910 | Balance=0.2857 | Turns=97.7 | ε=0.3294 | Loss=0.3318/0.4781 | Time=260s\n",
      "[monitor] ep=1200 mean=0.528 worst10%=0.486 var=0.0013 cov_min=8 cov_ok=False\n",
      "[QuantumNoise] ep=1200, draw_rate=0.860, injected_noise=0.0320\n",
      "Episode  1300 | P1=0.100 P2=0.060 D=0.840 | Balance=0.6000 | Turns=95.8 | ε=0.3133 | Loss=0.2435/0.3843 | Time=286s\n",
      "Episode  1400 | P1=0.240 P2=0.110 D=0.650 | Balance=0.4583 | Turns=90.8 | ε=0.2980 | Loss=0.2045/0.3215 | Time=309s\n",
      "Episode  1500 | P1=0.140 P2=0.080 D=0.780 | Balance=0.5714 | Turns=95.5 | ε=0.2835 | Loss=0.1805/0.3163 | Time=333s\n",
      "[monitor] ep=1500 mean=0.530 worst10%=0.472 var=0.0016 cov_min=11 cov_ok=False\n",
      "[QuantumNoise] ep=1500, draw_rate=0.800, injected_noise=0.0200\n",
      "Episode  1600 | P1=0.230 P2=0.070 D=0.700 | Balance=0.3043 | Turns=92.8 | ε=0.2697 | Loss=0.1857/0.3148 | Time=356s\n",
      "Episode  1700 | P1=0.140 P2=0.120 D=0.740 | Balance=0.8571 | Turns=92.8 | ε=0.2565 | Loss=0.1770/0.2948 | Time=381s\n",
      "Episode  1800 | P1=0.090 P2=0.130 D=0.780 | Balance=0.6923 | Turns=95.3 | ε=0.2440 | Loss=0.1800/0.2922 | Time=406s\n",
      "[monitor] ep=1800 mean=0.529 worst10%=0.468 var=0.0016 cov_min=14 cov_ok=False\n",
      "[QuantumNoise] ep=1800, draw_rate=0.900, injected_noise=0.0400\n",
      "Episode  1900 | P1=0.190 P2=0.070 D=0.740 | Balance=0.3684 | Turns=94.4 | ε=0.2321 | Loss=0.1798/0.2904 | Time=430s\n",
      "Episode  2000 | P1=0.150 P2=0.050 D=0.800 | Balance=0.3333 | Turns=95.8 | ε=0.2208 | Loss=0.1737/0.2703 | Time=456s\n",
      "Episode  2100 | P1=0.120 P2=0.030 D=0.850 | Balance=0.2500 | Turns=93.6 | ε=0.2100 | Loss=0.1478/0.2374 | Time=481s\n",
      "[monitor] ep=2100 mean=0.532 worst10%=0.470 var=0.0017 cov_min=16 cov_ok=False\n",
      "[QuantumNoise] ep=2100, draw_rate=0.720, injected_noise=0.0040\n",
      "Episode  2200 | P1=0.080 P2=0.030 D=0.890 | Balance=0.3750 | Turns=97.7 | ε=0.1998 | Loss=0.1369/0.2338 | Time=508s\n",
      "Episode  2300 | P1=0.080 P2=0.020 D=0.900 | Balance=0.2500 | Turns=97.3 | ε=0.1900 | Loss=0.1348/0.2704 | Time=534s\n",
      "Episode  2400 | P1=0.120 P2=0.050 D=0.830 | Balance=0.4167 | Turns=96.3 | ε=0.1808 | Loss=0.1403/0.2881 | Time=560s\n",
      "[monitor] ep=2400 mean=0.532 worst10%=0.466 var=0.0016 cov_min=21 cov_ok=False\n",
      "[QuantumNoise] ep=2400, draw_rate=0.940, injected_noise=0.0480\n",
      "Episode  2500 | P1=0.050 P2=0.000 D=0.950 | Balance=0.0000 | Turns=98.9 | ε=0.1719 | Loss=0.1380/0.2847 | Time=586s\n",
      "Episode  2600 | P1=0.040 P2=0.020 D=0.940 | Balance=0.5000 | Turns=98.6 | ε=0.1635 | Loss=0.1377/0.2883 | Time=612s\n",
      "Episode  2700 | P1=0.180 P2=0.100 D=0.720 | Balance=0.5556 | Turns=91.0 | ε=0.1556 | Loss=0.1325/0.2918 | Time=644s\n",
      "[monitor] ep=2700 mean=0.531 worst10%=0.476 var=0.0014 cov_min=26 cov_ok=False\n",
      "Episode  2800 | P1=0.140 P2=0.090 D=0.770 | Balance=0.6429 | Turns=95.6 | ε=0.1500 | Loss=0.1309/0.2832 | Time=676s\n",
      "Episode  2900 | P1=0.080 P2=0.030 D=0.890 | Balance=0.3750 | Turns=97.7 | ε=0.1500 | Loss=0.1265/0.2770 | Time=709s\n",
      "Episode  3000 | P1=0.070 P2=0.010 D=0.920 | Balance=0.1429 | Turns=99.0 | ε=0.1500 | Loss=0.1164/0.2598 | Time=737s\n",
      "[monitor] ep=3000 mean=0.533 worst10%=0.482 var=0.0012 cov_min=40 cov_ok=True\n",
      "[setup-ok] streak 1/1\n",
      "\n",
      "🛑 EarlyStop/setup at ep=3000 | mean=0.533, worst10%=0.482, var=0.0012\n",
      "[save] checkpoint -> checkpoints/ep003000_setup.pth\n",
      "\n",
      "=== 実験完了 ===\n",
      "Total Episodes: 3,000\n",
      "Total Time: 737.0s (0.2h)\n",
      "Average Episode Time: 0.246s\n",
      "\n",
      "✅ 実験完了!\n"
     ]
    }
   ],
   "source": [
    "# === 実験実行 ===\n",
    "print(\"🚀 CQCNN量子強化学習実験を開始します\")\n",
    "print(f\"設定: {config_path}\")\n",
    "print(f\"最大エピソード: {MAX_EPISODES:,}\")\n",
    "print(f\"目標: Balance ≥ 0.95, 50回連続達成で収束\")\n",
    "\n",
    "# 実験インスタンス生成\n",
    "experiment = CQCNNExperiment(config)\n",
    "\n",
    "# 実験実行\n",
    "final_episode, training_time, analysis = experiment.run_experiment()\n",
    "\n",
    "print(\"\\n✅ 実験完了!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exported: p1_final_ep3000.pth, p2_final_ep3000.pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "ckpt = torch.load(\"checkpoints/ep003000_setup.pth\", map_location=\"cpu\")\n",
    "torch.save(ckpt[\"p1_state\"], \"p1_final_ep3003.pth\")\n",
    "torch.save(ckpt[\"p2_state\"], \"p2_final_ep300.pth\")\n",
    "print(\"exported: p1_final_ep3000.pth, p2_final_ep3000.pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 実験結果を分析中...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'ConvergenceDetector' object has no attribute 'convergence_history'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     balance_history.append(balance)\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# 収束メトリクス\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m convergence_episodes = [h[\u001b[33m'\u001b[39m\u001b[33mepisode\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvergence_detector\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvergence_history\u001b[49m]\n\u001b[32m     31\u001b[39m convergence_balance = [h[\u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mbalance\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m experiment.convergence_detector.convergence_history]\n\u001b[32m     32\u001b[39m consecutive_good = [h[\u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mconsecutive_good\u001b[39m\u001b[33m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m experiment.convergence_detector.convergence_history]\n",
      "\u001b[31mAttributeError\u001b[39m: 'ConvergenceDetector' object has no attribute 'convergence_history'"
     ]
    }
   ],
   "source": [
    "# === 結果分析と可視化 ===\n",
    "print(\"📊 実験結果を分析中...\")\n",
    "\n",
    "# データ準備\n",
    "episodes = [r['episode'] for r in experiment.game_results]\n",
    "winners = [r['winner'] for r in experiment.game_results]\n",
    "turns = [r['turns'] for r in experiment.game_results]\n",
    "\n",
    "# 勝率計算（移動平均）\n",
    "window = 100\n",
    "p1_wins = [1 if w == 1 else 0 for w in winners]\n",
    "p2_wins = [1 if w == 2 else 0 for w in winners]\n",
    "draws = [1 if w is None else 0 for w in winners]\n",
    "\n",
    "p1_rate = np.convolve(p1_wins, np.ones(window)/window, mode='valid')\n",
    "p2_rate = np.convolve(p2_wins, np.ones(window)/window, mode='valid')\n",
    "draw_rate = np.convolve(draws, np.ones(window)/window, mode='valid')\n",
    "episodes_smooth = np.array(episodes[window-1:])\n",
    "\n",
    "# バランス計算\n",
    "balance_history = []\n",
    "for i in range(window-1, len(winners)):\n",
    "    recent = winners[i-window+1:i+1]\n",
    "    w1 = sum(1 for w in recent if w == 1)\n",
    "    w2 = sum(1 for w in recent if w == 2)\n",
    "    balance = min(w1, w2) / max(w1, w2) if max(w1, w2) > 0 else 1.0\n",
    "    balance_history.append(balance)\n",
    "\n",
    "# 収束メトリクス\n",
    "convergence_episodes = [h['episode'] for h in experiment.convergence_detector.convergence_history]\n",
    "convergence_balance = [h['metrics']['balance'] for h in experiment.convergence_detector.convergence_history]\n",
    "consecutive_good = [h['metrics']['consecutive_good'] for h in experiment.convergence_detector.convergence_history]\n",
    "\n",
    "# 可視化\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "fig.suptitle(f'CQCNN量子強化学習実験結果 (Episodes: {final_episode:,})', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. 勝率推移\n",
    "axes[0,0].plot(episodes_smooth, p1_rate, label='Player 1', color='blue', linewidth=2)\n",
    "axes[0,0].plot(episodes_smooth, p2_rate, label='Player 2', color='red', linewidth=2)\n",
    "axes[0,0].plot(episodes_smooth, draw_rate, label='Draws', color='gray', linewidth=2)\n",
    "axes[0,0].axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0,0].set_title('Win Rate Trends')\n",
    "axes[0,0].set_xlabel('Episode')\n",
    "axes[0,0].set_ylabel('Win Rate')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. バランス推移\n",
    "axes[0,1].plot(episodes_smooth, balance_history, color='green', linewidth=2)\n",
    "axes[0,1].axhline(y=0.95, color='red', linestyle='--', label='Target (0.95)')\n",
    "axes[0,1].axhline(y=0.995, color='orange', linestyle='--', label='Ultra-strict (0.995)')\n",
    "axes[0,1].set_title('Balance Evolution')\n",
    "axes[0,1].set_xlabel('Episode')\n",
    "axes[0,1].set_ylabel('Balance')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].set_ylim([0, 1.05])\n",
    "\n",
    "# 3. 収束進行\n",
    "if convergence_episodes:\n",
    "    axes[0,2].plot(convergence_episodes, consecutive_good, color='purple', linewidth=2, marker='o', markersize=3)\n",
    "    axes[0,2].axhline(y=50, color='red', linestyle='--', label='Target (50)')\n",
    "    axes[0,2].set_title('Convergence Progress')\n",
    "    axes[0,2].set_xlabel('Episode')\n",
    "    axes[0,2].set_ylabel('Consecutive Good Checks')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ゲーム長推移\n",
    "turns_smooth = np.convolve(turns, np.ones(window)/window, mode='valid')\n",
    "axes[1,0].plot(episodes_smooth, turns_smooth, color='brown', linewidth=2)\n",
    "axes[1,0].set_title('Average Game Length')\n",
    "axes[1,0].set_xlabel('Episode')\n",
    "axes[1,0].set_ylabel('Turns per Game')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. ε減衰\n",
    "if experiment.epsilon_history:\n",
    "    axes[1,1].plot(experiment.epsilon_history, color='orange', linewidth=2)\n",
    "    axes[1,1].set_title('Epsilon Decay')\n",
    "    axes[1,1].set_xlabel('Episode')\n",
    "    axes[1,1].set_ylabel('Epsilon')\n",
    "    axes[1,1].set_yscale('log')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. 損失推移\n",
    "if experiment.losses_1 and experiment.losses_2:\n",
    "    loss_episodes_1 = np.linspace(0, final_episode, len(experiment.losses_1))\n",
    "    loss_episodes_2 = np.linspace(0, final_episode, len(experiment.losses_2))\n",
    "    axes[1,2].plot(loss_episodes_1, experiment.losses_1, alpha=0.7, color='blue', label='Player 1')\n",
    "    axes[1,2].plot(loss_episodes_2, experiment.losses_2, alpha=0.7, color='red', label='Player 2')\n",
    "    \n",
    "    # 移動平均\n",
    "    if len(experiment.losses_1) > 50:\n",
    "        loss1_smooth = np.convolve(experiment.losses_1, np.ones(50)/50, mode='valid')\n",
    "        loss2_smooth = np.convolve(experiment.losses_2, np.ones(50)/50, mode='valid')\n",
    "        axes[1,2].plot(loss_episodes_1[49:], loss1_smooth, color='darkblue', linewidth=2)\n",
    "        axes[1,2].plot(loss_episodes_2[49:], loss2_smooth, color='darkred', linewidth=2)\n",
    "    \n",
    "    axes[1,2].set_title('Training Loss')\n",
    "    axes[1,2].set_xlabel('Episode')\n",
    "    axes[1,2].set_ylabel('Loss')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. 最終結果分布\n",
    "final_1000 = winners[-1000:] if len(winners) >= 1000 else winners\n",
    "w1_final = sum(1 for w in final_1000 if w == 1)\n",
    "w2_final = sum(1 for w in final_1000 if w == 2)\n",
    "d_final = sum(1 for w in final_1000 if w is None)\n",
    "\n",
    "categories = ['Player 1', 'Player 2', 'Draws']\n",
    "values = [w1_final, w2_final, d_final]\n",
    "colors = ['blue', 'red', 'gray']\n",
    "axes[2,0].pie(values, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[2,0].set_title(f'Final Results (Last {len(final_1000)} games)')\n",
    "\n",
    "# 8. 量子パラメータ分布\n",
    "params1 = experiment.cqcnn_1.quantum_params.detach().numpy().flatten()\n",
    "params2 = experiment.cqcnn_2.quantum_params.detach().numpy().flatten()\n",
    "axes[2,1].hist(params1, bins=30, alpha=0.7, label='Player 1', color='blue', density=True)\n",
    "axes[2,1].hist(params2, bins=30, alpha=0.7, label='Player 2', color='red', density=True)\n",
    "axes[2,1].set_title('Quantum Parameter Distribution')\n",
    "axes[2,1].set_xlabel('Parameter Value')\n",
    "axes[2,1].set_ylabel('Density')\n",
    "axes[2,1].legend()\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 9. エピソード時間\n",
    "if experiment.episode_times:\n",
    "    time_smooth = np.convolve(experiment.episode_times, np.ones(min(100, len(experiment.episode_times)))//min(100, len(experiment.episode_times)), mode='valid')\n",
    "    time_episodes = range(len(time_smooth))\n",
    "    axes[2,2].plot(time_episodes, time_smooth, color='green', linewidth=2)\n",
    "    axes[2,2].set_title('Episode Time')\n",
    "    axes[2,2].set_xlabel('Episode')\n",
    "    axes[2,2].set_ylabel('Time (seconds)')\n",
    "    axes[2,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"可視化完了!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "          CQCNN量子強化学習実験 - 最終レポート\n",
      "============================================================\n",
      "\n",
      "📈 基本統計\n",
      "  総エピソード数: 3,000\n",
      "  Player 1 勝利: 302 (10.1%)\n",
      "  Player 2 勝利: 117 (3.9%)\n",
      "  引き分け: 2,581 (86.0%)\n",
      "  平均ゲーム長: 96.5 ターン\n",
      "  実験時間: 737.0秒 (0.20時間)\n",
      "\n",
      "🎯 最終期間分析 (直近1000ゲーム)\n",
      "  Player 1: 96 (9.6%)\n",
      "  Player 2: 38 (3.8%)\n",
      "  引き分け: 866 (86.6%)\n",
      "  バランス: 0.3958\n",
      "  目標達成: ❌ NO (目標: ≥0.95)\n",
      "\n",
      "🔄 収束分析\n",
      "  ❌ 収束未達成\n",
      "  現在バランス: 0.1429\n",
      "  連続達成回数: 0/50\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'status'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  現在バランス: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis[\u001b[33m'\u001b[39m\u001b[33mbalance\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     44\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  連続達成回数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manalysis[\u001b[33m'\u001b[39m\u001b[33mmetrics\u001b[39m\u001b[33m'\u001b[39m][\u001b[33m'\u001b[39m\u001b[33mconsecutive_good\u001b[39m\u001b[33m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/50\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ステータス: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43manalysis\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmetrics\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mstatus\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     47\u001b[39m \u001b[38;5;66;03m# Ultra-strict実験との比較\u001b[39;00m\n\u001b[32m     48\u001b[39m ultra_strict_balance = \u001b[32m1.000\u001b[39m  \u001b[38;5;66;03m# Ultra-strict実験の結果\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'status'"
     ]
    }
   ],
   "source": [
    "# === 詳細分析レポート ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"          CQCNN量子強化学習実験 - 最終レポート\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 基本統計\n",
    "total_games = len(experiment.game_results)\n",
    "total_p1_wins = sum(1 for r in experiment.game_results if r['winner'] == 1)\n",
    "total_p2_wins = sum(1 for r in experiment.game_results if r['winner'] == 2)\n",
    "total_draws = sum(1 for r in experiment.game_results if r['winner'] is None)\n",
    "\n",
    "print(f\"\\n📈 基本統計\")\n",
    "print(f\"  総エピソード数: {total_games:,}\")\n",
    "print(f\"  Player 1 勝利: {total_p1_wins:,} ({total_p1_wins/total_games*100:.1f}%)\")\n",
    "print(f\"  Player 2 勝利: {total_p2_wins:,} ({total_p2_wins/total_games*100:.1f}%)\")\n",
    "print(f\"  引き分け: {total_draws:,} ({total_draws/total_games*100:.1f}%)\")\n",
    "print(f\"  平均ゲーム長: {np.mean(turns):.1f} ターン\")\n",
    "print(f\"  実験時間: {training_time:.1f}秒 ({training_time/3600:.2f}時間)\")\n",
    "\n",
    "# 最終期間の詳細分析\n",
    "final_period = min(1000, total_games)\n",
    "final_results = experiment.game_results[-final_period:]\n",
    "final_p1 = sum(1 for r in final_results if r['winner'] == 1)\n",
    "final_p2 = sum(1 for r in final_results if r['winner'] == 2)\n",
    "final_draws = sum(1 for r in final_results if r['winner'] is None)\n",
    "final_balance = min(final_p1, final_p2) / max(final_p1, final_p2) if max(final_p1, final_p2) > 0 else 1.0\n",
    "\n",
    "print(f\"\\n🎯 最終期間分析 (直近{final_period}ゲーム)\")\n",
    "print(f\"  Player 1: {final_p1} ({final_p1/final_period*100:.1f}%)\")\n",
    "print(f\"  Player 2: {final_p2} ({final_p2/final_period*100:.1f}%)\")\n",
    "print(f\"  引き分け: {final_draws} ({final_draws/final_period*100:.1f}%)\")\n",
    "print(f\"  バランス: {final_balance:.4f}\")\n",
    "print(f\"  目標達成: {'✅ YES' if final_balance >= 0.95 else '❌ NO'} (目標: ≥0.95)\")\n",
    "\n",
    "# 収束分析\n",
    "print(f\"\\n🔄 収束分析\")\n",
    "if analysis['converged']:\n",
    "    print(f\"  ✅ 収束達成!\")\n",
    "    print(f\"  最終バランス: {analysis['balance']:.4f}\")\n",
    "    print(f\"  連続達成回数: {analysis['metrics']['consecutive_good']}\")\n",
    "else:\n",
    "    print(f\"  ❌ 収束未達成\")\n",
    "    print(f\"  現在バランス: {analysis['balance']:.4f}\")\n",
    "    print(f\"  連続達成回数: {analysis['metrics']['consecutive_good']}/50\")\n",
    "    print(f\"  ステータス: {analysis['metrics']['status']}\")\n",
    "\n",
    "# Ultra-strict実験との比較\n",
    "ultra_strict_balance = 1.000  # Ultra-strict実験の結果\n",
    "ultra_strict_episodes = 46400\n",
    "ultra_strict_draws = 1.0\n",
    "\n",
    "print(f\"\\n⚖️  Ultra-strict実験との比較\")\n",
    "print(f\"  設定      │ Ultra-strict │ 現在の実験\")\n",
    "print(f\"  ─────────┼─────────────┼──────────────\")\n",
    "print(f\"  閾値      │     0.995    │    0.95\")\n",
    "print(f\"  エピソード│   {ultra_strict_episodes:,}    │   {total_games:,}\")\n",
    "print(f\"  バランス  │   {ultra_strict_balance:.3f}    │   {final_balance:.3f}\")\n",
    "print(f\"  引き分け率│   {ultra_strict_draws*100:.1f}%     │   {final_draws/final_period*100:.1f}%\")\n",
    "print(f\"  収束      │     未達成    │   {'達成' if analysis['converged'] else '未達成'}\")\n",
    "\n",
    "# 量子効果の分析\n",
    "quantum_std_1 = np.std(params1)\n",
    "quantum_std_2 = np.std(params2)\n",
    "quantum_diff = np.mean(np.abs(params1 - params2))\n",
    "\n",
    "print(f\"\\n🌌 量子効果分析\")\n",
    "print(f\"  Player 1 量子パラメータ範囲: [{params1.min():.3f}, {params1.max():.3f}]\")\n",
    "print(f\"  Player 2 量子パラメータ範囲: [{params2.min():.3f}, {params2.max():.3f}]\")\n",
    "print(f\"  Player 1 標準偏差: {quantum_std_1:.3f}\")\n",
    "print(f\"  Player 2 標準偏差: {quantum_std_2:.3f}\")\n",
    "print(f\"  プレイヤー間差異: {quantum_diff:.3f}\")\n",
    "\n",
    "# 学習効率\n",
    "if experiment.losses_1 and experiment.losses_2:\n",
    "    initial_loss_1 = np.mean(experiment.losses_1[:50]) if len(experiment.losses_1) >= 50 else 0\n",
    "    final_loss_1 = np.mean(experiment.losses_1[-50:]) if len(experiment.losses_1) >= 50 else 0\n",
    "    initial_loss_2 = np.mean(experiment.losses_2[:50]) if len(experiment.losses_2) >= 50 else 0\n",
    "    final_loss_2 = np.mean(experiment.losses_2[-50:]) if len(experiment.losses_2) >= 50 else 0\n",
    "    \n",
    "    print(f\"\\n📚 学習効率\")\n",
    "    print(f\"  Player 1 損失: {initial_loss_1:.4f} → {final_loss_1:.4f} ({((final_loss_1-initial_loss_1)/initial_loss_1*100):+.1f}%)\")\n",
    "    print(f\"  Player 2 損失: {initial_loss_2:.4f} → {final_loss_2:.4f} ({((final_loss_2-initial_loss_2)/initial_loss_2*100):+.1f}%)\")\n",
    "    print(f\"  総学習ステップ: {len(experiment.losses_1) + len(experiment.losses_2):,}\")\n",
    "\n",
    "# 実験設定サマリー\n",
    "print(f\"\\n⚙️  実験設定\")\n",
    "print(f\"  量子構成: {N_QUBITS}Q{N_LAYERS}L\")\n",
    "print(f\"  状態次元: {STATE_DIM}\")\n",
    "print(f\"  行動空間: {ACTION_DIM}\")\n",
    "print(f\"  バッチサイズ: {BATCH_SIZE}\")\n",
    "print(f\"  学習率: {LEARNING_RATE}\")\n",
    "print(f\"  ε減衰: {EPSILON_START} → {EPSILON_MIN} (係数: {EPSILON_DECAY})\")\n",
    "print(f\"  バッファサイズ: {BUFFER_SIZE:,}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"実験完了! 結果は上記の通りです。\")\n",
    "print(f\"ノートブック保存推奨: このセルの結果を記録してください。\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qugeister_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
