#!/usr/bin/env python3
"""
Fast Quantum Circuit Simulation Trainer

Quantum circuit-based trainer that achieves practical learning speeds
while maintaining quantum advantages for Geister game AI.
"""

import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import pennylane as qml
from collections import deque
import random
import time
import logging
from tqdm import tqdm
from typing import Dict, List, Tuple, Optional, Any, Union, Literal
import pickle
from functools import lru_cache
import hashlib
import matplotlib.pyplot as plt
from pathlib import Path

# Import updated FastQuantumCircuit
from .quantum_circuit import FastQuantumCircuit

logger = logging.getLogger(__name__)

# ===== Quantum circuits imported from quantum_circuit.py =====


# ===== Hybrid Quantum-Classical Neural Network =====
class FastQuantumNeuralNetwork(nn.Module):
    """Fast quantum neural network with user configuration support

    Enhanced with proper type hints, validation, and error handling.
    """

    def __init__(
        self,
        input_dim: int = 252,
        output_dim: int = 36,
        n_qubits: int = 4,
        n_layers: int = 2,
        embedding: Literal["angle", "amplitude"] = "angle",
        entanglement: Literal["linear", "circular", "full"] = "linear",
        device: str = "lightning.qubit",
    ) -> None:
        """Initialize quantum neural network

        Args:
            input_dim: Input dimension
            output_dim: Output dimension (usually 36 for 6x6 spatial mapping)
            n_qubits: Number of qubits (2-16 recommended)
            n_layers: Number of quantum layers (1-8 recommended)
            embedding: Embedding type ('angle' or 'amplitude')
            entanglement: Entanglement pattern ('linear', 'circular', 'full')
            device: Quantum device backend

        Raises:
            ValueError: If parameters are invalid
        """
        super().__init__()

        # Validate parameters
        self._validate_params(
            input_dim, output_dim, n_qubits, n_layers, embedding, entanglement
        )

        self.input_dim = input_dim
        self.output_dim = output_dim
        self.n_qubits = n_qubits
        self.n_layers = n_layers
        self.embedding = embedding
        self.entanglement = entanglement
        self.device = device

        logger.info(
            f"Initializing FastQuantumNeuralNetwork: {n_qubits}Q, {n_layers}L, {embedding}/{entanglement}"
        )

        self._build_network()

    def _validate_params(
        self,
        input_dim: int,
        output_dim: int,
        n_qubits: int,
        n_layers: int,
        embedding: str,
        entanglement: str,
    ) -> None:
        """Validate network parameters"""
        if input_dim <= 0:
            raise ValueError(f"input_dim must be positive, got {input_dim}")
        if output_dim <= 0:
            raise ValueError(f"output_dim must be positive, got {output_dim}")
        if not (2 <= n_qubits <= 16):
            raise ValueError(f"n_qubits must be 2-16, got {n_qubits}")
        if not (1 <= n_layers <= 8):
            raise ValueError(f"n_layers must be 1-8, got {n_layers}")
        if embedding not in ["angle", "amplitude"]:
            raise ValueError(
                f"embedding must be 'angle' or 'amplitude', got {embedding}"
            )
        if entanglement not in ["linear", "circular", "full"]:
            raise ValueError(
                f"entanglement must be 'linear', 'circular', or 'full', got {entanglement}"
            )

    def _build_network(self) -> None:
        """Build the hybrid quantum-classical network"""

        # 1. å¼·åŒ–ã•ã‚ŒãŸå‰å‡¦ç†CNNï¼ˆDeep Feature Extractionï¼‰
        self.preprocessor = nn.Sequential(
            # First CNN Block - Pattern Recognition
            nn.Linear(self.input_dim, 512),
            nn.LayerNorm(512),  # LayerNormä½¿ç”¨ã§ãƒãƒƒãƒã‚µã‚¤ã‚º1ã«å¯¾å¿œ
            nn.ReLU(),
            nn.Dropout(0.2),
            # Second CNN Block - Feature Combination
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            # Third CNN Block - Strategic Analysis
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            # Fourth CNN Block - Quantum Preparation
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.05),
            # Final Quantum Interface Layer
            nn.Linear(64, self.n_qubits),
            nn.Tanh(),  # é‡å­å›è·¯ã®å…¥åŠ›ç¯„å›²ã«æ­£è¦åŒ– [-1, 1]
        )

        # 2. å¼·åŒ–é‡å­å›è·¯å±¤ï¼ˆMulti-Layer Quantum Processingï¼‰
        self.quantum_layer = FastQuantumCircuit(
            n_qubits=self.n_qubits,
            n_layers=self.n_layers,
            embedding=self.embedding,
            entanglement=self.entanglement,
        )

        # é‡å­ç‰¹å¾´å¢—å¼·å±¤ï¼ˆQuantum Feature Enhancementï¼‰
        self.quantum_enhancer = nn.Sequential(
            nn.Linear(self.n_qubits, self.n_qubits * 2),
            nn.Tanh(),
            nn.Linear(self.n_qubits * 2, self.n_qubits),
            nn.Tanh(),
        )

        # 3. è¶…å¼·åŒ–å¾Œå‡¦ç†CNNï¼ˆDeep Q-Value Generationï¼‰
        self.postprocessor = nn.Sequential(
            # First Expansion Block - Quantum Feature Amplification
            nn.Linear(self.n_qubits, 128),
            nn.LayerNorm(128),  # LayerNormä½¿ç”¨ã§ãƒãƒƒãƒã‚µã‚¤ã‚º1ã«å¯¾å¿œ
            nn.ReLU(),
            nn.Dropout(0.1),
            # Second Expansion Block - Strategic Pattern Formation
            nn.Linear(128, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            # Third Expansion Block - Spatial Understanding
            nn.Linear(256, 512),
            nn.LayerNorm(512),
            nn.ReLU(),
            nn.Dropout(0.2),
            # Fourth Block - Advanced Strategy Synthesis
            nn.Linear(512, 256),
            nn.LayerNorm(256),
            nn.ReLU(),
            nn.Dropout(0.15),
            # Fifth Block - Q-Value Refinement
            nn.Linear(256, 128),
            nn.LayerNorm(128),
            nn.ReLU(),
            nn.Dropout(0.1),
            # Sixth Block - Spatial Mapping Preparation
            nn.Linear(128, 64),
            nn.LayerNorm(64),
            nn.ReLU(),
            nn.Dropout(0.05),
            # Final Q-Value Generation Layer
            nn.Linear(64, self.output_dim),  # 36 outputs for 6x6 Q-value map
            nn.Tanh(),  # Normalize Q-values to [-1, 1] range for stability
        )

        # 4. å¼·åŒ–é‡å­å›è·¯ã®é‡ã¿ï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°å¯¾å¿œï¼‰
        self.quantum_weights = nn.Parameter(
            torch.randn(self.n_layers, self.n_qubits, 2) * 0.1
        )  # ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šãƒ¬ã‚¤ãƒ¤ãƒ¼åˆ†ã®é‡ã¿

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ãƒãƒƒãƒå‡¦ç†ã®æœ€é©åŒ–
        batch_size = x.shape[0] if x.dim() > 1 else 1
        if x.dim() == 1:
            x = x.unsqueeze(0)

        # 1. å¼·åŒ–å‰å‡¦ç†ã§æ·±å±¤ç‰¹å¾´æŠ½å‡ºï¼ˆ252â†’4ï¼‰
        compressed = self.preprocessor(x)

        # 2. å¼·åŒ–é‡å­å›è·¯å‡¦ç†ï¼ˆ4â†’4ï¼‰
        quantum_outputs = []
        for i in range(batch_size):
            # å„ã‚µãƒ³ãƒ—ãƒ«ã‚’å¼·åŒ–é‡å­å›è·¯ã«é€šã™
            quantum_input = compressed[i] * np.pi  # [-Ï€, Ï€]ã«ã‚¹ã‚±ãƒ¼ãƒªãƒ³ã‚°
            quantum_output = self.quantum_layer.forward(
                quantum_input, self.quantum_weights
            )
            quantum_outputs.append(quantum_output)

        # 3. é‡å­å‡ºåŠ›ã‚’ãƒ†ãƒ³ã‚½ãƒ«ã«å¤‰æ›
        if batch_size > 1:
            quantum_features = torch.stack(
                [torch.tensor(out, dtype=torch.float32) for out in quantum_outputs]
            )
        else:
            quantum_features = torch.tensor(
                quantum_outputs[0], dtype=torch.float32
            ).unsqueeze(0)

        # 4. é‡å­ç‰¹å¾´å¢—å¼·å‡¦ç†ï¼ˆ4â†’8â†’4ï¼‰
        enhanced_features = self.quantum_enhancer(quantum_features)

        # 5. é‡å­ç‰¹å¾´ã¨å¢—å¼·ç‰¹å¾´ã‚’çµåˆ
        combined_features = quantum_features + enhanced_features

        # 6. è¶…å¼·åŒ–å¾Œå‡¦ç†ã§36æ¬¡å…ƒQå€¤ãƒãƒƒãƒ—ç”Ÿæˆï¼ˆ4â†’36ï¼‰
        output = self.postprocessor(combined_features)

        return output.squeeze(0) if batch_size == 1 else output

    def get_qvalue_map(self, x: torch.Tensor) -> torch.Tensor:
        """36æ¬¡å…ƒå‡ºåŠ›ã‚’6x6ã®Qå€¤ãƒãƒƒãƒ—ã«å¤‰æ›"""
        output = self.forward(x)
        if output.dim() == 1:
            # Single sample: reshape to 6x6
            return output.reshape(6, 6)
        else:
            # Batch: reshape each sample to 6x6
            return output.reshape(-1, 6, 6)

    def get_action_from_qmap(self, x: torch.Tensor) -> torch.Tensor:
        """Qå€¤ãƒãƒƒãƒ—ã‹ã‚‰æœ€é©è¡Œå‹•ã‚’é¸æŠï¼ˆå¾“æ¥ã®5è¡Œå‹•ã‚·ã‚¹ãƒ†ãƒ ç”¨ï¼‰"""
        qvalue_map = self.get_qvalue_map(x)
        if qvalue_map.dim() == 2:  # Single sample
            # 6x6ãƒãƒƒãƒ—ã‹ã‚‰ä»£è¡¨çš„ãª5ã¤ã®é ˜åŸŸã®æœ€å¤§å€¤ã‚’å–å¾—
            regions = {
                0: qvalue_map[0:2, 0:3].max(),  # å·¦ä¸Šé ˜åŸŸ
                1: qvalue_map[0:2, 3:6].max(),  # å³ä¸Šé ˜åŸŸ
                2: qvalue_map[2:4, 1:5].max(),  # ä¸­å¤®é ˜åŸŸ
                3: qvalue_map[4:6, 0:3].max(),  # å·¦ä¸‹é ˜åŸŸ
                4: qvalue_map[4:6, 3:6].max(),  # å³ä¸‹é ˜åŸŸ
            }
            # 5ã¤ã®è¡Œå‹•ã«å¯¾å¿œã™ã‚‹Qå€¤ã‚’è¿”ã™
            return torch.tensor([regions[i] for i in range(5)])
        else:  # Batch
            batch_size = qvalue_map.shape[0]
            batch_actions = []
            for i in range(batch_size):
                single_map = qvalue_map[i]
                regions = {
                    0: single_map[0:2, 0:3].max(),
                    1: single_map[0:2, 3:6].max(),
                    2: single_map[2:4, 1:5].max(),
                    3: single_map[4:6, 0:3].max(),
                    4: single_map[4:6, 3:6].max(),
                }
                batch_actions.append([regions[i] for i in range(5)])
            return torch.tensor(batch_actions)


# ===== é«˜é€Ÿå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ  =====
class FastQuantumTrainer:
    """é«˜é€Ÿé‡å­å›è·¯å­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, model: FastQuantumNeuralNetwork, lr: float = 0.001) -> None:
        self.model = model
        self.optimizer = optim.Adam(model.parameters(), lr=lr)
        self.replay_buffer = deque(maxlen=1000)

        # å­¦ç¿’çµ±è¨ˆ
        self.losses = []
        self.rewards = []
        self.episode_losses = []  # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã®å¹³å‡ãƒ­ã‚¹
        self.loss_history = []    # è©³ç´°ãªãƒ­ã‚¹å±¥æ­´

    def train_step(self, batch_size: int = 8) -> Optional[float]:
        """åŠ¹ç‡çš„ãªãƒãƒƒãƒå­¦ç¿’"""
        if len(self.replay_buffer) < batch_size:
            return None

        # ãƒŸãƒ‹ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°
        batch = random.sample(self.replay_buffer, batch_size)
        states, actions, rewards, next_states, dones = zip(*batch)

        # ãƒ†ãƒ³ã‚½ãƒ«å¤‰æ›ï¼ˆåŠ¹ç‡åŒ–ï¼‰
        states = torch.cat(states)
        actions = torch.tensor(actions, dtype=torch.long)
        rewards = torch.tensor(rewards, dtype=torch.float32)
        next_states = torch.cat(next_states)
        dones = torch.tensor(dones, dtype=torch.float32)

        # Qå€¤è¨ˆç®—ï¼ˆ36æ¬¡å…ƒå‡ºåŠ›ã‹ã‚‰5è¡Œå‹•ç”¨Qå€¤ã‚’æŠ½å‡ºï¼‰
        current_q_actions = self.model.get_action_from_qmap(states)
        current_q = current_q_actions.gather(1, actions.unsqueeze(1)).squeeze()

        with torch.no_grad():
            next_q_actions = self.model.get_action_from_qmap(next_states)
            next_q = next_q_actions.max(1)[0]
            target_q = rewards + 0.99 * next_q * (1 - dones)

        # æå¤±è¨ˆç®—
        loss = nn.MSELoss()(current_q, target_q)

        # æœ€é©åŒ–
        self.optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
        self.optimizer.step()

        self.losses.append(loss.item())
        self.loss_history.append(loss.item())
        return loss.item()

    def plot_training_progress(self, save_path: str = None, show_plot: bool = True) -> None:
        """å­¦ç¿’é€²æ—ã‚’ãƒ—ãƒ­ãƒƒãƒˆã—ã¦è¡¨ç¤ºãƒ»ä¿å­˜"""
        if len(self.losses) == 0 and len(self.rewards) == 0:
            print("ãƒ—ãƒ­ãƒƒãƒˆã™ã‚‹ãƒ‡ãƒ¼ã‚¿ãŒã‚ã‚Šã¾ã›ã‚“")
            return

        # å›³ã®ã‚µã‚¤ã‚ºã¨ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆè¨­å®š
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Quantum Neural Network Training Progress', fontsize=16)

        # 1. ãƒ­ã‚¹å±¥æ­´
        if len(self.losses) > 0:
            axes[0, 0].plot(self.losses, alpha=0.7, label='Training Loss')
            if len(self.losses) > 100:
                # ç§»å‹•å¹³å‡ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
                window = min(100, len(self.losses) // 10)
                smoothed_loss = np.convolve(self.losses, np.ones(window)/window, mode='valid')
                axes[0, 0].plot(range(window-1, len(self.losses)), smoothed_loss,
                               color='red', linewidth=2, label=f'Moving Avg ({window})')
            axes[0, 0].set_xlabel('Training Steps')
            axes[0, 0].set_ylabel('MSE Loss')
            axes[0, 0].set_title('Training Loss Over Time')
            axes[0, 0].legend()
            axes[0, 0].grid(True, alpha=0.3)

        # 2. å ±é…¬å±¥æ­´
        if len(self.rewards) > 0:
            axes[0, 1].plot(self.rewards, alpha=0.7, label='Episode Rewards')
            if len(self.rewards) > 100:
                # ç§»å‹•å¹³å‡ã§ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
                window = min(100, len(self.rewards) // 10)
                smoothed_rewards = np.convolve(self.rewards, np.ones(window)/window, mode='valid')
                axes[0, 1].plot(range(window-1, len(self.rewards)), smoothed_rewards,
                               color='green', linewidth=2, label=f'Moving Avg ({window})')
            axes[0, 1].set_xlabel('Episodes')
            axes[0, 1].set_ylabel('Reward')
            axes[0, 1].set_title('Episode Rewards Over Time')
            axes[0, 1].legend()
            axes[0, 1].grid(True, alpha=0.3)

        # 3. ãƒ­ã‚¹åˆ†å¸ƒï¼ˆãƒ’ã‚¹ãƒˆã‚°ãƒ©ãƒ ï¼‰
        if len(self.losses) > 0:
            axes[1, 0].hist(self.losses, bins=50, alpha=0.7, edgecolor='black')
            axes[1, 0].axvline(np.mean(self.losses), color='red', linestyle='--',
                              label=f'Mean: {np.mean(self.losses):.4f}')
            axes[1, 0].axvline(np.median(self.losses), color='green', linestyle='--',
                              label=f'Median: {np.median(self.losses):.4f}')
            axes[1, 0].set_xlabel('Loss Value')
            axes[1, 0].set_ylabel('Frequency')
            axes[1, 0].set_title('Loss Distribution')
            axes[1, 0].legend()
            axes[1, 0].grid(True, alpha=0.3)

        # 4. åæŸåˆ†æï¼ˆæœ€è¿‘ã®ãƒ­ã‚¹å‚¾å‘ï¼‰
        if len(self.losses) > 100:
            # æœ€è¿‘ã®ãƒ­ã‚¹å‚¾å‘ã‚’åˆ†æ
            recent_losses = self.losses[-1000:] if len(self.losses) > 1000 else self.losses
            axes[1, 1].plot(recent_losses, alpha=0.7, label='Recent Loss')

            # ç·šå½¢å›å¸°ã§å‚¾å‘ã‚’åˆ†æ
            x = np.arange(len(recent_losses))
            z = np.polyfit(x, recent_losses, 1)
            p = np.poly1d(z)
            axes[1, 1].plot(x, p(x), "r--", alpha=0.8,
                           label=f'Trend (slope: {z[0]:.6f})')

            axes[1, 1].set_xlabel('Recent Training Steps')
            axes[1, 1].set_ylabel('Loss')
            axes[1, 1].set_title('Recent Loss Trend (Convergence Analysis)')
            axes[1, 1].legend()
            axes[1, 1].grid(True, alpha=0.3)
        else:
            axes[1, 1].text(0.5, 0.5, 'Not enough data\nfor convergence analysis',
                           horizontalalignment='center', verticalalignment='center',
                           transform=axes[1, 1].transAxes, fontsize=12)
            axes[1, 1].set_title('Convergence Analysis')

        plt.tight_layout()

        # ä¿å­˜
        if save_path:
            Path(save_path).parent.mkdir(parents=True, exist_ok=True)
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"ğŸ“Š å­¦ç¿’é€²æ—ã‚°ãƒ©ãƒ•ã‚’ä¿å­˜: {save_path}")

        # è¡¨ç¤º
        if show_plot:
            plt.show()
        else:
            plt.close()

    def analyze_convergence(self) -> Dict[str, Any]:
        """åæŸçŠ¶æ³ã‚’æ•°å€¤çš„ã«åˆ†æ"""
        if len(self.losses) < 100:
            return {"status": "insufficient_data", "message": "åˆ†æã«ã¯æœ€ä½100ã‚¹ãƒ†ãƒƒãƒ—å¿…è¦"}

        analysis = {}

        # åŸºæœ¬çµ±è¨ˆ
        analysis["total_steps"] = len(self.losses)
        analysis["mean_loss"] = np.mean(self.losses)
        analysis["std_loss"] = np.std(self.losses)
        analysis["min_loss"] = np.min(self.losses)
        analysis["max_loss"] = np.max(self.losses)

        # åæŸå‚¾å‘åˆ†æï¼ˆæœ€è¿‘ã®1000ã‚¹ãƒ†ãƒƒãƒ—ï¼‰
        recent_window = min(1000, len(self.losses) // 4)
        recent_losses = self.losses[-recent_window:]

        # ç·šå½¢å›å¸°ã§å‚¾å‘ã‚’åˆ†æ
        x = np.arange(len(recent_losses))
        slope, intercept = np.polyfit(x, recent_losses, 1)

        analysis["recent_slope"] = slope
        analysis["recent_mean"] = np.mean(recent_losses)
        analysis["recent_std"] = np.std(recent_losses)

        # åæŸåˆ¤å®š
        if abs(slope) < 1e-6:  # å‚¾ããŒã»ã¼0
            analysis["convergence_status"] = "converged"
        elif slope < -1e-4:  # æ˜ç¢ºã«æ¸›å°‘å‚¾å‘
            analysis["convergence_status"] = "improving"
        elif slope > 1e-4:   # æ˜ç¢ºã«å¢—åŠ å‚¾å‘
            analysis["convergence_status"] = "diverging"
        else:
            analysis["convergence_status"] = "stable"

        # å¤‰å‹•ã®å®‰å®šæ€§
        if len(self.losses) > 500:
            first_half_std = np.std(self.losses[:len(self.losses)//2])
            second_half_std = np.std(self.losses[len(self.losses)//2:])
            analysis["stability_ratio"] = second_half_std / first_half_std

        return analysis

    def print_convergence_report(self) -> None:
        """åæŸãƒ¬ãƒãƒ¼ãƒˆã‚’è¡¨ç¤º"""
        analysis = self.analyze_convergence()

        if analysis.get("status") == "insufficient_data":
            print(analysis["message"])
            return

        print("\n" + "="*60)
        print("ğŸ“ˆ CONVERGENCE ANALYSIS REPORT")
        print("="*60)
        print(f"ç·å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—æ•°: {analysis['total_steps']:,}")
        print(f"å¹³å‡ãƒ­ã‚¹: {analysis['mean_loss']:.6f}")
        print(f"ãƒ­ã‚¹æ¨™æº–åå·®: {analysis['std_loss']:.6f}")
        print(f"æœ€å°ãƒ­ã‚¹: {analysis['min_loss']:.6f}")
        print(f"æœ€å¤§ãƒ­ã‚¹: {analysis['max_loss']:.6f}")
        print()
        print("ğŸ“Š æœ€è¿‘ã®å‚¾å‘åˆ†æ:")
        print(f"å‚¾ã (slope): {analysis['recent_slope']:.8f}")
        print(f"æœ€è¿‘ã®å¹³å‡ãƒ­ã‚¹: {analysis['recent_mean']:.6f}")
        print(f"æœ€è¿‘ã®æ¨™æº–åå·®: {analysis['recent_std']:.6f}")
        print()

        status_emoji = {
            "converged": "âœ…",
            "improving": "ğŸ“ˆ",
            "stable": "ğŸ“Š",
            "diverging": "ğŸ“‰"
        }

        status_msg = {
            "converged": "åæŸæ¸ˆã¿ - ãƒ­ã‚¹ãŒå®‰å®š",
            "improving": "æ”¹å–„ä¸­ - ãƒ­ã‚¹ãŒæ¸›å°‘å‚¾å‘",
            "stable": "å®‰å®š - ãƒ­ã‚¹ãŒã»ã¼ä¸€å®š",
            "diverging": "ç™ºæ•£å‚¾å‘ - è¦æ³¨æ„"
        }

        status = analysis['convergence_status']
        print(f"ğŸ¯ åæŸçŠ¶æ³: {status_emoji[status]} {status_msg[status]}")

        if 'stability_ratio' in analysis:
            if analysis['stability_ratio'] < 0.8:
                print("ğŸ“‰ å­¦ç¿’ãŒå®‰å®šåŒ–ã—ã¦ã„ã‚‹å‚¾å‘")
            elif analysis['stability_ratio'] > 1.2:
                print("ğŸ“ˆ å­¦ç¿’ãŒä¸å®‰å®šåŒ–ã—ã¦ã„ã‚‹å¯èƒ½æ€§")
            else:
                print("ğŸ“Š å­¦ç¿’ã®å®‰å®šæ€§ã¯é©åˆ‡")

        print("="*60)


# ===== ãƒ¡ã‚¤ãƒ³å­¦ç¿’ãƒ«ãƒ¼ãƒ—ï¼ˆå»ƒæ­¢äºˆå®š - å®Ÿéš›ã®Geisterç’°å¢ƒã«ç½®ãæ›ãˆã‚‹ï¼‰ =====
def train_fast_quantum(
    episodes: int = 1000, n_qubits: int = 4
) -> Tuple[FastQuantumNeuralNetwork, List[float]]:
    """é«˜é€Ÿé‡å­å›è·¯å­¦ç¿’ã®å®Ÿè¡Œ

    WARNING: This function uses fake random rewards and should be replaced
    with real Geister game environment for meaningful learning.
    """

    print("=" * 60)
    print("âš ï¸  WARNING: PLACEHOLDER SIMULATION - NOT REAL GEISTER GAME")
    print("ğŸš€ é«˜é€Ÿé‡å­å›è·¯ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å­¦ç¿’")
    print("=" * 60)
    print(f"é‡å­ãƒ“ãƒƒãƒˆæ•°: {n_qubits}")
    print(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {episodes}")
    print("âš ï¸  ã“ã®å­¦ç¿’ã¯å®Ÿéš›ã®ã‚²ãƒ¼ãƒ ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    print("=" * 60)

    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
    model = FastQuantumNeuralNetwork(n_qubits=n_qubits)
    trainer = FastQuantumTrainer(model)

    # é€²æ—è¡¨ç¤º
    episode_rewards = []
    start_time = time.time()

    with tqdm(total=episodes, desc="é‡å­å›è·¯å­¦ç¿’") as pbar:
        for episode in range(episodes):
            # ç’°å¢ƒãƒªã‚»ãƒƒãƒˆï¼ˆç°¡ç•¥åŒ–ï¼‰
            state = torch.randn(1, 252)
            episode_reward = 0
            done = False
            steps = 0

            while not done and steps < 100:
                # è¡Œå‹•é¸æŠï¼ˆÎµ-greedyï¼‰
                epsilon = max(0.01, 0.1 * (0.995**episode))
                if random.random() < epsilon:
                    action = random.randrange(5)
                else:
                    with torch.no_grad():
                        q_values = model.get_action_from_qmap(state)
                        action = q_values.argmax().item()

                # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå®Ÿéš›ã®Geisterã‚²ãƒ¼ãƒ ç’°å¢ƒã«ç½®ãæ›ãˆã‚‹å¿…è¦ï¼‰
                # WARNING: This is placeholder simulation - replace with real Geister game
                next_state = torch.randn(1, 252)
                reward = 0.0  # Real game reward will replace this
                done = random.random() < 0.1

                # çµŒé¨“ã‚’ä¿å­˜
                trainer.replay_buffer.append((state, action, reward, next_state, done))

                # å­¦ç¿’
                loss = trainer.train_step()

                # æ›´æ–°
                state = next_state
                episode_reward += reward
                steps += 1

            episode_rewards.append(episode_reward)
            trainer.rewards.append(episode_reward)

            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ã®å¹³å‡ãƒ­ã‚¹ã‚’è¨˜éŒ²
            if len(trainer.losses) > 0:
                episode_loss = np.mean(trainer.losses[-steps:]) if steps > 0 else 0
                trainer.episode_losses.append(episode_loss)

            # é€²æ—æ›´æ–°
            if (episode + 1) % 10 == 0:
                avg_reward = np.mean(episode_rewards[-10:])
                elapsed = time.time() - start_time
                speed = (episode + 1) / elapsed

                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
                cache_size = len(model.quantum_layer.lookup_table)

                pbar.set_postfix(
                    {
                        "Avg Reward": f"{avg_reward:.2f}",
                        "Speed": f"{speed:.1f} eps/s",
                        "Cache": f"{cache_size}/{model.quantum_layer.cache_size}",
                        "Îµ": f"{epsilon:.3f}",
                    }
                )

            pbar.update(1)

    # çµæœè¡¨ç¤º
    total_time = time.time() - start_time
    print(f"\nâœ… å­¦ç¿’å®Œäº†ï¼")
    print(f"ç·æ™‚é–“: {total_time:.1f}ç§’")
    print(f"é€Ÿåº¦: {episodes/total_time:.1f} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰/ç§’")
    print(f"æœ€çµ‚å ±é…¬: {np.mean(episode_rewards[-100:]):.2f}")

    # åæŸåˆ†æã¨å¯è¦–åŒ–
    print("\n" + "="*60)
    print("ğŸ“Š å­¦ç¿’çµæœåˆ†æ")
    print("="*60)

    # åæŸãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º
    trainer.print_convergence_report()

    # ã‚°ãƒ©ãƒ•ä¿å­˜
    plot_save_path = f"training_results_{episodes}_episodes.png"
    trainer.plot_training_progress(save_path=plot_save_path, show_plot=False)

    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    torch.save(
        {
            "model_state_dict": model.state_dict(),
            "quantum_cache": model.quantum_layer.lookup_table,
            "rewards": episode_rewards,
            "losses": trainer.losses,
            "episode_losses": trainer.episode_losses,
            "training_analysis": trainer.analyze_convergence(),
        },
        "fast_quantum_model.pth",
    )

    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ã‚’ 'fast_quantum_model.pth' ã¨ã—ã¦ä¿å­˜")
    print(f"ğŸ“Š å­¦ç¿’ã‚°ãƒ©ãƒ•ã‚’ '{plot_save_path}' ã¨ã—ã¦ä¿å­˜")

    return model, episode_rewards


# ===== 100000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åæŸç¢ºèª =====
def train_convergence_test(episodes: int = 100000, n_qubits: int = 4, save_interval: int = 10000) -> Tuple[FastQuantumNeuralNetwork, List[float]]:
    """100000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å­¦ç¿’ã§åæŸç¢ºèª

    WARNING: This function uses fake random rewards and should be replaced
    with real Geister game environment for meaningful learning.
    """

    print("=" * 80)
    print("âš ï¸  WARNING: PLACEHOLDER SIMULATION - NOT REAL GEISTER GAME")
    print("ğŸ§ª CONVERGENCE TEST: 100000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å­¦ç¿’")
    print("=" * 80)
    print(f"é‡å­ãƒ“ãƒƒãƒˆæ•°: {n_qubits}")
    print(f"ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {episodes:,}")
    print(f"ä¸­é–“ä¿å­˜é–“éš”: {save_interval:,}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
    print("âš ï¸  ã“ã®å­¦ç¿’ã¯å®Ÿéš›ã®ã‚²ãƒ¼ãƒ ã§ã¯ã‚ã‚Šã¾ã›ã‚“")
    print("=" * 80)

    # ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¬ãƒ¼ãƒŠãƒ¼ã®åˆæœŸåŒ–
    model = FastQuantumNeuralNetwork(n_qubits=n_qubits)
    trainer = FastQuantumTrainer(model, lr=0.001)

    # å­¦ç¿’çµ±è¨ˆ
    episode_rewards = []
    start_time = time.time()

    # ä¸­é–“çµæœä¿å­˜ç”¨
    checkpoints = []

    with tqdm(total=episodes, desc="åæŸãƒ†ã‚¹ãƒˆå­¦ç¿’") as pbar:
        for episode in range(episodes):
            # ç’°å¢ƒãƒªã‚»ãƒƒãƒˆ
            state = torch.randn(1, 252)
            episode_reward = 0
            done = False
            steps = 0

            while not done and steps < 100:
                # è¡Œå‹•é¸æŠï¼ˆÎµ-greedy with longer decayï¼‰
                epsilon = max(0.001, 0.1 * (0.9999**episode))  # ã‚ˆã‚Šé•·ã„æ¸›è¡°
                if random.random() < epsilon:
                    action = random.randrange(5)
                else:
                    with torch.no_grad():
                        q_values = model.get_action_from_qmap(state)
                        action = q_values.argmax().item()

                # ç’°å¢ƒã‚¹ãƒ†ãƒƒãƒ—ï¼ˆå®Ÿéš›ã®Geisterã‚²ãƒ¼ãƒ ç’°å¢ƒã«ç½®ãæ›ãˆã‚‹å¿…è¦ï¼‰
                # WARNING: This is placeholder simulation - replace with real Geister game
                next_state = torch.randn(1, 252)
                reward = 0.0  # Real game reward will replace this
                done = random.random() < 0.05  # ã‚ˆã‚Šé•·ã„ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰

                # çµŒé¨“ã‚’ä¿å­˜
                trainer.replay_buffer.append((state, action, reward, next_state, done))

                # å­¦ç¿’
                loss = trainer.train_step(batch_size=16)  # ã‚ˆã‚Šå¤§ããªãƒãƒƒãƒã‚µã‚¤ã‚º

                # æ›´æ–°
                state = next_state
                episode_reward += reward
                steps += 1

            episode_rewards.append(episode_reward)
            trainer.rewards.append(episode_reward)

            # ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰çµ‚äº†æ™‚ã®å¹³å‡ãƒ­ã‚¹ã‚’è¨˜éŒ²
            if len(trainer.losses) > 0:
                episode_loss = np.mean(trainer.losses[-steps:]) if steps > 0 else 0
                trainer.episode_losses.append(episode_loss)

            # ä¸­é–“ä¿å­˜ã¨ãƒ¬ãƒãƒ¼ãƒˆ
            if (episode + 1) % save_interval == 0:
                elapsed = time.time() - start_time
                speed = (episode + 1) / elapsed

                print(f"\n{'='*60}")
                print(f"ğŸ“Š ä¸­é–“ãƒ¬ãƒãƒ¼ãƒˆ: {episode + 1:,}/{episodes:,} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰")
                print(f"{'='*60}")
                print(f"çµŒéæ™‚é–“: {elapsed/3600:.1f}æ™‚é–“")
                print(f"å­¦ç¿’é€Ÿåº¦: {speed:.1f} eps/s")
                print(f"æ¨å®šæ®‹ã‚Šæ™‚é–“: {(episodes - episode - 1) / speed / 3600:.1f}æ™‚é–“")

                # åæŸåˆ†æ
                if len(trainer.losses) > 1000:
                    analysis = trainer.analyze_convergence()
                    print(f"ç¾åœ¨ã®åæŸçŠ¶æ³: {analysis['convergence_status']}")
                    print(f"å¹³å‡ãƒ­ã‚¹: {analysis['mean_loss']:.6f}")
                    print(f"æœ€è¿‘ã®å‚¾ã: {analysis['recent_slope']:.8f}")

                # ä¸­é–“ä¿å­˜
                checkpoint_data = {
                    "episode": episode + 1,
                    "model_state_dict": model.state_dict(),
                    "losses": trainer.losses.copy(),
                    "rewards": trainer.rewards.copy(),
                    "analysis": trainer.analyze_convergence() if len(trainer.losses) > 100 else None,
                    "timestamp": time.time()
                }
                checkpoints.append(checkpoint_data)

                # ã‚°ãƒ©ãƒ•ä¿å­˜
                plot_path = f"convergence_test_{episode + 1}.png"
                trainer.plot_training_progress(save_path=plot_path, show_plot=False)
                print(f"ğŸ“ˆ ä¸­é–“ã‚°ãƒ©ãƒ•ä¿å­˜: {plot_path}")

            # é€²æ—æ›´æ–°
            if (episode + 1) % 1000 == 0:
                avg_reward = np.mean(episode_rewards[-1000:])
                avg_loss = np.mean(trainer.losses[-1000:]) if len(trainer.losses) > 1000 else 0
                cache_size = len(model.quantum_layer.lookup_table)

                pbar.set_postfix({
                    "Avg Reward": f"{avg_reward:.2f}",
                    "Avg Loss": f"{avg_loss:.4f}",
                    "Cache": f"{cache_size}",
                    "Îµ": f"{epsilon:.4f}",
                })

            pbar.update(1)

    # æœ€çµ‚çµæœ
    total_time = time.time() - start_time
    print(f"\n{'='*80}")
    print("ğŸ¯ CONVERGENCE TEST å®Œäº†ï¼")
    print(f"{'='*80}")
    print(f"ç·æ™‚é–“: {total_time/3600:.1f}æ™‚é–“ ({total_time:.0f}ç§’)")
    print(f"å¹³å‡é€Ÿåº¦: {episodes/total_time:.1f} ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰/ç§’")
    print(f"æœ€çµ‚å ±é…¬: {np.mean(episode_rewards[-1000:]):.4f}")

    # æœ€çµ‚åæŸåˆ†æ
    trainer.print_convergence_report()

    # æœ€çµ‚ã‚°ãƒ©ãƒ•ä¿å­˜
    final_plot_path = f"convergence_test_final_{episodes}.png"
    trainer.plot_training_progress(save_path=final_plot_path, show_plot=False)

    # æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    final_save_data = {
        "episodes": episodes,
        "model_state_dict": model.state_dict(),
        "quantum_cache": model.quantum_layer.lookup_table,
        "rewards": episode_rewards,
        "losses": trainer.losses,
        "episode_losses": trainer.episode_losses,
        "final_analysis": trainer.analyze_convergence(),
        "checkpoints": checkpoints,
        "total_time": total_time,
        "final_speed": episodes/total_time
    }

    final_model_path = f"convergence_test_model_{episodes}.pth"
    torch.save(final_save_data, final_model_path)

    print(f"ğŸ’¾ æœ€çµ‚ãƒ¢ãƒ‡ãƒ«ä¿å­˜: {final_model_path}")
    print(f"ğŸ“Š æœ€çµ‚ã‚°ãƒ©ãƒ•ä¿å­˜: {final_plot_path}")
    print(f"ğŸ“ ä¸­é–“ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆ: {len(checkpoints)}å€‹ä¿å­˜æ¸ˆã¿")

    return model, episode_rewards


# ===== ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ =====
def benchmark_quantum_speedup() -> None:
    """é‡å­å›è·¯ã®é«˜é€ŸåŒ–åŠ¹æœã‚’æ¸¬å®š"""

    print("\n" + "=" * 60)
    print("ğŸ“Š é‡å­å›è·¯é«˜é€ŸåŒ–ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯")
    print("=" * 60)

    # é€šå¸¸ã®é‡å­å›è·¯
    print("\n1. é€šå¸¸ã®é‡å­å›è·¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—ï¼‰:")
    circuit_nocache = FastQuantumCircuit(n_qubits=4)
    circuit_nocache._cache_enabled = False

    start = time.time()
    for _ in range(100):
        inputs = torch.randn(4) * np.pi
        weights = torch.randn(1, 4, 2) * 0.1
        _ = circuit_nocache.forward(inputs, weights)
    normal_time = time.time() - start
    print(f"   100å›å®Ÿè¡Œ: {normal_time:.2f}ç§’")

    # é«˜é€ŸåŒ–é‡å­å›è·¯
    print("\n2. é«˜é€ŸåŒ–é‡å­å›è·¯ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰:")
    circuit_fast = FastQuantumCircuit(n_qubits=4)

    start = time.time()
    for i in range(100):
        # ä¸€éƒ¨ã¯åŒã˜å…¥åŠ›ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰
        if i % 3 == 0:
            inputs = torch.zeros(4)
        else:
            inputs = torch.randn(4) * np.pi
        weights = torch.randn(1, 4, 2) * 0.1
        _ = circuit_fast.forward(inputs, weights)
    fast_time = time.time() - start
    print(f"   100å›å®Ÿè¡Œ: {fast_time:.2f}ç§’")
    print(f"   ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆç‡: {len(circuit_fast.lookup_table)}/100")

    print(f"\nâš¡ é«˜é€ŸåŒ–å€ç‡: {normal_time/fast_time:.1f}å€")

    # ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ã®ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    print("\n3. ãƒ•ãƒ«ãƒ¢ãƒ‡ãƒ«ï¼ˆ1ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰:")
    model = FastQuantumNeuralNetwork(n_qubits=4)

    start = time.time()
    for _ in range(10):
        state = torch.randn(1, 252)
        _ = model(state)
    model_time = time.time() - start
    print(f"   10ã‚¹ãƒ†ãƒƒãƒ—: {model_time:.2f}ç§’")
    print(f"   æ¨å®š1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {model_time * 100:.0f}ç§’")


if __name__ == "__main__":
    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    benchmark_quantum_speedup()

    # å­¦ç¿’å®Ÿè¡Œ
    print("\né‡å­å›è·¯å­¦ç¿’ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ")
    print("1. ãƒ‡ãƒ¢ï¼ˆ100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰")
    print("2. æ¨™æº–ï¼ˆ1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰")
    print("3. ãƒ•ãƒ«ï¼ˆ10000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰")
    print("4. åæŸãƒ†ã‚¹ãƒˆï¼ˆ100000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ï¼‰")

    import sys

    if len(sys.argv) > 1:
        choice = sys.argv[1]
        if choice == "1":
            model, rewards = train_fast_quantum(100, n_qubits=4)
        elif choice == "2":
            model, rewards = train_fast_quantum(1000, n_qubits=4)
        elif choice == "3":
            model, rewards = train_fast_quantum(10000, n_qubits=4)
        elif choice == "4":
            print("\nğŸ§ª 100000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰åæŸãƒ†ã‚¹ãƒˆã‚’é–‹å§‹...")
            print("âš ï¸  æ³¨æ„: ã“ã®ãƒ†ã‚¹ãƒˆã¯æ•°æ™‚é–“ã‹ã‹ã‚‹å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™")
            model, rewards = train_convergence_test(100000, n_qubits=4)
        else:
            print(f"\nâŒ ç„¡åŠ¹ãªé¸æŠ: {choice}")
            print("æœ‰åŠ¹ãªé¸æŠ: 1, 2, 3, 4")
    else:
        print("\nãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 100ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã®ãƒ‡ãƒ¢ã‚’å®Ÿè¡Œ")
        model, rewards = train_fast_quantum(100, n_qubits=4)
