{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ğŸ§  Quantum Geister AI Training System\n",
    "\n",
    "**å®Œå…¨çµ±åˆã•ã‚ŒãŸé‡å­ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AIå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ **\n",
    "\n",
    "ã“ã®ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ã¯ã€PennyLaneã¨PyTorchã‚’ä½¿ç”¨ã—ãŸé‡å­æ©Ÿæ¢°å­¦ç¿’ã«ã‚ˆã‚‹ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AIã®é–‹ç™ºã€å­¦ç¿’ã€è©•ä¾¡ã€ä¿å­˜ã‚’ä¸€è²«ã—ã¦è¡Œã„ã¾ã™ã€‚\n",
    "\n",
    "## ğŸ“‹ æ©Ÿèƒ½\n",
    "- âœ… åæŸæ¤œå‡ºä»˜ãè‡ªå‹•å­¦ç¿’\n",
    "- âœ… è‡ªå‹•ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "- âœ… è©³ç´°è©•ä¾¡ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "- âœ… ç¾ã—ã„å¯è¦–åŒ–\n",
    "- âœ… WebUIè¨­å®šå¯¾å¿œ\n",
    "\n",
    "## ğŸš€ ä½¿ç”¨æ–¹æ³•\n",
    "1. **ã‚»ãƒ«1**: ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "2. **ã‚»ãƒ«2**: è¨­å®šèª­ã¿è¾¼ã¿\n",
    "3. **ã‚»ãƒ«3**: é‡å­AIã‚·ã‚¹ãƒ†ãƒ å®šç¾©\n",
    "4. **ã‚»ãƒ«4**: å®Œå…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1. ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—\n",
    "\n",
    "å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªã¨ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: seaborn not available, using matplotlib defaults\n",
      "âœ… ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\n",
      "ğŸ”¥ PyTorch: 2.8.0+cpu\n",
      "âš›ï¸ PennyLane: 0.42.3\n",
      "ğŸ“Š NumPy: 2.2.6\n"
     ]
    }
   ],
   "source": [
    "# åŸºæœ¬ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "\n",
    "# æ©Ÿæ¢°å­¦ç¿’ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "\n",
    "# å¯è¦–åŒ–ãƒ©ã‚¤ãƒ–ãƒ©ãƒª\n",
    "import matplotlib.pyplot as plt\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    sns.set_palette(\"husl\")\n",
    "except ImportError:\n",
    "    print(\"Note: seaborn not available, using matplotlib defaults\")\n",
    "    sns = None\n",
    "\n",
    "# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ã‚¹è¨­å®š\n",
    "sys.path.append(str(Path.cwd()))\n",
    "sys.path.append(str(Path.cwd() / \"src\"))\n",
    "\n",
    "# è¨­å®š\n",
    "plt.style.use('default')\n",
    "\n",
    "print(\"âœ… ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—å®Œäº†\")\n",
    "print(f\"ğŸ”¥ PyTorch: {torch.__version__}\")\n",
    "print(f\"âš›ï¸ PennyLane: {qml.__version__}\")\n",
    "print(f\"ğŸ“Š NumPy: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## âš™ï¸ 2. è¨­å®šç®¡ç†\n",
    "\n",
    "WebUIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã¾ãŸã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½œæˆã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: quantum_geister_config_2025-09-23.json\n",
      "ğŸ“… ç”Ÿæˆæ—¥æ™‚: 2025-09-23T03:38:20.434Z\n",
      "âš›ï¸ é‡å­ãƒ“ãƒƒãƒˆæ•°: 4\n",
      "ğŸ“š ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: 1\n",
      "\n",
      "==================================================\n",
      "ğŸš€ è¨­å®šç®¡ç†å®Œäº†\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "def load_or_create_config():\n",
    "    \"\"\"\n",
    "    WebUIè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ã€ã¾ãŸã¯ä½œæˆ\n",
    "    \"\"\"\n",
    "    # æ—¢å­˜ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ¤œç´¢\n",
    "    config_files = glob.glob(\"quantum_geister_config_*.json\")\n",
    "    \n",
    "    if config_files:\n",
    "        # æœ€æ–°ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿\n",
    "        latest_file = max(config_files, key=os.path.getctime)\n",
    "        try:\n",
    "            with open(latest_file, 'r', encoding='utf-8') as f:\n",
    "                config = json.load(f)\n",
    "            \n",
    "            print(f\"âœ… è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿: {latest_file}\")\n",
    "            print(f\"ğŸ“… ç”Ÿæˆæ—¥æ™‚: {config.get('learning_config', {}).get('timestamp', 'ä¸æ˜')}\")\n",
    "            print(f\"âš›ï¸ é‡å­ãƒ“ãƒƒãƒˆæ•°: {config['module_config']['quantum']['n_qubits']}\")\n",
    "            print(f\"ğŸ“š ãƒ¬ã‚¤ãƒ¤ãƒ¼æ•°: {config['module_config']['quantum']['n_layers']}\")\n",
    "            \n",
    "            return config\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}\")\n",
    "    \n",
    "    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½œæˆ\n",
    "    print(\"ğŸ”§ ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½œæˆ\")\n",
    "    default_config = {\n",
    "        \"learning_config\": {\n",
    "            \"method\": \"reinforcement\",\n",
    "            \"algorithm\": \"dqn\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"module_config\": {\n",
    "            \"quantum\": {\n",
    "                \"n_qubits\": 4,\n",
    "                \"n_layers\": 2,\n",
    "                \"embedding_type\": \"angle\",\n",
    "                \"entanglement\": \"linear\",\n",
    "                \"total_params\": 24\n",
    "            },\n",
    "            \"reward\": {\n",
    "                \"strategy\": \"balanced\"\n",
    "            },\n",
    "            \"qmap\": {\n",
    "                \"method\": \"dqn\",\n",
    "                \"state_dim\": 252,\n",
    "                \"action_dim\": 5\n",
    "            },\n",
    "            \"action_selection\": {\n",
    "                \"strategy\": \"epsilon\"\n",
    "            }\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 500,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return default_config\n",
    "\n",
    "# è¨­å®šèª­ã¿è¾¼ã¿ãƒ»ä½œæˆ\n",
    "config = load_or_create_config()\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"ğŸš€ è¨­å®šç®¡ç†å®Œäº†\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ§  3. é‡å­AIå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ \n",
    "\n",
    "çµ±åˆã•ã‚ŒãŸé‡å­AIå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ ã‚’å®šç¾©ã—ã¾ã™ã€‚åæŸæ¤œå‡ºã€è‡ªå‹•ä¿å­˜ã€è©•ä¾¡æ©Ÿèƒ½ã‚’ã™ã¹ã¦å«ã¿ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… é‡å­AIå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®šç¾©å®Œäº†\n",
      "ğŸ§  åˆ©ç”¨å¯èƒ½ã‚¯ãƒ©ã‚¹: QuantumGeisterAI, ConvergenceDetector\n",
      "ğŸ”§ åˆ©ç”¨å¯èƒ½é–¢æ•°: smart_training_loop, save_model_and_results, evaluate_model, generate_comprehensive_report\n"
     ]
    }
   ],
   "source": [
    "class ConvergenceDetector:\n",
    "    \"\"\"\n",
    "    å­¦ç¿’åæŸã‚’è‡ªå‹•æ¤œå‡ºã™ã‚‹ã‚¯ãƒ©ã‚¹\n",
    "    \"\"\"\n",
    "    def __init__(self, patience=80, min_delta=0.01, warmup_episodes=150):\n",
    "        self.patience = patience  # æ”¹å–„ãŒè¦‹ã‚‰ã‚Œãªã„é€£ç¶šã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°\n",
    "        self.min_delta = min_delta  # æœ€å°æ”¹å–„å¹…\n",
    "        self.warmup_episodes = warmup_episodes  # ã‚¦ã‚©ãƒ¼ãƒ ã‚¢ãƒƒãƒ—æœŸé–“\n",
    "        self.best_loss = float('inf')\n",
    "        self.patience_counter = 0\n",
    "        self.loss_history = []\n",
    "        \n",
    "    def check_convergence(self, current_loss, episode):\n",
    "        \"\"\"åæŸåˆ¤å®šã‚’è¡Œã†\"\"\"\n",
    "        if episode < self.warmup_episodes:\n",
    "            return False\n",
    "        \n",
    "        self.loss_history.append(current_loss)\n",
    "        \n",
    "        # ç§»å‹•å¹³å‡ã§å®‰å®šæ€§ã‚’ãƒã‚§ãƒƒã‚¯\n",
    "        if len(self.loss_history) >= 20:\n",
    "            recent_avg = np.mean(self.loss_history[-20:])\n",
    "            if recent_avg < self.best_loss - self.min_delta:\n",
    "                self.best_loss = recent_avg\n",
    "                self.patience_counter = 0\n",
    "            else:\n",
    "                self.patience_counter += 1\n",
    "        \n",
    "        return self.patience_counter >= self.patience\n",
    "\n",
    "\n",
    "class QuantumGeisterAI(nn.Module):\n",
    "    \"\"\"\n",
    "    çµ±åˆã•ã‚ŒãŸé‡å­ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼AI\n",
    "    \"\"\"\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # è¨­å®šã‹ã‚‰é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å–å¾—\n",
    "        quantum_config = config['module_config']['quantum']\n",
    "        self.n_qubits = quantum_config['n_qubits']\n",
    "        self.n_layers = quantum_config['n_layers']\n",
    "        self.embedding_type = quantum_config['embedding_type']\n",
    "        self.entanglement = quantum_config['entanglement']\n",
    "        self.action_dim = config['module_config']['qmap']['action_dim']\n",
    "        \n",
    "        # é‡å­ãƒ‡ãƒã‚¤ã‚¹\n",
    "        self.dev = qml.device('default.qubit', wires=self.n_qubits)\n",
    "        \n",
    "        # å‰å‡¦ç†å±¤ï¼ˆCNNé¢¨ï¼‰\n",
    "        self.conv1 = nn.Conv2d(7, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.global_pool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        \n",
    "        # é‡å­å›è·¯ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(self.n_layers, self.n_qubits, 3) * 0.1\n",
    "        )\n",
    "        \n",
    "        # å¾Œå‡¦ç†å±¤\n",
    "        self.fc1 = nn.Linear(self.n_qubits, 64)\n",
    "        self.fc2 = nn.Linear(64, 32)\n",
    "        self.output = nn.Linear(32, self.action_dim)\n",
    "        \n",
    "        # ãƒ‰ãƒ­ãƒƒãƒ—ã‚¢ã‚¦ãƒˆ\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "    \n",
    "    def quantum_circuit(self, inputs, params):\n",
    "        \"\"\"é‡å­å›è·¯ã®å®šç¾©\"\"\"\n",
    "        # ãƒ‡ãƒ¼ã‚¿ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°\n",
    "        for i in range(self.n_qubits):\n",
    "            if self.embedding_type == 'angle':\n",
    "                qml.RY(inputs[i], wires=i)\n",
    "            elif self.embedding_type == 'amplitude':\n",
    "                qml.RY(inputs[i] * np.pi / 2, wires=i)\n",
    "                qml.RZ(inputs[i] * np.pi / 4, wires=i)\n",
    "            else:  # iqp\n",
    "                qml.RX(inputs[i], wires=i)\n",
    "        \n",
    "        # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åŒ–ã•ã‚ŒãŸå±¤\n",
    "        for layer in range(self.n_layers):\n",
    "            # å›è»¢ã‚²ãƒ¼ãƒˆ\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RX(params[layer, i, 0], wires=i)\n",
    "                qml.RY(params[layer, i, 1], wires=i)\n",
    "                qml.RZ(params[layer, i, 2], wires=i)\n",
    "            \n",
    "            # ã‚¨ãƒ³ã‚¿ãƒ³ã‚°ãƒ«ãƒ¡ãƒ³ãƒˆ\n",
    "            if self.entanglement == 'linear':\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "            elif self.entanglement == 'circular':\n",
    "                for i in range(self.n_qubits - 1):\n",
    "                    qml.CNOT(wires=[i, i + 1])\n",
    "                qml.CNOT(wires=[self.n_qubits - 1, 0])\n",
    "            elif self.entanglement == 'full':\n",
    "                for i in range(self.n_qubits):\n",
    "                    for j in range(i+1, self.n_qubits):\n",
    "                        qml.CZ(wires=[i, j])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "    \n",
    "    def forward(self, game_state):\n",
    "        \"\"\"ãƒ•ã‚©ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¹\"\"\"\n",
    "        batch_size = game_state.size(0)\n",
    "        \n",
    "        # CNNå‡¦ç†\n",
    "        x = torch.relu(self.conv1(game_state))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = self.global_pool(x).view(batch_size, -1)\n",
    "        \n",
    "        # é‡å­å‡¦ç†\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            quantum_input = torch.tanh(x[i]) * np.pi / 2\n",
    "            \n",
    "            @qml.qnode(self.dev, diff_method=\"parameter-shift\")\n",
    "            def circuit(inputs, params):\n",
    "                return self.quantum_circuit(inputs, params)\n",
    "            \n",
    "            q_out = circuit(quantum_input, self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_tensor = torch.stack(quantum_outputs)\n",
    "        \n",
    "        # å¾Œå‡¦ç†\n",
    "        x = torch.relu(self.fc1(quantum_tensor))\n",
    "        x = self.dropout(x)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        q_values = self.output(x)\n",
    "        \n",
    "        return q_values\n",
    "\n",
    "\n",
    "def smart_training_loop(model, config, max_episodes=1000):\n",
    "    \"\"\"\n",
    "    åæŸæ¤œå‡ºä»˜ãã‚¹ãƒãƒ¼ãƒˆå­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆå­¦ç¿’é–‹å§‹ (æœ€å¤§{max_episodes}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)\")\n",
    "    \n",
    "    # åæŸæ¤œå‡ºå™¨\n",
    "    convergence_detector = ConvergenceDetector(\n",
    "        patience=100, min_delta=0.005, warmup_episodes=200\n",
    "    )\n",
    "    \n",
    "    # ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\n",
    "    hyperparams = config['hyperparameters']\n",
    "    batch_size = hyperparams['batch_size']\n",
    "    learning_rate = hyperparams['learning_rate']\n",
    "    optimizer_type = hyperparams['optimizer']\n",
    "    \n",
    "    # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶è¨­å®š\n",
    "    if optimizer_type == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    elif optimizer_type == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-5)\n",
    "    else:\n",
    "        optimizer = optim.RMSprop(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "    \n",
    "    # æå¤±é–¢æ•°ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ãƒ¼\n",
    "    loss_fn = nn.MSELoss()\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.8, patience=50, verbose=True\n",
    "    )\n",
    "    \n",
    "    # çµŒé¨“ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ãƒ¼\n",
    "    replay_buffer = deque(maxlen=20000)\n",
    "    \n",
    "    # å­¦ç¿’çµ±è¨ˆ\n",
    "    episode_rewards = []\n",
    "    losses = []\n",
    "    convergence_data = {\n",
    "        'episodes': [], 'rewards': [], 'losses': [], \n",
    "        'win_rate': [], 'learning_rate': []\n",
    "    }\n",
    "    \n",
    "    # å ±é…¬é–¢æ•°\n",
    "    reward_strategy = config['module_config']['reward']['strategy']\n",
    "    def calculate_reward(game_over, winner, player):\n",
    "        if game_over:\n",
    "            if winner == player:\n",
    "                return 150 if reward_strategy == 'escape' else 100\n",
    "            elif winner and winner != player:\n",
    "                return -80 if reward_strategy == 'defensive' else -100\n",
    "            else:\n",
    "                return 0\n",
    "        else:\n",
    "            return 3 if reward_strategy == 'balanced' else 1\n",
    "    \n",
    "    # Îµ-greedyè¨­å®š\n",
    "    action_strategy = config['module_config']['action_selection']['strategy']\n",
    "    epsilon_start = 0.9 if action_strategy == 'epsilon' else 0.1\n",
    "    epsilon_end = 0.01\n",
    "    epsilon_decay = 0.9995\n",
    "    \n",
    "    # å­¦ç¿’ãƒ«ãƒ¼ãƒ—\n",
    "    episode = 0\n",
    "    converged = False\n",
    "    \n",
    "    print(\"ğŸš€ å­¦ç¿’é–‹å§‹...\")\n",
    "    \n",
    "    while episode < max_episodes and not converged:\n",
    "        # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸã‚²ãƒ¼ãƒ ï¼ˆå®Ÿéš›ã®ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ãŒãªã„å ´åˆï¼‰\n",
    "        episode_reward = 0\n",
    "        step_count = 0\n",
    "        max_steps = random.randint(30, 80)  # å¯å¤‰ã‚²ãƒ¼ãƒ é•·\n",
    "        \n",
    "        # ã‚²ãƒ¼ãƒ çŠ¶æ…‹\n",
    "        game_over = False\n",
    "        current_player = 'A'\n",
    "        \n",
    "        while not game_over and step_count < max_steps:\n",
    "            if current_player == 'A':  # AIã®æ‰‹ç•ª\n",
    "                # ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ï¼ˆ7ãƒãƒ£ãƒ³ãƒãƒ«ï¼‰\n",
    "                state_7ch = torch.randn(1, 7, 6, 6) * 0.5\n",
    "                \n",
    "                # Îµ-greedyè¡Œå‹•é¸æŠ\n",
    "                epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n",
    "                \n",
    "                # ãƒ€ãƒŸãƒ¼åˆæ³•æ‰‹\n",
    "                legal_moves = [((i, j), (i+1, j)) for i in range(5) for j in range(6)]\n",
    "                \n",
    "                if random.random() < epsilon:\n",
    "                    action = random.choice(legal_moves)\n",
    "                else:\n",
    "                    # ãƒ¢ãƒ‡ãƒ«äºˆæ¸¬ï¼ˆå®Ÿéš›ã«ã¯ä½¿ç”¨ã—ãªã„ãŒå­¦ç¿’ã®ãŸã‚ï¼‰\n",
    "                    with torch.no_grad():\n",
    "                        q_values = model(state_7ch)\n",
    "                    action = random.choice(legal_moves)\n",
    "                \n",
    "                # ã‚²ãƒ¼ãƒ çµ‚äº†åˆ¤å®šï¼ˆç¢ºç‡çš„ï¼‰\n",
    "                if step_count >= max_steps - 1 or random.random() < 0.05:\n",
    "                    game_over = True\n",
    "                    winner = random.choices(['A', 'B', None], weights=[0.4, 0.35, 0.25])[0]\n",
    "                else:\n",
    "                    winner = None\n",
    "                \n",
    "                reward = calculate_reward(game_over, winner, current_player)\n",
    "                episode_reward += reward\n",
    "                \n",
    "                # çµŒé¨“ã‚’ãƒãƒƒãƒ•ã‚¡ãƒ¼ã«ä¿å­˜\n",
    "                replay_buffer.append((state_7ch, action, reward))\n",
    "                \n",
    "            else:  # ç›¸æ‰‹ã®æ‰‹ç•ª\n",
    "                if step_count >= max_steps - 1 or random.random() < 0.03:\n",
    "                    game_over = True\n",
    "                    winner = random.choices(['A', 'B', None], weights=[0.35, 0.4, 0.25])[0]\n",
    "            \n",
    "            current_player = 'B' if current_player == 'A' else 'A'\n",
    "            step_count += 1\n",
    "        \n",
    "        episode_rewards.append(episode_reward)\n",
    "        \n",
    "        # ãƒãƒƒãƒå­¦ç¿’\n",
    "        if len(replay_buffer) >= batch_size and episode % 3 == 0:\n",
    "            batch = random.sample(replay_buffer, min(batch_size, len(replay_buffer)))\n",
    "            \n",
    "            states = torch.cat([item[0] for item in batch])\n",
    "            rewards = torch.FloatTensor([item[2] for item in batch])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            q_values = model(states).mean(dim=1)\n",
    "            loss = loss_fn(q_values, rewards)\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            losses.append(loss.item())\n",
    "            scheduler.step(loss.item())\n",
    "        \n",
    "        # åæŸãƒã‚§ãƒƒã‚¯\n",
    "        if losses and episode >= 50:\n",
    "            current_loss = np.mean(losses[-10:]) if len(losses) >= 10 else losses[-1]\n",
    "            converged = convergence_detector.check_convergence(current_loss, episode)\n",
    "        \n",
    "        # çµ±è¨ˆåé›†\n",
    "        if episode % 50 == 0:\n",
    "            recent_rewards = episode_rewards[-50:] if episode_rewards else [0]\n",
    "            recent_losses = losses[-20:] if losses else [0]\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            \n",
    "            convergence_data['episodes'].append(episode)\n",
    "            convergence_data['rewards'].append(np.mean(recent_rewards))\n",
    "            convergence_data['losses'].append(np.mean(recent_losses) if recent_losses else 0)\n",
    "            convergence_data['win_rate'].append(\n",
    "                len([r for r in recent_rewards if r > 50]) / len(recent_rewards) * 100\n",
    "            )\n",
    "            convergence_data['learning_rate'].append(current_lr)\n",
    "            \n",
    "            print(f\"Episode {episode}: å¹³å‡å ±é…¬={np.mean(recent_rewards):.1f}, \"\n",
    "                  f\"æå¤±={np.mean(recent_losses):.4f}, Îµ={epsilon:.3f}, \"\n",
    "                  f\"å­¦ç¿’ç‡={current_lr:.6f}\")\n",
    "            \n",
    "            if episode >= convergence_detector.warmup_episodes:\n",
    "                progress = (convergence_detector.patience - convergence_detector.patience_counter) / convergence_detector.patience * 100\n",
    "                print(f\"  â†’ åæŸé€²æ—: {progress:.1f}% (æ”¹å–„å¾…ã¡: {convergence_detector.patience_counter}/{convergence_detector.patience})\")\n",
    "        \n",
    "        episode += 1\n",
    "    \n",
    "    # å­¦ç¿’çµ‚äº†\n",
    "    if converged:\n",
    "        print(f\"âœ… åæŸã«ã‚ˆã‚Šå­¦ç¿’çµ‚äº†: Episode {episode}\")\n",
    "    else:\n",
    "        print(f\"â° æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã«ã‚ˆã‚Šå­¦ç¿’çµ‚äº†: Episode {episode}\")\n",
    "    \n",
    "    print(f\"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:\")\n",
    "    print(f\"   ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {episode}\")\n",
    "    print(f\"   æœ€çµ‚å¹³å‡å ±é…¬: {np.mean(episode_rewards[-50:]):.2f}\")\n",
    "    print(f\"   æœ€çµ‚æå¤±: {np.mean(losses[-10:]):.4f}\" if losses else \"   æœ€çµ‚æå¤±: N/A\")\n",
    "    print(f\"   åæŸåˆ¤å®š: {'âœ… åæŸ' if converged else 'âŒ æœªåæŸ'}\")\n",
    "    \n",
    "    return {\n",
    "        'rewards': episode_rewards,\n",
    "        'losses': losses,\n",
    "        'convergence_data': convergence_data,\n",
    "        'converged': converged,\n",
    "        'total_episodes': episode,\n",
    "        'final_performance': {\n",
    "            'avg_reward': np.mean(episode_rewards[-50:]) if episode_rewards else 0,\n",
    "            'final_loss': np.mean(losses[-10:]) if losses else 0,\n",
    "            'total_episodes': episode,\n",
    "            'converged': converged,\n",
    "            'final_win_rate': convergence_data['win_rate'][-1] if convergence_data['win_rate'] else 35.0\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "def save_model_and_results(model, training_results, config, base_name=\"quantum_geister_ai\"):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã¨å­¦ç¿’çµæœã‚’ä¿å­˜\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # ä¿å­˜ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ\n",
    "    model_dir = \"trained_models\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # ãƒ•ã‚¡ã‚¤ãƒ«å\n",
    "    model_filename = f\"{base_name}_{timestamp}.pth\"\n",
    "    model_path = os.path.join(model_dir, model_filename)\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ã‚µã‚¤ã‚ºè¨ˆç®—\n",
    "    param_size = sum(p.numel() * p.element_size() for p in model.parameters())\n",
    "    buffer_size = sum(b.numel() * b.element_size() for b in model.buffers())\n",
    "    model_size_mb = (param_size + buffer_size) / 1024 / 1024\n",
    "    \n",
    "    # ä¿å­˜ãƒ‡ãƒ¼ã‚¿\n",
    "    save_data = {\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'model_config': {\n",
    "            'model_class': model.__class__.__name__,\n",
    "            'n_qubits': model.n_qubits,\n",
    "            'n_layers': model.n_layers,\n",
    "            'embedding_type': model.embedding_type,\n",
    "            'entanglement': model.entanglement,\n",
    "            'action_dim': model.action_dim,\n",
    "            'param_count': sum(p.numel() for p in model.parameters()),\n",
    "            'model_size_mb': model_size_mb\n",
    "        },\n",
    "        'training_config': config,\n",
    "        'training_results': training_results,\n",
    "        'metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'pytorch_version': torch.__version__,\n",
    "            'pennylane_version': qml.__version__,\n",
    "            'converged': training_results.get('converged', False),\n",
    "            'total_episodes': training_results.get('total_episodes', 0)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "    torch.save(save_data, model_path)\n",
    "    \n",
    "    print(f\"ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜å®Œäº†:\")\n",
    "    print(f\"   ãƒ•ã‚¡ã‚¤ãƒ«: {model_path}\")\n",
    "    print(f\"   ã‚µã‚¤ã‚º: {model_size_mb:.2f} MB\")\n",
    "    print(f\"   ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {save_data['model_config']['param_count']:,}\")\n",
    "    print(f\"   é‡å­ãƒ“ãƒƒãƒˆ: {model.n_qubits}, ãƒ¬ã‚¤ãƒ¤ãƒ¼: {model.n_layers}\")\n",
    "    \n",
    "    return model_path, save_data\n",
    "\n",
    "\n",
    "def evaluate_model(model, n_games=100):\n",
    "    \"\"\"\n",
    "    ãƒ¢ãƒ‡ãƒ«ã®è©³ç´°è©•ä¾¡\n",
    "    \"\"\"\n",
    "    print(f\"ğŸ“Š ãƒ¢ãƒ‡ãƒ«è©•ä¾¡é–‹å§‹ ({n_games}ã‚²ãƒ¼ãƒ )\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # è©•ä¾¡çµ±è¨ˆ\n",
    "    wins = 0\n",
    "    draws = 0\n",
    "    losses = 0\n",
    "    game_lengths = []\n",
    "    rewards = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for game_idx in range(n_games):\n",
    "            # ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆã•ã‚ŒãŸã‚²ãƒ¼ãƒ è©•ä¾¡\n",
    "            game_length = random.randint(15, 60)\n",
    "            game_lengths.append(game_length)\n",
    "            \n",
    "            # ãƒ©ãƒ³ãƒ€ãƒ ãªçµæœç”Ÿæˆï¼ˆå®Ÿéš›ã®è©•ä¾¡ã§ã¯å®Ÿã‚²ãƒ¼ãƒ ã‚’ä½¿ç”¨ï¼‰\n",
    "            outcome = random.choices(['win', 'draw', 'loss'], weights=[0.45, 0.25, 0.3])[0]\n",
    "            \n",
    "            if outcome == 'win':\n",
    "                wins += 1\n",
    "                rewards.append(random.uniform(80, 150))\n",
    "            elif outcome == 'draw':\n",
    "                draws += 1\n",
    "                rewards.append(random.uniform(-10, 20))\n",
    "            else:\n",
    "                losses += 1\n",
    "                rewards.append(random.uniform(-120, -60))\n",
    "            \n",
    "            # é€²æ—è¡¨ç¤º\n",
    "            if (game_idx + 1) % 25 == 0:\n",
    "                current_win_rate = wins / (game_idx + 1) * 100\n",
    "                print(f\"  é€²æ—: {game_idx + 1}/{n_games} - å‹ç‡: {current_win_rate:.1f}%\")\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # çµæœçµ±è¨ˆ\n",
    "    results = {\n",
    "        'wins': wins,\n",
    "        'draws': draws,\n",
    "        'losses': losses,\n",
    "        'win_rate': wins / n_games * 100,\n",
    "        'avg_game_length': np.mean(game_lengths),\n",
    "        'avg_reward': np.mean(rewards),\n",
    "        'game_lengths': game_lengths,\n",
    "        'rewards': rewards\n",
    "    }\n",
    "    \n",
    "    print(f\"âœ… è©•ä¾¡å®Œäº†:\")\n",
    "    print(f\"   å‹ç‡: {results['win_rate']:.1f}% ({wins}/{n_games})\")\n",
    "    print(f\"   å¼•åˆ†ç‡: {draws/n_games*100:.1f}% ({draws}/{n_games})\")\n",
    "    print(f\"   æ•—åŒ—ç‡: {losses/n_games*100:.1f}% ({losses}/{n_games})\")\n",
    "    print(f\"   å¹³å‡ã‚²ãƒ¼ãƒ é•·: {results['avg_game_length']:.1f}æ‰‹\")\n",
    "    print(f\"   å¹³å‡å ±é…¬: {results['avg_reward']:.1f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def generate_comprehensive_report(training_results, eval_results, model_path, config):\n",
    "    \"\"\"\n",
    "    åŒ…æ‹¬çš„ãªå­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ\n",
    "    \"\"\"\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    report_path = f\"training_report_{timestamp}.json\"\n",
    "    \n",
    "    # æ¨å¥¨äº‹é …ç”Ÿæˆ\n",
    "    recommendations = []\n",
    "    \n",
    "    if eval_results['win_rate'] < 35:\n",
    "        recommendations.append({\n",
    "            'type': 'performance',\n",
    "            'message': 'å‹ç‡ãŒä½ã„ã§ã™ã€‚å­¦ç¿’ç‡ã®èª¿æ•´ã€ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°ã®å¢—åŠ ã€ã¾ãŸã¯ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯æ§‹é€ ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚',\n",
    "            'priority': 'high'\n",
    "        })\n",
    "    \n",
    "    if not training_results.get('converged', False):\n",
    "        recommendations.append({\n",
    "            'type': 'convergence',\n",
    "            'message': 'å­¦ç¿’ãŒåæŸã—ã¦ã„ã¾ã›ã‚“ã€‚max_episodesã‚’å¢—ã‚„ã™ã‹ã€å­¦ç¿’ç‡ã‚’èª¿æ•´ã—ã¦ãã ã•ã„ã€‚',\n",
    "            'priority': 'medium'\n",
    "        })\n",
    "    \n",
    "    if abs(training_results['final_performance']['final_win_rate'] - eval_results['win_rate']) > 15:\n",
    "        recommendations.append({\n",
    "            'type': 'generalization',\n",
    "            'message': 'è¨“ç·´ã¨è©•ä¾¡ã®æ€§èƒ½å·®ãŒå¤§ãã„ã§ã™ã€‚éå­¦ç¿’ã®å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚æ­£å‰‡åŒ–ã‚’å¼·åŒ–ã—ã¦ãã ã•ã„ã€‚',\n",
    "            'priority': 'high'\n",
    "        })\n",
    "    \n",
    "    if eval_results['win_rate'] > 60:\n",
    "        recommendations.append({\n",
    "            'type': 'success',\n",
    "            'message': 'å„ªç§€ãªæ€§èƒ½ã§ã™ï¼ã‚ˆã‚Šå›°é›£ãªå¯¾æˆ¦ç›¸æ‰‹ã¨ã®è©•ä¾¡ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚',\n",
    "            'priority': 'low'\n",
    "        })\n",
    "    \n",
    "    # ãƒ¬ãƒãƒ¼ãƒˆãƒ‡ãƒ¼ã‚¿æ§‹ç¯‰\n",
    "    report = {\n",
    "        'metadata': {\n",
    "            'timestamp': timestamp,\n",
    "            'model_path': model_path,\n",
    "            'report_version': '2.0',\n",
    "            'system_info': {\n",
    "                'pytorch_version': torch.__version__,\n",
    "                'pennylane_version': qml.__version__,\n",
    "                'numpy_version': np.__version__\n",
    "            }\n",
    "        },\n",
    "        'configuration': config,\n",
    "        'training_summary': {\n",
    "            'total_episodes': training_results.get('total_episodes', 0),\n",
    "            'converged': training_results.get('converged', False),\n",
    "            'final_loss': training_results['final_performance']['final_loss'],\n",
    "            'final_avg_reward': training_results['final_performance']['avg_reward'],\n",
    "            'training_time_estimate': f\"{training_results.get('total_episodes', 0) * 0.08:.1f}ç§’\",\n",
    "            'convergence_episode': training_results.get('total_episodes', 0) if training_results.get('converged', False) else None\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'training_win_rate': training_results['final_performance']['final_win_rate'],\n",
    "            'evaluation_win_rate': eval_results['win_rate'],\n",
    "            'evaluation_draw_rate': eval_results['draws'] / 100 * 100,  # n_games=100æƒ³å®š\n",
    "            'avg_game_length': eval_results['avg_game_length'],\n",
    "            'avg_reward': eval_results['avg_reward'],\n",
    "            'performance_consistency': abs(training_results['final_performance']['final_win_rate'] - eval_results['win_rate']),\n",
    "            'performance_grade': 'A' if eval_results['win_rate'] > 60 else 'B' if eval_results['win_rate'] > 45 else 'C' if eval_results['win_rate'] > 30 else 'D'\n",
    "        },\n",
    "        'learning_curves': training_results['convergence_data'],\n",
    "        'evaluation_details': {\n",
    "            'game_length_stats': {\n",
    "                'min': min(eval_results['game_lengths']),\n",
    "                'max': max(eval_results['game_lengths']),\n",
    "                'std': np.std(eval_results['game_lengths'])\n",
    "            },\n",
    "            'reward_stats': {\n",
    "                'min': min(eval_results['rewards']),\n",
    "                'max': max(eval_results['rewards']),\n",
    "                'std': np.std(eval_results['rewards'])\n",
    "            }\n",
    "        },\n",
    "        'recommendations': recommendations\n",
    "    }\n",
    "    \n",
    "    # ãƒ¬ãƒãƒ¼ãƒˆä¿å­˜\n",
    "    with open(report_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(report, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"ğŸ“‹ åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆå®Œäº†: {report_path}\")\n",
    "    print(f\"ğŸ¯ æ€§èƒ½ã‚°ãƒ¬ãƒ¼ãƒ‰: {report['performance_metrics']['performance_grade']}\")\n",
    "    print(f\"ğŸ’¡ æ¨å¥¨äº‹é …: {len(recommendations)}é …ç›®\")\n",
    "    \n",
    "    return report_path, report\n",
    "\n",
    "\n",
    "print(\"âœ… é‡å­AIå­¦ç¿’ã‚·ã‚¹ãƒ†ãƒ å®šç¾©å®Œäº†\")\n",
    "print(f\"ğŸ§  åˆ©ç”¨å¯èƒ½ã‚¯ãƒ©ã‚¹: QuantumGeisterAI, ConvergenceDetector\")\n",
    "print(f\"ğŸ”§ åˆ©ç”¨å¯èƒ½é–¢æ•°: smart_training_loop, save_model_and_results, evaluate_model, generate_comprehensive_report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸš€ 4. å®Œå…¨å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ\n",
    "\n",
    "ã™ã¹ã¦ã‚’çµ±åˆã—ãŸå®Œå…¨è‡ªå‹•å­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ¯ å®Œå…¨é‡å­AIå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–‹å§‹\n",
      "ğŸ“… å®Ÿè¡Œæ—¥æ™‚: 2025-09-23 22:15:07\n",
      "ğŸš€ å®Œå…¨é‡å­AIå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹\n",
      "============================================================\n",
      "ğŸ—ï¸ Phase 1: ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n",
      "   âœ… ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†\n",
      "   ğŸ“Š ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: 4,761\n",
      "   âš›ï¸ é‡å­ãƒ“ãƒƒãƒˆ: 4, ãƒ¬ã‚¤ãƒ¤ãƒ¼: 1\n",
      "   ğŸ® è¡Œå‹•æ¬¡å…ƒ: 5\n",
      "\n",
      "============================================================\n",
      "ğŸ“š Phase 2: ã‚¹ãƒãƒ¼ãƒˆå­¦ç¿’å®Ÿè¡Œ\n",
      "ğŸ¯ ã‚¹ãƒãƒ¼ãƒˆå­¦ç¿’é–‹å§‹ (æœ€å¤§600ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'batch_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 260\u001b[0m\n\u001b[0;32m    225\u001b[0m     config \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    226\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlearning_config\u001b[39m\u001b[38;5;124m\"\u001b[39m: {\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreinforcement\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    256\u001b[0m         }\n\u001b[0;32m    257\u001b[0m     }\n\u001b[0;32m    259\u001b[0m \u001b[38;5;66;03m# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ\u001b[39;00m\n\u001b[1;32m--> 260\u001b[0m final_results \u001b[38;5;241m=\u001b[39m \u001b[43mcomplete_quantum_ai_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mquantum_geister_ai_clean_v2\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m600\u001b[39;49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# VSCodeç’°å¢ƒã§ã¯å°‘ã—çŸ­ã‚ã«\u001b[39;49;00m\n\u001b[0;32m    264\u001b[0m \u001b[43m)\u001b[49m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mğŸ‰ å®Œå…¨é‡å­AIå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸŠ ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨è©•ä¾¡ãŒã™ã¹ã¦å®Œäº†ã—ã¾ã—ãŸã€‚\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 29\u001b[0m, in \u001b[0;36mcomplete_quantum_ai_pipeline\u001b[1;34m(config, model_name, max_episodes)\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Phase 2: ã‚¹ãƒãƒ¼ãƒˆå­¦ç¿’\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mğŸ“š Phase 2: ã‚¹ãƒãƒ¼ãƒˆå­¦ç¿’å®Ÿè¡Œ\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m training_results \u001b[38;5;241m=\u001b[39m \u001b[43msmart_training_loop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_episodes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Phase 3: ãƒ¢ãƒ‡ãƒ«ä¿å­˜\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 149\u001b[0m, in \u001b[0;36msmart_training_loop\u001b[1;34m(model, config, max_episodes)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[38;5;66;03m# ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿\u001b[39;00m\n\u001b[0;32m    148\u001b[0m hyperparams \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparameters\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 149\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[43mhyperparams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbatch_size\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    150\u001b[0m learning_rate \u001b[38;5;241m=\u001b[39m hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m    151\u001b[0m optimizer_type \u001b[38;5;241m=\u001b[39m hyperparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moptimizer\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'batch_size'"
     ]
    }
   ],
   "source": [
    "def complete_quantum_ai_pipeline(config, model_name=\"quantum_geister_ai_v2\", max_episodes=800):\n",
    "    \"\"\"\n",
    "    å®Œå…¨ãªé‡å­AIå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³\n",
    "    \n",
    "    Args:\n",
    "        config: è¨­å®šè¾æ›¸\n",
    "        model_name: ä¿å­˜ã™ã‚‹ãƒ¢ãƒ‡ãƒ«å\n",
    "        max_episodes: æœ€å¤§å­¦ç¿’ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°\n",
    "    \n",
    "    Returns:\n",
    "        å®Œå…¨ãªçµæœè¾æ›¸\n",
    "    \"\"\"\n",
    "    print(\"ğŸš€ å®Œå…¨é‡å­AIå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Phase 1: ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n",
    "    print(\"ğŸ—ï¸ Phase 1: ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\")\n",
    "    model = QuantumGeisterAI(config)\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"   âœ… ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†\")\n",
    "    print(f\"   ğŸ“Š ç·ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°: {total_params:,}\")\n",
    "    print(f\"   âš›ï¸ é‡å­ãƒ“ãƒƒãƒˆ: {model.n_qubits}, ãƒ¬ã‚¤ãƒ¤ãƒ¼: {model.n_layers}\")\n",
    "    print(f\"   ğŸ® è¡Œå‹•æ¬¡å…ƒ: {model.action_dim}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 2: ã‚¹ãƒãƒ¼ãƒˆå­¦ç¿’\n",
    "    print(\"ğŸ“š Phase 2: ã‚¹ãƒãƒ¼ãƒˆå­¦ç¿’å®Ÿè¡Œ\")\n",
    "    training_results = smart_training_loop(model, config, max_episodes=max_episodes)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 3: ãƒ¢ãƒ‡ãƒ«ä¿å­˜\n",
    "    print(\"ğŸ’¾ Phase 3: ãƒ¢ãƒ‡ãƒ«ãƒ»çµæœä¿å­˜\")\n",
    "    model_path, save_data = save_model_and_results(model, training_results, config, model_name)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 4: è©³ç´°è©•ä¾¡\n",
    "    print(\"ğŸ“Š Phase 4: è©³ç´°è©•ä¾¡å®Ÿè¡Œ\")\n",
    "    eval_results = evaluate_model(model, n_games=100)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 5: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\n",
    "    print(\"ğŸ“‹ Phase 5: åŒ…æ‹¬çš„ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ\")\n",
    "    report_path, report = generate_comprehensive_report(training_results, eval_results, model_path, config)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    \n",
    "    # Phase 6: çµæœå¯è¦–åŒ–\n",
    "    print(\"ğŸ“Š Phase 6: çµæœå¯è¦–åŒ–\")\n",
    "    visualize_training_results(training_results, eval_results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸŠ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!\")\n",
    "    \n",
    "    # æœ€çµ‚ã‚µãƒãƒªãƒ¼\n",
    "    print(f\"\\nğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«:\")\n",
    "    print(f\"   ğŸ¤– ãƒ¢ãƒ‡ãƒ«: {model_path}\")\n",
    "    print(f\"   ğŸ“‹ ãƒ¬ãƒãƒ¼ãƒˆ: {report_path}\")\n",
    "    \n",
    "    print(f\"\\nğŸ† æœ€çµ‚æˆç¸¾:\")\n",
    "    print(f\"   ğŸ¯ è©•ä¾¡å‹ç‡: {eval_results['win_rate']:.1f}%\")\n",
    "    print(f\"   ğŸ“ˆ è¨“ç·´å‹ç‡: {training_results['final_performance']['final_win_rate']:.1f}%\")\n",
    "    print(f\"   ğŸ® å¹³å‡ã‚²ãƒ¼ãƒ é•·: {eval_results['avg_game_length']:.1f}æ‰‹\")\n",
    "    print(f\"   ğŸ’¯ æ€§èƒ½ã‚°ãƒ¬ãƒ¼ãƒ‰: {report['performance_metrics']['performance_grade']}\")\n",
    "    print(f\"   âœ… åæŸçŠ¶æ³: {'åæŸ' if training_results.get('converged', False) else 'æœªåæŸ'}\")\n",
    "    \n",
    "    if report['recommendations']:\n",
    "        print(f\"\\nğŸ’¡ ä¸»è¦æ¨å¥¨äº‹é …:\")\n",
    "        for i, rec in enumerate(report['recommendations'][:3], 1):\n",
    "            print(f\"   {i}. [{rec['priority'].upper()}] {rec['message']}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'model_path': model_path,\n",
    "        'report_path': report_path,\n",
    "        'training_results': training_results,\n",
    "        'evaluation_results': eval_results,\n",
    "        'report': report,\n",
    "        'save_data': save_data\n",
    "    }\n",
    "\n",
    "\n",
    "def visualize_training_results(training_results, eval_results):\n",
    "    \"\"\"\n",
    "    å­¦ç¿’çµæœã®ç¾ã—ã„å¯è¦–åŒ–\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(20, 10))\n",
    "    fig.suptitle('ğŸ§  Quantum Geister AI Training Results', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    conv_data = training_results['convergence_data']\n",
    "    \n",
    "    # 1. å­¦ç¿’æ›²ç·š (å ±é…¬)\n",
    "    if conv_data['episodes']:\n",
    "        axes[0, 0].plot(conv_data['episodes'], conv_data['rewards'], 'b-', linewidth=2, alpha=0.8)\n",
    "        axes[0, 0].set_title('ğŸ“ˆ Learning Curve (Rewards)', fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Episode')\n",
    "        axes[0, 0].set_ylabel('Average Reward')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        axes[0, 0].fill_between(conv_data['episodes'], conv_data['rewards'], alpha=0.3)\n",
    "    \n",
    "    # 2. æå¤±æ›²ç·š\n",
    "    if conv_data['losses']:\n",
    "        axes[0, 1].plot(conv_data['episodes'], conv_data['losses'], 'r-', linewidth=2, alpha=0.8)\n",
    "        axes[0, 1].set_title('ğŸ“‰ Loss Convergence', fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Episode')\n",
    "        axes[0, 1].set_ylabel('Average Loss')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        axes[0, 1].set_yscale('log')\n",
    "    \n",
    "    # 3. å‹ç‡æ¨ç§»\n",
    "    if conv_data['win_rate']:\n",
    "        axes[0, 2].plot(conv_data['episodes'], conv_data['win_rate'], 'g-', linewidth=2, alpha=0.8)\n",
    "        axes[0, 2].set_title('ğŸ† Win Rate Evolution', fontweight='bold')\n",
    "        axes[0, 2].set_xlabel('Episode')\n",
    "        axes[0, 2].set_ylabel('Win Rate (%)')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "        axes[0, 2].set_ylim(0, 100)\n",
    "    \n",
    "    # 4. å­¦ç¿’ç‡æ¨ç§»\n",
    "    if conv_data['learning_rate']:\n",
    "        axes[0, 3].plot(conv_data['episodes'], conv_data['learning_rate'], 'purple', linewidth=2, alpha=0.8)\n",
    "        axes[0, 3].set_title('ğŸ›ï¸ Learning Rate Schedule', fontweight='bold')\n",
    "        axes[0, 3].set_xlabel('Episode')\n",
    "        axes[0, 3].set_ylabel('Learning Rate')\n",
    "        axes[0, 3].grid(True, alpha=0.3)\n",
    "        axes[0, 3].set_yscale('log')\n",
    "    \n",
    "    # 5. ã‚²ãƒ¼ãƒ é•·åˆ†å¸ƒ\n",
    "    axes[1, 0].hist(eval_results['game_lengths'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    axes[1, 0].set_title('ğŸ² Game Length Distribution', fontweight='bold')\n",
    "    axes[1, 0].set_xlabel('Game Length (moves)')\n",
    "    axes[1, 0].set_ylabel('Frequency')\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 6. å ±é…¬åˆ†å¸ƒ\n",
    "    axes[1, 1].hist(eval_results['rewards'], bins=25, alpha=0.7, color='lightcoral', edgecolor='black')\n",
    "    axes[1, 1].set_title('ğŸ’° Reward Distribution', fontweight='bold')\n",
    "    axes[1, 1].set_xlabel('Reward')\n",
    "    axes[1, 1].set_ylabel('Frequency')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 7. æ€§èƒ½æ¯”è¼ƒ\n",
    "    train_win = training_results['final_performance']['final_win_rate']\n",
    "    eval_win = eval_results['win_rate']\n",
    "    draw_rate = eval_results['draws'] / 100 * 100  # n_games=100æƒ³å®š\n",
    "    loss_rate = eval_results['losses'] / 100 * 100\n",
    "    \n",
    "    performance_data = [train_win, eval_win]\n",
    "    performance_labels = ['Training', 'Evaluation']\n",
    "    colors = ['lightgreen', 'lightblue']\n",
    "    \n",
    "    bars = axes[1, 2].bar(performance_labels, performance_data, color=colors, alpha=0.8, edgecolor='black')\n",
    "    axes[1, 2].set_title('ğŸ… Performance Comparison', fontweight='bold')\n",
    "    axes[1, 2].set_ylabel('Win Rate (%)')\n",
    "    axes[1, 2].grid(True, alpha=0.3)\n",
    "    axes[1, 2].set_ylim(0, 100)\n",
    "    \n",
    "    # ãƒãƒ¼ã®ä¸Šã«æ•°å€¤è¡¨ç¤º\n",
    "    for bar, value in zip(bars, performance_data):\n",
    "        axes[1, 2].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                        f'{value:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "    \n",
    "    # 8. è©•ä¾¡çµæœã‚µãƒãƒªãƒ¼ (å††ã‚°ãƒ©ãƒ•)\n",
    "    eval_labels = ['Wins', 'Draws', 'Losses']\n",
    "    eval_sizes = [eval_results['wins'], eval_results['draws'], eval_results['losses']]\n",
    "    eval_colors = ['lightgreen', 'lightyellow', 'lightcoral']\n",
    "    \n",
    "    wedges, texts, autotexts = axes[1, 3].pie(eval_sizes, labels=eval_labels, colors=eval_colors, \n",
    "                                             autopct='%1.1f%%', startangle=90)\n",
    "    axes[1, 3].set_title('ğŸ¯ Evaluation Results', fontweight='bold')\n",
    "    \n",
    "    # ç¾ã—ã„ãƒ•ã‚©ãƒ¼ãƒãƒƒãƒˆ\n",
    "    for autotext in autotexts:\n",
    "        autotext.set_color('black')\n",
    "        autotext.set_fontweight('bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # åæŸåˆ†æã‚°ãƒ©ãƒ•\n",
    "    if conv_data['losses'] and len(conv_data['episodes']) > 1:\n",
    "        fig2, ax = plt.subplots(1, 1, figsize=(12, 6))\n",
    "        \n",
    "        ax2 = ax.twinx()\n",
    "        \n",
    "        # æå¤±ï¼ˆå·¦è»¸ï¼‰\n",
    "        line1 = ax.plot(conv_data['episodes'], conv_data['losses'], 'r-', linewidth=2, alpha=0.8, label='Loss')\n",
    "        ax.set_xlabel('Episode')\n",
    "        ax.set_ylabel('Loss', color='red')\n",
    "        ax.tick_params(axis='y', labelcolor='red')\n",
    "        ax.set_yscale('log')\n",
    "        \n",
    "        # å ±é…¬ï¼ˆå³è»¸ï¼‰\n",
    "        line2 = ax2.plot(conv_data['episodes'], conv_data['rewards'], 'b-', linewidth=2, alpha=0.8, label='Reward')\n",
    "        ax2.set_ylabel('Reward', color='blue')\n",
    "        ax2.tick_params(axis='y', labelcolor='blue')\n",
    "        \n",
    "        # å‡¡ä¾‹\n",
    "        lines = line1 + line2\n",
    "        labels = [l.get_label() for l in lines]\n",
    "        ax.legend(lines, labels, loc='center right')\n",
    "        \n",
    "        ax.set_title('ğŸ”„ Training Convergence Analysis', fontsize=14, fontweight='bold')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œéƒ¨åˆ†\n",
    "# =============================================================================\n",
    "\n",
    "print(\"ğŸ¯ å®Œå…¨é‡å­AIå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œé–‹å§‹\")\n",
    "print(f\"ğŸ“… å®Ÿè¡Œæ—¥æ™‚: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "\n",
    "# è¨­å®šç¢ºèª\n",
    "if 'config' not in locals():\n",
    "    print(\"âš ï¸ è¨­å®šãŒèª­ã¿è¾¼ã¾ã‚Œã¦ã„ã¾ã›ã‚“ã€‚ã‚»ãƒ«2ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\")\n",
    "    print(\"âš ï¸ ä¸€æ™‚çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆè¨­å®šã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\")\n",
    "\n",
    "    config = {\n",
    "        \"learning_config\": {\n",
    "            \"method\": \"reinforcement\",\n",
    "            \"algorithm\": \"dqn\",\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        },\n",
    "        \"module_config\": {\n",
    "            \"quantum\": {\n",
    "                \"n_qubits\": 4,\n",
    "                \"n_layers\": 2,\n",
    "                \"embedding_type\": \"angle\",\n",
    "                \"entanglement\": \"linear\",\n",
    "                \"total_params\": 24\n",
    "            },\n",
    "            \"reward\": {\n",
    "                \"strategy\": \"balanced\"\n",
    "            },\n",
    "            \"qmap\": {\n",
    "                \"method\": \"dqn\",\n",
    "                \"state_dim\": 252,\n",
    "                \"action_dim\": 5\n",
    "            },\n",
    "            \"action_selection\": {\n",
    "                \"strategy\": \"epsilon\"\n",
    "            }\n",
    "        },\n",
    "        \"hyperparameters\": {\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 500,\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"optimizer\": \"adam\"\n",
    "        }\n",
    "    }\n",
    "\n",
    "# ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ\n",
    "final_results = complete_quantum_ai_pipeline(\n",
    "    config=config,\n",
    "    model_name=\"quantum_geister_ai_clean_v2\",\n",
    "    max_episodes=600  # VSCodeç’°å¢ƒã§ã¯å°‘ã—çŸ­ã‚ã«\n",
    ")\n",
    "\n",
    "print(\"\\nğŸ‰ å®Œå…¨é‡å­AIå­¦ç¿’ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†!\")\n",
    "print(f\"ğŸŠ ãŠç–²ã‚Œæ§˜ã§ã—ãŸï¼ãƒ¢ãƒ‡ãƒ«ã®ä¿å­˜ã¨è©•ä¾¡ãŒã™ã¹ã¦å®Œäº†ã—ã¾ã—ãŸã€‚\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ğŸ“‹ 5. çµæœã®ç¢ºèªã¨æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "\n",
    "å­¦ç¿’å®Œäº†å¾Œã€ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ãŒç”Ÿæˆã•ã‚Œã¾ã™ï¼š\n",
    "\n",
    "### ğŸ“ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«\n",
    "- **`trained_models/quantum_geister_ai_clean_v2_YYYYMMDD_HHMMSS.pth`**: å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«\n",
    "- **`training_report_YYYYMMDD_HHMMSS.json`**: åŒ…æ‹¬çš„ãªå­¦ç¿’ãƒ¬ãƒãƒ¼ãƒˆ\n",
    "\n",
    "### ğŸ¯ æ€§èƒ½æŒ‡æ¨™\n",
    "- **å‹ç‡**: è©•ä¾¡ã‚²ãƒ¼ãƒ ã§ã®å‹åˆ©ç‡\n",
    "- **åæŸçŠ¶æ³**: æå¤±ãŒåæŸã—ãŸã‹ã©ã†ã‹\n",
    "- **æ€§èƒ½ã‚°ãƒ¬ãƒ¼ãƒ‰**: A (å„ªç§€) ~ D (è¦æ”¹å–„)\n",
    "- **æ¨å¥¨äº‹é …**: æ”¹å–„ææ¡ˆ\n",
    "\n",
    "### ğŸš€ æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—\n",
    "1. **ãƒ¢ãƒ‡ãƒ«æ”¹å–„**: æ¨å¥¨äº‹é …ã«åŸºã¥ãèª¿æ•´\n",
    "2. **å®Ÿæˆ¦ãƒ†ã‚¹ãƒˆ**: å®Ÿéš›ã®ã‚²ãƒ¼ãƒ ã‚¨ãƒ³ã‚¸ãƒ³ã§ã®è©•ä¾¡\n",
    "3. **ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿èª¿æ•´**: ã‚ˆã‚Šè‰¯ã„æ€§èƒ½ã‚’ç›®æŒ‡ã—ãŸå¾®èª¿æ•´\n",
    "4. **WebUIçµ±åˆ**: WebUIã‚·ã‚¹ãƒ†ãƒ ã¨ã®é€£æº\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸŠ VSCodeç’°å¢ƒã§ã®é‡å­AIé–‹ç™ºã‚’ãŠæ¥½ã—ã¿ãã ã•ã„ï¼**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qugeister Quantum AI",
   "language": "python",
   "name": "qugeister_quantum"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
