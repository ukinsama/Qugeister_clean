{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æœ¬æ ¼CQCNNé‡å­å¼·åŒ–å­¦ç¿’å®Ÿé¨“\n",
    "## JSON Configå®Œå…¨å¯¾å¿œç‰ˆ - åæŸã¾ã§ç¶™ç¶šå­¦ç¿’\n",
    "\n",
    "- **è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«**: quantum_cqcnn_config_2025-09-24.json\n",
    "- **é‡å­æ§‹æˆ**: 4Q1L (4é‡å­ãƒ“ãƒƒãƒˆ, 1ãƒ¬ã‚¤ãƒ¤ãƒ¼)\n",
    "- **åæŸé–¾å€¤**: 0.95 (Balance)\n",
    "- **æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰**: 50,000\n",
    "- **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£**: å®Œå…¨CQCNN (Frontend CNN â†’ Quantum â†’ Backend CNN)\n",
    "\n",
    "### å®Ÿé¨“ç›®æ¨™\n",
    "1. 0.95åæŸé–¾å€¤ã§ã®å­¦ç¿’åæŸç¢ºèª\n",
    "2. Ultra-strictå®Ÿé¨“(0.995)ã¨ã®æ¯”è¼ƒåˆ†æ\n",
    "3. åˆæœŸçŠ¶æ…‹ä¾å­˜æ€§ã®å½±éŸ¿è©•ä¾¡\n",
    "4. é‡å­åŠ¹æœã®å®šé‡çš„æ¸¬å®š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== é‡å­å¼·åŒ–å­¦ç¿’å®Ÿé¨“ç’°å¢ƒ ===\n",
      "PyTorch version: 2.8.0+cpu\n",
      "PennyLane version: 0.42.3\n",
      "NumPy version: 2.3.3\n",
      "Random seed: 42\n",
      "Device: CPU\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# === ç’°å¢ƒè¨­å®šã¨ãƒ©ã‚¤ãƒ–ãƒ©ãƒªå°å…¥ ===\n",
    "import sys\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pennylane as qml\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple, Optional, Any\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# ä¹±æ•°ã‚·ãƒ¼ãƒ‰è¨­å®šï¼ˆå†ç¾æ€§ç¢ºä¿ï¼‰\n",
    "EXPERIMENT_SEED = 42\n",
    "random.seed(EXPERIMENT_SEED)\n",
    "np.random.seed(EXPERIMENT_SEED)\n",
    "torch.manual_seed(EXPERIMENT_SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "print(\"=== é‡å­å¼·åŒ–å­¦ç¿’å®Ÿé¨“ç’°å¢ƒ ===\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PennyLane version: {qml.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Random seed: {EXPERIMENT_SEED}\")\n",
    "print(f\"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'}\")\n",
    "print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== å®Ÿé¨“è¨­å®šã‚µãƒãƒªãƒ¼ ===\n",
      "Algorithm: quantum_stable_spatial_36d\n",
      "Quantum: 4Q2L\n",
      "Embedding: angle\n",
      "Entanglement: full\n",
      "State Dimension: 252\n",
      "Action Space: 36D\n",
      "Batch Size: 96\n",
      "Learning Rate: 0.0006\n",
      "Epochs: 50000\n",
      "Epsilon: 0.5 â†’ 0.08 (decay: 0.9998)\n",
      "Replay Buffer: 6000\n",
      "Target Update: every 150 episodes\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "# === JSONè¨­å®šèª­ã¿è¾¼ã¿ ===\n",
    "config_path = \"quantum_recovery_stable_config_2025-09-25.json\"\n",
    "\n",
    "with open(config_path, 'r', encoding='utf-8') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(\"=== å®Ÿé¨“è¨­å®šã‚µãƒãƒªãƒ¼ ===\")\n",
    "print(f\"Algorithm: {config['learning_config']['algorithm']}\")\n",
    "print(f\"Quantum: {config['module_config']['quantum']['n_qubits']}Q{config['module_config']['quantum']['n_layers']}L\")\n",
    "print(f\"Embedding: {config['module_config']['quantum']['embedding_type']}\")\n",
    "print(f\"Entanglement: {config['module_config']['quantum']['entanglement']}\")\n",
    "print(f\"State Dimension: {config['module_config']['quantum']['state_dimension']}\")\n",
    "print(f\"Action Space: {config['module_config']['qmap']['action_dim']}D\")\n",
    "print(f\"Batch Size: {config['hyperparameters']['batch_size']}\")\n",
    "print(f\"Learning Rate: {config['hyperparameters']['learning_rate']}\")\n",
    "print(f\"Epochs: {config['hyperparameters']['epochs']}\")\n",
    "print(f\"Epsilon: {config['hyperparameters']['epsilon']} â†’ {config['hyperparameters']['epsilon_min']} (decay: {config['hyperparameters']['epsilon_decay']})\")\n",
    "print(f\"Replay Buffer: {config['hyperparameters']['replay_buffer_size']}\")\n",
    "print(f\"Target Update: every {config['hyperparameters']['target_update_freq']} episodes\")\n",
    "print(\"=\"*40)\n",
    "\n",
    "# è¨­å®šå€¤ã‚’å¤‰æ•°ã«å±•é–‹\n",
    "N_QUBITS = config['module_config']['quantum']['n_qubits']\n",
    "N_LAYERS = config['module_config']['quantum']['n_layers']\n",
    "STATE_DIM = config['module_config']['quantum']['state_dimension']\n",
    "ACTION_DIM = config['module_config']['qmap']['action_dim']\n",
    "BATCH_SIZE = config['hyperparameters']['batch_size']\n",
    "LEARNING_RATE = config['hyperparameters']['learning_rate']\n",
    "MAX_EPISODES = config['hyperparameters']['epochs']\n",
    "EPSILON_START = config['hyperparameters']['epsilon']\n",
    "EPSILON_DECAY = config['hyperparameters']['epsilon_decay']\n",
    "EPSILON_MIN = config['hyperparameters']['epsilon_min']\n",
    "BUFFER_SIZE = config['hyperparameters']['replay_buffer_size']\n",
    "TARGET_UPDATE = config['hyperparameters']['target_update_freq']\n",
    "GAMMA = config['hyperparameters']['gamma']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®Œå…¨ç‰ˆã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ç’°å¢ƒå®šç¾©å®Œäº†\n",
      "ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ç’°å¢ƒåˆæœŸåŒ–å®Œäº† - å ±é…¬æˆ¦ç•¥: adaptive_anti_collapse\n",
      "çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶: (252,)\n",
      "æœ‰åŠ¹æ‰‹æ•°: 8\n"
     ]
    }
   ],
   "source": [
    "# === å®Œå…¨ç‰ˆã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ç’°å¢ƒ ===\n",
    "class GeisterEnvironment:\n",
    "    \"\"\"å®Œå…¨ãªã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ã‚²ãƒ¼ãƒ ç’°å¢ƒï¼ˆJSONè¨­å®šå¯¾å¿œï¼‰\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.board_size = 6\n",
    "        self.forced_p2_setup_id = None\n",
    "        self.p2_backrow_cells = [(0, 1), (0, 2), (0, 3), (0, 4),\n",
    "                                (1, 1), (1, 2), (1, 3), (1, 4)]\n",
    "\n",
    "        self.reset()\n",
    "        \n",
    "        # å ±é…¬è¨­å®šï¼ˆJSONè¨­å®šã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰\n",
    "        reward_config = config['module_config']['reward']\n",
    "        self.capture_good_reward = reward_config['capture_good_reward']\n",
    "        self.capture_bad_penalty = reward_config['capture_bad_penalty']\n",
    "        self.escape_reward = reward_config['escape_reward']\n",
    "        self.captured_good_penalty = reward_config['captured_good_penalty']\n",
    "        self.captured_bad_reward = reward_config['captured_bad_reward']\n",
    "        self.position_rewards = reward_config['position_rewards']\n",
    "        \n",
    "        print(f\"ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ç’°å¢ƒåˆæœŸåŒ–å®Œäº† - å ±é…¬æˆ¦ç•¥: {reward_config['strategy']}\")\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"ã‚²ãƒ¼ãƒ çŠ¶æ…‹ã‚’ãƒªã‚»ãƒƒãƒˆ\"\"\"\n",
    "        self.board = np.zeros((6, 6), dtype=int)\n",
    "        self.turn = 0\n",
    "        self.current_player = 1\n",
    "        self.game_over = False\n",
    "        self.winner = None\n",
    "        self.captured_pieces = {'player_1': [], 'player_2': []}\n",
    "        self.move_history = []\n",
    "\n",
    "        # é…ç½®è¨­å®šï¼ˆJSONè¨­å®šã‹ã‚‰èª­ã¿è¾¼ã¿ï¼‰\n",
    "        placement_config = self.config['module_config']['placement']\n",
    "        \n",
    "        if placement_config['type'] == 'custom':\n",
    "            # ã‚«ã‚¹ã‚¿ãƒ é…ç½®ï¼ˆJSONæŒ‡å®šï¼‰\n",
    "            my_pieces = placement_config['my_pieces_config']\n",
    "            \n",
    "            # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼1ï¼ˆä¸‹å´ï¼‰\n",
    "            for i, row in enumerate(my_pieces):\n",
    "                for j, piece in enumerate(row):\n",
    "                    if piece != 0:\n",
    "                        self.board[4 + i][j] = piece\n",
    "\n",
    "            # ===== ã“ã“ã‹ã‚‰ P2ï¼ˆä¸Šå´ï¼‰ã®é…ç½® =====\n",
    "            # å–„=+2, æ‚ª=-2 ã‚’å‰æã«ã—ã¦ã„ã¾ã™ï¼ˆå¿…è¦ãªã‚‰ã‚ãªãŸã®ç¬¦å·ã«åˆã‚ã›ã¦å¤‰æ›´ï¼‰\n",
    "            GOOD, BAD = 2, -2\n",
    "\n",
    "            # 8ãƒã‚¹ï¼ˆ2x4ï¼‰ã®é †åºã¯ __init__ ã§å®šç¾©ã—ãŸ p2_backrow_cells ã‚’ä½¿ç”¨\n",
    "            # ã‚‚ã—æœªå®šç¾©ãªã‚‰å®šç¾©ã—ã¦ãŠã\n",
    "            if not hasattr(self, 'p2_backrow_cells'):\n",
    "                self.p2_backrow_cells = [(0, 1), (0, 2), (0, 3), (0, 4),\n",
    "                                        (1, 1), (1, 2), (1, 3), (1, 4)]\n",
    "\n",
    "            # ---- 8C4 ã®ã€Œkç•ªç›®ã®çµ„åˆã›ã€ã‚’å¾©å…ƒã™ã‚‹é–¢æ•°ï¼ˆè¾æ›¸é †ï¼‰----\n",
    "            from math import comb\n",
    "            def kth_comb_8_4(k: int):\n",
    "                \"\"\"0<=k<70 ã‚’ 8è¦ç´ ã‹ã‚‰4è¦ç´ ã‚’é¸ã¶è¾æ›¸é †ã®kç•ªç›®ã«å¯¾å¿œã•ã›ã€æ˜‡é †ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹(4ã¤)ã‚’è¿”ã™\"\"\"\n",
    "                res = []\n",
    "                n, r = 8, 4\n",
    "                x = 0\n",
    "                for i in range(r, 0, -1):\n",
    "                    for v in range(x, n):\n",
    "                        c = comb(n - v - 1, i - 1)\n",
    "                        if k < c:\n",
    "                            res.append(v)\n",
    "                            x = v + 1\n",
    "                            break\n",
    "                        k -= c\n",
    "                return res\n",
    "\n",
    "            # ---- P2é…ç½®ã®æ±ºå®šï¼šå¼·åˆ¶IDãŒã‚ã‚Œã°é©ç”¨ã€ãªã‘ã‚Œã°ãƒ©ãƒ³ãƒ€ãƒ  ----\n",
    "            if getattr(self, 'forced_p2_setup_id', None) is not None:\n",
    "                sid = int(self.forced_p2_setup_id) % 70\n",
    "                red_idx = kth_comb_8_4(sid)          # èµ¤ï¼ˆæ‚ªï¼‰ã®ä½ç½®(0..7)ã‚’å–å¾—\n",
    "                # ã¾ãšå…¨ã¦å–„ã«ã—ã¦ã‹ã‚‰ã€èµ¤ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã®æ‰€ã ã‘æ‚ªã«ç½®ãæ›ãˆ\n",
    "                for idx, (r, c) in enumerate(self.p2_backrow_cells):\n",
    "                    self.board[r][c] = GOOD\n",
    "                for idx in red_idx:\n",
    "                    r, c = self.p2_backrow_cells[idx]\n",
    "                    self.board[r][c] = BAD\n",
    "            else:\n",
    "                # å¾“æ¥ã®ãƒ©ãƒ³ãƒ€ãƒ é…ç½®ï¼ˆå–„4ã€æ‚ª4ï¼‰ã‚’ç¶­æŒ\n",
    "                pieces = [GOOD, GOOD, GOOD, GOOD, BAD, BAD, BAD, BAD]\n",
    "                random.shuffle(pieces)\n",
    "                for i, (r, c) in enumerate(self.p2_backrow_cells):\n",
    "                    self.board[r][c] = pieces[i]\n",
    "\n",
    "            # ---- å®Ÿéš›ã«ç”¨ã„ã‚‰ã‚ŒãŸ P2 é…ç½®ã®IDï¼ˆ0..69ï¼‰ã‚’è¨ˆç®—ã—ã¦ä¿æŒ ----\n",
    "            # èµ¤ï¼ˆæ‚ª=-2ï¼‰ãŒç½®ã‹ã‚ŒãŸã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ 0..7 ã§åé›†\n",
    "            red_positions = []\n",
    "            for idx, (r, c) in enumerate(self.p2_backrow_cells):\n",
    "                if self.board[r][c] == BAD:\n",
    "                    red_positions.append(idx)\n",
    "            red_positions.sort()\n",
    "\n",
    "            # rankï¼ˆ=çµ„åˆã›ã®è¾æ›¸é †ãƒ©ãƒ³ã‚¯ï¼‰ã‚’è¨ˆç®—ï¼š 8C4 = 70\n",
    "            rank = 0\n",
    "            last = -1\n",
    "            for i, rr in enumerate(red_positions):\n",
    "                start = last + 1\n",
    "                for x in range(start, rr):\n",
    "                    rank += comb(8 - (x + 1), 4 - (i + 1))\n",
    "                last = rr\n",
    "            self.p2_setup_id = int(rank)   # â† ã“ã‚Œã‚’å¾Œã§ãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ã«ä½¿ã„ã¾ã™\n",
    "\n",
    "        return self.get_state()\n",
    "\n",
    "    def get_state(self):\n",
    "        \"\"\"252æ¬¡å…ƒçŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«ç”Ÿæˆï¼ˆ7ãƒãƒ£ãƒ³ãƒãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°ï¼‰\"\"\"\n",
    "        state = np.zeros(STATE_DIM)  # 6*6*7 = 252\n",
    "        \n",
    "        for i in range(6):\n",
    "            for j in range(6):\n",
    "                base_idx = (i * 6 + j) * 7\n",
    "                value = self.board[i][j]\n",
    "                \n",
    "                # 7ãƒãƒ£ãƒ³ãƒãƒ«: ç©º, P1å–„, P1æ‚ª, P2å–„, P2æ‚ª, ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼, ã‚¿ãƒ¼ãƒ³\n",
    "                if value == 0:     # ç©º\n",
    "                    state[base_idx] = 1\n",
    "                elif value == 1:   # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼1å–„ç‰\n",
    "                    state[base_idx + 1] = 1\n",
    "                elif value == -1:  # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼1æ‚ªç‰\n",
    "                    state[base_idx + 2] = 1\n",
    "                elif value == 2:   # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼2å–„ç‰\n",
    "                    state[base_idx + 3] = 1\n",
    "                elif value == -2:  # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼2æ‚ªç‰\n",
    "                    state[base_idx + 4] = 1\n",
    "                \n",
    "                # è¿½åŠ æƒ…å ±\n",
    "                state[base_idx + 5] = 1 if self.current_player == 1 else 0\n",
    "                state[base_idx + 6] = min(self.turn / 100.0, 1.0)  # æ­£è¦åŒ–ã‚¿ãƒ¼ãƒ³\n",
    "        \n",
    "        return state\n",
    "\n",
    "    def get_valid_moves(self, player=None):\n",
    "        \"\"\"æœ‰åŠ¹ãªæ‰‹ã‚’36æ¬¡å…ƒã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆtoã‚»ãƒ«ï¼‰ã§å–å¾—\"\"\"\n",
    "        if player is None:\n",
    "            player = self.current_player\n",
    "\n",
    "        valid_moves = []\n",
    "        piece_values = [1, -1] if player == 1 else [2, -2]\n",
    "\n",
    "        for from_i in range(6):\n",
    "            for from_j in range(6):\n",
    "                if self.board[from_i][from_j] in piece_values:\n",
    "                    for di, dj in [(0, 1), (0, -1), (1, 0), (-1, 0)]:\n",
    "                        to_i, to_j = from_i + di, from_j + dj\n",
    "                        if 0 <= to_i < 6 and 0 <= to_j < 6:\n",
    "                            target = self.board[to_i][to_j]\n",
    "                            if (target == 0 or\n",
    "                                (player == 1 and abs(target) == 2) or\n",
    "                                (player == 2 and abs(target) == 1)):\n",
    "                                idx = to_i * 6 + to_j  # â† 0..35 ã«å›ºå®š\n",
    "                                valid_moves.append((idx, \"move\", (from_i, from_j), (to_i, to_j)))\n",
    "        return valid_moves\n",
    "\n",
    "    def make_move(self, move):\n",
    "        \"\"\"æ‰‹ã‚’å®Ÿè¡Œã—ã€å ±é…¬ã‚’è¨ˆç®—\"\"\"\n",
    "        move_index, direction, from_pos, to_pos = move\n",
    "        from_i, from_j = from_pos\n",
    "        to_i, to_j = to_pos\n",
    "        \n",
    "        piece = self.board[from_i][from_j]\n",
    "        target = self.board[to_i][to_j]\n",
    "        \n",
    "        # ç§»å‹•è¨˜éŒ²\n",
    "        self.move_history.append({\n",
    "            'turn': self.turn,\n",
    "            'player': self.current_player,\n",
    "            'move': move,\n",
    "            'piece': piece,\n",
    "            'captured': target if target != 0 else None\n",
    "        })\n",
    "        \n",
    "        # é§’ã‚’ç§»å‹•\n",
    "        self.board[from_i][from_j] = 0\n",
    "        self.board[to_i][to_j] = piece\n",
    "        \n",
    "        # å ±é…¬è¨ˆç®—\n",
    "        reward = self._calculate_reward(piece, target, from_pos, to_pos)\n",
    "        \n",
    "        # å‹åˆ©æ¡ä»¶ãƒã‚§ãƒƒã‚¯\n",
    "        done = self._check_win_condition(piece, to_pos, target)\n",
    "        \n",
    "        # ã‚¿ãƒ¼ãƒ³é€²è¡Œ\n",
    "        self.turn += 1\n",
    "        \n",
    "        # æœ€å¤§ã‚¿ãƒ¼ãƒ³æ•°ãƒã‚§ãƒƒã‚¯\n",
    "        if self.turn >= 200:  # é•·æœŸæˆ¦å¯¾å¿œ\n",
    "            self.game_over = True\n",
    "            self.winner = None\n",
    "            done = True\n",
    "        \n",
    "        if not done:\n",
    "            self.current_player = 2 if self.current_player == 1 else 1\n",
    "        \n",
    "        return self.get_state(), reward, done, {\n",
    "            'captured': target,\n",
    "            'winner': self.winner,\n",
    "            'turn': self.turn\n",
    "        }\n",
    "    \n",
    "    def _calculate_reward(self, piece, target, from_pos, to_pos):\n",
    "        \"\"\"è©³ç´°å ±é…¬è¨ˆç®—ï¼ˆJSONè¨­å®šæº–æ‹ ï¼‰\"\"\"\n",
    "        reward = 0.0\n",
    "        \n",
    "        # é§’æ•ç²å ±é…¬\n",
    "        if target != 0:\n",
    "            if target > 0:  # å–„ç‰æ•ç²\n",
    "                reward += self.capture_good_reward\n",
    "            else:  # æ‚ªç‰æ•ç²\n",
    "                reward += self.capture_bad_penalty\n",
    "        \n",
    "        # ä½ç½®çš„å ±é…¬\n",
    "        from_i, from_j = from_pos\n",
    "        to_i, to_j = to_pos\n",
    "        \n",
    "        # å‰é€²å ±é…¬\n",
    "        if abs(piece) == 1 and piece > 0:  # å–„ç‰ã®å ´åˆ\n",
    "            if self.current_player == 1 and to_i < from_i:  # å‰é€²\n",
    "                reward += self.position_rewards['advance_toward_escape']\n",
    "            elif self.current_player == 2 and to_i > from_i:  # å‰é€²\n",
    "                reward += self.position_rewards['advance_toward_escape']\n",
    "        \n",
    "        # ä¸­å¤®åˆ¶å¾¡\n",
    "        if 2 <= to_i <= 3 and 2 <= to_j <= 3:\n",
    "            reward += self.position_rewards['center_control']\n",
    "        \n",
    "        # ç›¸æ‰‹é™£åœ°é€²å…¥\n",
    "        if ((self.current_player == 1 and to_i <= 2) or \n",
    "            (self.current_player == 2 and to_i >= 3)):\n",
    "            reward += self.position_rewards['opponent_territory']\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def _check_win_condition(self, piece, to_pos, captured):\n",
    "        \"\"\"å‹åˆ©æ¡ä»¶åˆ¤å®š\"\"\"\n",
    "        to_i, to_j = to_pos\n",
    "        \n",
    "        # è„±å‡ºå‹åˆ©\n",
    "        if abs(piece) == 1 and piece > 0:  # å–„ç‰\n",
    "            if ((self.current_player == 1 and to_i == 0 and to_j in [0, 5]) or\n",
    "                (self.current_player == 2 and to_i == 5 and to_j in [0, 5])):\n",
    "                self.game_over = True\n",
    "                self.winner = self.current_player\n",
    "                return True\n",
    "        \n",
    "        # å–„ç‰å…¨æ•ç²å‹åˆ©\n",
    "        if captured is not None and captured > 0:\n",
    "            opponent_good_count = 0\n",
    "            search_value = 2 if self.current_player == 1 else 1\n",
    "            \n",
    "            for i in range(6):\n",
    "                for j in range(6):\n",
    "                    if self.board[i][j] == search_value:\n",
    "                        opponent_good_count += 1\n",
    "            \n",
    "            if opponent_good_count == 0:\n",
    "                self.game_over = True\n",
    "                self.winner = self.current_player\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "\n",
    "print(\"å®Œå…¨ç‰ˆã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ç’°å¢ƒå®šç¾©å®Œäº†\")\n",
    "\n",
    "# ç’°å¢ƒãƒ†ã‚¹ãƒˆ\n",
    "test_env = GeisterEnvironment(config)\n",
    "test_state = test_env.reset()\n",
    "print(f\"çŠ¶æ…‹ãƒ™ã‚¯ãƒˆãƒ«å½¢çŠ¶: {test_state.shape}\")\n",
    "print(f\"æœ‰åŠ¹æ‰‹æ•°: {len(test_env.get_valid_moves())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®Œå…¨ç‰ˆCQCNNå®šç¾©å®Œäº†\n",
      "CQCNNåˆæœŸåŒ–: 252D â†’ 4Q2L â†’ 36D\n",
      "Frontend: 4 Linear layers\n",
      "Quantum: 4 qubits, 2 layers, angle embedding\n",
      "Backend: 5 Linear layers\n",
      "\n",
      "ãƒ¢ãƒ‡ãƒ«æ§‹é€ ãƒ†ã‚¹ãƒˆä¸­...\n",
      "å…¥åŠ›å½¢çŠ¶: torch.Size([4, 252])\n",
      "å‡ºåŠ›å½¢çŠ¶: torch.Size([4, 36])\n",
      "å‡ºåŠ›ç¯„å›²: [-0.786, 0.611]\n",
      "\n",
      "ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°:\n",
      "  Classical: 73,128\n",
      "  Quantum: 16\n",
      "  Total: 73,144\n",
      "\n",
      "CQCNNãƒ†ã‚¹ãƒˆå®Œäº†!\n"
     ]
    }
   ],
   "source": [
    "# === å®Œå…¨ç‰ˆCQCNNå®Ÿè£… ===\n",
    "class CQCNN(nn.Module):\n",
    "    \"\"\"å®Œå…¨ãªClassical-Quantum Convolutional Neural Network\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # è¨­å®šèª­ã¿è¾¼ã¿\n",
    "        arch_config = config['architecture']\n",
    "        quantum_config = config['module_config']['quantum']\n",
    "        \n",
    "        self.n_qubits = quantum_config['n_qubits']\n",
    "        self.n_layers = quantum_config['n_layers']\n",
    "        self.state_dim = quantum_config['state_dimension']\n",
    "        self.action_dim = config['module_config']['qmap']['action_dim']\n",
    "        \n",
    "        print(f\"CQCNNåˆæœŸåŒ–: {self.state_dim}D â†’ {self.n_qubits}Q{self.n_layers}L â†’ {self.action_dim}D\")\n",
    "        \n",
    "        # Frontend CNN (252 â†’ 4)\n",
    "        self.frontend_layers = self._build_layers(arch_config['frontend_cnn']['layers'])\n",
    "        \n",
    "        # Quantum Section\n",
    "        self.dev = qml.device(\n",
    "            arch_config['quantum_section']['device'], \n",
    "            wires=self.n_qubits\n",
    "        )\n",
    "        self.quantum_params = nn.Parameter(\n",
    "            torch.randn(self.n_layers, self.n_qubits, 2) * 0.1\n",
    "        )\n",
    "        self.quantum_node = qml.QNode(\n",
    "            self._quantum_circuit, \n",
    "            self.dev, \n",
    "            interface='torch'\n",
    "        )\n",
    "        \n",
    "        # Backend CNN (4 â†’ 36)\n",
    "        self.backend_layers = self._build_layers(arch_config['backend_cnn']['layers'])\n",
    "        \n",
    "        print(f\"Frontend: {len([l for l in self.frontend_layers if isinstance(l, nn.Linear)])} Linear layers\")\n",
    "        print(f\"Quantum: {self.n_qubits} qubits, {self.n_layers} layers, {quantum_config['embedding_type']} embedding\")\n",
    "        print(f\"Backend: {len([l for l in self.backend_layers if isinstance(l, nn.Linear)])} Linear layers\")\n",
    "\n",
    "    def _build_layers(self, layer_configs):\n",
    "        \"\"\"JSONè¨­å®šã‹ã‚‰ãƒ¬ã‚¤ãƒ¤ãƒ¼æ§‹ç¯‰\"\"\"\n",
    "        layers = []\n",
    "        \n",
    "        for layer_config in layer_configs:\n",
    "            layer_type = layer_config['type']\n",
    "            \n",
    "            if layer_type == 'linear':\n",
    "                layers.append(nn.Linear(\n",
    "                    layer_config['in_features'],\n",
    "                    layer_config['out_features']\n",
    "                ))\n",
    "            elif layer_type == 'batch_norm':\n",
    "                layers.append(nn.BatchNorm1d(layer_config['num_features']))\n",
    "            elif layer_type == 'relu':\n",
    "                layers.append(nn.ReLU())\n",
    "            elif layer_type == 'tanh':\n",
    "                layers.append(nn.Tanh())\n",
    "            elif layer_type == 'dropout':\n",
    "                layers.append(nn.Dropout(layer_config['p']))\n",
    "        \n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _quantum_circuit(self, features, params):\n",
    "        \"\"\"é‡å­å›è·¯ï¼ˆJSONè¨­å®šæº–æ‹ ï¼‰\"\"\"\n",
    "        # Angle embedding\n",
    "        for i in range(self.n_qubits):\n",
    "            qml.RY(features[i], wires=i)\n",
    "        \n",
    "        # Variational layers\n",
    "        for layer in range(self.n_layers):\n",
    "            for i in range(self.n_qubits):\n",
    "                qml.RY(params[layer, i, 0], wires=i)\n",
    "                qml.RZ(params[layer, i, 1], wires=i)\n",
    "            \n",
    "            # Linear entanglement\n",
    "            for i in range(self.n_qubits - 1):\n",
    "                qml.CNOT(wires=[i, i + 1])\n",
    "        \n",
    "        return [qml.expval(qml.PauliZ(i)) for i in range(self.n_qubits)]\n",
    "\n",
    "    def forward(self, state):\n",
    "        \"\"\"é †ä¼æ’­å‡¦ç†\"\"\"\n",
    "        batch_size = state.shape[0]\n",
    "        \n",
    "        # Frontend processing\n",
    "        state_flat = state.view(batch_size, -1)  # Flatten\n",
    "        frontend_out = self.frontend_layers(state_flat)  # (batch, 4)\n",
    "        \n",
    "        # Quantum processing (batchå¯¾å¿œ)\n",
    "        quantum_outputs = []\n",
    "        for i in range(batch_size):\n",
    "            q_out = self.quantum_node(frontend_out[i], self.quantum_params)\n",
    "            quantum_outputs.append(torch.stack(q_out))\n",
    "        \n",
    "        quantum_out = torch.stack(quantum_outputs)  # (batch, 4)\n",
    "        \n",
    "        # Backend processing\n",
    "        output = self.backend_layers(quantum_out)  # (batch, 36)\n",
    "        \n",
    "        return output\n",
    "\n",
    "print(\"å®Œå…¨ç‰ˆCQCNNå®šç¾©å®Œäº†\")\n",
    "\n",
    "# ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ\n",
    "test_model = CQCNN(config)\n",
    "test_input = torch.randn(4, STATE_DIM)  # ãƒãƒƒãƒã‚µã‚¤ã‚º4\n",
    "\n",
    "print(\"\\nãƒ¢ãƒ‡ãƒ«æ§‹é€ ãƒ†ã‚¹ãƒˆä¸­...\")\n",
    "with torch.no_grad():\n",
    "    test_output = test_model(test_input)\n",
    "    print(f\"å…¥åŠ›å½¢çŠ¶: {test_input.shape}\")\n",
    "    print(f\"å‡ºåŠ›å½¢çŠ¶: {test_output.shape}\")\n",
    "    print(f\"å‡ºåŠ›ç¯„å›²: [{test_output.min():.3f}, {test_output.max():.3f}]\")\n",
    "    \n",
    "    # ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°è¨ˆç®—\n",
    "    total_params = sum(p.numel() for p in test_model.parameters() if p.requires_grad)\n",
    "    quantum_params = test_model.quantum_params.numel()\n",
    "    classical_params = total_params - quantum_params\n",
    "    \n",
    "    print(f\"\\nãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿æ•°:\")\n",
    "    print(f\"  Classical: {classical_params:,}\")\n",
    "    print(f\"  Quantum: {quantum_params:,}\")\n",
    "    print(f\"  Total: {total_params:,}\")\n",
    "\n",
    "print(\"\\nCQCNNãƒ†ã‚¹ãƒˆå®Œäº†!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "åæŸæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ å®šç¾©å®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# === åæŸæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ  ===\n",
    "class ConvergenceDetector:\n",
    "    \"\"\"åæŸæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ ï¼ˆ0.95é–¾å€¤ï¼‰\"\"\"\n",
    "\n",
    "    def __init__(self, patience=50, min_games=1000, balance_threshold=0.95):\n",
    "        self.patience = patience\n",
    "        self.min_games = min_games\n",
    "        self.balance_threshold = balance_threshold\n",
    "        self.consecutive_good = 0\n",
    "        self.best_balance = 0.0\n",
    "        self.convergence_history = []\n",
    "        \n",
    "        \n",
    "        print(f\"åæŸæ¤œå‡ºå™¨: é–¾å€¤={balance_threshold}, patience={patience}, æœ€å°ã‚²ãƒ¼ãƒ æ•°={min_games}\")\n",
    "\n",
    "    def check_convergence(self, game_results, episode):\n",
    "        \"\"\"åæŸåˆ¤å®š\"\"\"\n",
    "        if len(game_results) < self.min_games:\n",
    "            return False, 0.0, {'reason': 'insufficient_games', 'games': len(game_results)}\n",
    "\n",
    "        # æœ€è¿‘ã®çµæœã‚’åˆ†æ\n",
    "        recent_games = game_results[-500:]\n",
    "        \n",
    "        wins_1 = sum(1 for r in recent_games if r.get('winner') == 1)\n",
    "        wins_2 = sum(1 for r in recent_games if r.get('winner') == 2)\n",
    "        draws = sum(1 for r in recent_games if r.get('winner') is None)\n",
    "        \n",
    "        total_games = len(recent_games)\n",
    "        decisive_games = wins_1 + wins_2\n",
    "        \n",
    "        # ãƒãƒ©ãƒ³ã‚¹è¨ˆç®—\n",
    "        if decisive_games > 0:\n",
    "            balance = min(wins_1, wins_2) / max(wins_1, wins_2)\n",
    "            win_rate_1 = wins_1 / total_games\n",
    "            win_rate_2 = wins_2 / total_games\n",
    "            draw_rate = draws / total_games\n",
    "        else:\n",
    "            balance = 1.0  # å…¨å¼•ãåˆ†ã‘ã®å ´åˆ\n",
    "            win_rate_1 = win_rate_2 = 0.0\n",
    "            draw_rate = 1.0\n",
    "        \n",
    "        # åæŸåˆ¤å®š\n",
    "        is_balanced = balance >= self.balance_threshold\n",
    "        has_active_games = decisive_games >= 50  # å°‘ãªãã¨ã‚‚50ã‚²ãƒ¼ãƒ ã¯æ±ºç€\n",
    "        \n",
    "        metrics = {\n",
    "            'balance': balance,\n",
    "            'win_rate_1': win_rate_1,\n",
    "            'win_rate_2': win_rate_2,\n",
    "            'draw_rate': draw_rate,\n",
    "            'decisive_games': decisive_games,\n",
    "            'total_games': total_games,\n",
    "            'is_balanced': is_balanced,\n",
    "            'has_active_games': has_active_games\n",
    "        }\n",
    "        \n",
    "        # é€£ç¶šã‚«ã‚¦ãƒ³ãƒˆ\n",
    "        if is_balanced and has_active_games:\n",
    "            self.consecutive_good += 1\n",
    "            metrics['status'] = 'converging'\n",
    "        else:\n",
    "            self.consecutive_good = 0\n",
    "            if not is_balanced:\n",
    "                metrics['status'] = 'unbalanced'\n",
    "            elif not has_active_games:\n",
    "                metrics['status'] = 'too_many_draws'\n",
    "        \n",
    "        metrics['consecutive_good'] = self.consecutive_good\n",
    "        self.best_balance = max(self.best_balance, balance)\n",
    "        metrics['best_balance'] = self.best_balance\n",
    "        \n",
    "        # å±¥æ­´è¨˜éŒ²\n",
    "        self.convergence_history.append({\n",
    "            'episode': episode,\n",
    "            'metrics': metrics.copy()\n",
    "        })\n",
    "        \n",
    "        # åæŸåˆ¤å®š\n",
    "        converged = self.consecutive_good >= self.patience\n",
    "        \n",
    "        return converged, balance, metrics\n",
    "\n",
    "print(\"åæŸæ¤œå‡ºã‚·ã‚¹ãƒ†ãƒ å®šç¾©å®Œäº†\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å®Œå…¨ç‰ˆCQCNNå®Ÿé¨“ã‚¯ãƒ©ã‚¹å®šç¾©å®Œäº†\n"
     ]
    }
   ],
   "source": [
    "# === ãƒ¡ã‚¤ãƒ³å®Ÿé¨“ã‚¯ãƒ©ã‚¹ ===\n",
    "class CQCNNExperiment:\n",
    "    \"\"\"å®Œå…¨ç‰ˆCQCNNè‡ªå·±å¯¾æˆ¦å®Ÿé¨“\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        self.env = GeisterEnvironment(config)\n",
    "\n",
    "        # 70é…ç½®ã®æˆ¦ç¸¾ã¨ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«è¨­å®š\n",
    "        self.setup_stats = {sid: {'w':0, 'd':0, 'l':0, 'n':0} for sid in range(70)}\n",
    "        self.min_coverage_per_sid = 50     # å„setupã®æœ€ä½å¯¾å±€æ•°\n",
    "        self.ucb_alpha = 0.8               # UCBã®æ¢ç´¢é‡ã¿\n",
    "        self.ucb_top_k = 8                 # UCBã‚¹ã‚³ã‚¢ä¸Šä½ã‹ã‚‰æŠ½é¸\n",
    "        self.role_mirror_prob = 0.15       # ï¼ˆå°†æ¥ï¼‰P1å¯å¤‰/P2å›ºå®šã®ãƒŸãƒ©ãƒ¼ã‚’æ··ãœã‚‹ç¢ºç‡\n",
    "\n",
    "        # ãƒ¢ãƒ‹ã‚¿ãƒ¼ç”¨CSV\n",
    "        self.metrics_csv_path = 'training_metrics.csv'\n",
    "        with open(self.metrics_csv_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('episode,mean_wr,worst_decile,variance,coverage_min,coverage_ok\\n')\n",
    "        \n",
    "        # ãƒ¢ãƒ‡ãƒ«åˆæœŸåŒ–\n",
    "        self.cqcnn_1 = CQCNN(config)\n",
    "        self.cqcnn_2 = CQCNN(config)\n",
    "        \n",
    "        # ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼2ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å°‘ã—å¤‰æ›´ï¼ˆå¤šæ§˜æ€§ç¢ºä¿ï¼‰\n",
    "        with torch.no_grad():\n",
    "            for param in self.cqcnn_2.parameters():\n",
    "                param.add_(torch.randn_like(param) * 0.01)\n",
    "        \n",
    "        # ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼\n",
    "        self.optimizer_1 = optim.Adam(self.cqcnn_1.parameters(), lr=LEARNING_RATE)\n",
    "        self.optimizer_2 = optim.Adam(self.cqcnn_2.parameters(), lr=LEARNING_RATE)\n",
    "        \n",
    "        # å­¦ç¿’é–¢é€£\n",
    "        self.criterion = nn.MSELoss()\n",
    "        self.replay_buffer_1 = deque(maxlen=BUFFER_SIZE)\n",
    "        self.replay_buffer_2 = deque(maxlen=BUFFER_SIZE)\n",
    "        \n",
    "        # çµ±è¨ˆè¨˜éŒ²\n",
    "        self.game_results = []\n",
    "        self.losses_1 = []\n",
    "        self.losses_2 = []\n",
    "        self.epsilon_history = []\n",
    "        self.training_metrics = []\n",
    "        \n",
    "        # åæŸæ¤œå‡º\n",
    "        self.convergence_detector = ConvergenceDetector(\n",
    "            balance_threshold=0.95,  # 95%ãƒãƒ©ãƒ³ã‚¹é–¾å€¤\n",
    "            patience=50,\n",
    "            min_games=1000\n",
    "        )\n",
    "        \n",
    "        # ===== è¿½åŠ : é…ç½®ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒªãƒ³ã‚° & ãƒ¢ãƒ‹ã‚¿åˆæœŸåŒ– =====\n",
    "        self.setup_stats = {sid: {'w':0, 'd':0, 'l':0, 'n':0} for sid in range(70)}\n",
    "        self.min_coverage_per_sid = 50     # å„setupã®æœ€ä½å¯¾å±€æ•°\n",
    "        self.ucb_alpha = 0.8               # UCBæ¢ç´¢é‡ã¿\n",
    "        self.ucb_top_k = 8                 # UCBä¸Šä½ã‹ã‚‰ãƒ©ãƒ³ãƒ€ãƒ æŠ½é¸\n",
    "        self.role_mirror_prob = 0.15       # ï¼ˆæ‹¡å¼µç”¨ï¼‰P1å¯å¤‰/P2å›ºå®šã‚’æ··ãœã‚‹ç¢ºç‡\n",
    "        self.metrics_csv_path = 'training_metrics.csv'\n",
    "        with open(self.metrics_csv_path, 'w', encoding='utf-8') as f:\n",
    "            f.write('episode,mean_wr,worst_decile,variance,coverage_min,coverage_ok\\n')\n",
    "        # ================================================\n",
    "\n",
    "        # ãã®ä»–\n",
    "        self.start_time = time.time()\n",
    "        self.episode_times = []\n",
    "        \n",
    "        print(\"\\n=== CQCNNå®Ÿé¨“åˆæœŸåŒ–å®Œäº† ===\")\n",
    "        print(f\"ãƒ¢ãƒ‡ãƒ«1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {sum(p.numel() for p in self.cqcnn_1.parameters()):,}\")\n",
    "        print(f\"ãƒ¢ãƒ‡ãƒ«2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: {sum(p.numel() for p in self.cqcnn_2.parameters()):,}\")\n",
    "        print(f\"ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {BUFFER_SIZE:,}\")\n",
    "        print(f\"ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}\")\n",
    "        print(f\"æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {MAX_EPISODES:,}\")\n",
    "\n",
    "    # ===== è¿½åŠ : é…ç½®ãƒ¢ãƒ‹ã‚¿/ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ©ã®ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ =====\n",
    "    def _record_setup_result(self, setup_id: int, winner: int | None):\n",
    "        s = self.setup_stats[setup_id]\n",
    "        if winner is None: s['d'] += 1\n",
    "        elif winner == 1:  s['w'] += 1\n",
    "        else:              s['l'] += 1\n",
    "        s['n'] += 1\n",
    "\n",
    "    def _compute_setup_metrics(self):\n",
    "        wrs, counts = [], []\n",
    "        for sid in range(70):\n",
    "            s = self.setup_stats[sid]; n = max(1, s['n'])\n",
    "            wr = (s['w'] + 0.5*s['d']) / n\n",
    "            wrs.append(wr); counts.append(s['n'])\n",
    "        wrs = np.array(wrs); counts = np.array(counts)\n",
    "        mean_wr = float(wrs.mean())\n",
    "        worst_decile = float(np.sort(wrs)[:max(1, 70//10)].mean())\n",
    "        variance = float(wrs.var())\n",
    "        coverage_ok = bool(counts.min() >= self.min_coverage_per_sid)\n",
    "        return dict(mean_wr=mean_wr, worst_decile=worst_decile,\n",
    "                    variance=variance, coverage_ok=coverage_ok,\n",
    "                    coverage_min=int(counts.min()))\n",
    "\n",
    "    def _pick_setup_min_coverage(self):\n",
    "        need = [sid for sid in range(70) if self.setup_stats[sid]['n'] < self.min_coverage_per_sid]\n",
    "        return random.choice(need) if need else None\n",
    "\n",
    "    def _pick_setup_ucb(self):\n",
    "        import math  # å±€æ‰€importã§ä¾å­˜ã‚’é–‰ã˜ã‚‹\n",
    "        total = 1 + sum(self.setup_stats[sid]['n'] for sid in range(70))\n",
    "        scores = []\n",
    "        for sid in range(70):\n",
    "            s = self.setup_stats[sid]; n = s['n']\n",
    "            wr = (s['w'] + 0.5*s['d']) / max(1, n)\n",
    "            exploit = 1.0 - wr\n",
    "            explore = self.ucb_alpha * math.sqrt(math.log(total) / (1 + n))\n",
    "            scores.append((exploit + explore, sid))\n",
    "        scores.sort(reverse=True)\n",
    "        cand = [sid for _, sid in scores[:self.ucb_top_k]]\n",
    "        return random.choice(cand)\n",
    "\n",
    "    def choose_p2_setup_id(self):\n",
    "        sid = self._pick_setup_min_coverage()\n",
    "        if sid is not None: return sid\n",
    "        return self._pick_setup_ucb()\n",
    "    # =====================================================\n",
    "\n",
    "    def select_action(self, model, state, valid_moves, epsilon):\n",
    "        \"\"\"36æ¬¡å…ƒç©ºé–“ã§ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³é¸æŠ\"\"\"\n",
    "        if random.random() < epsilon:\n",
    "            return random.choice(valid_moves)\n",
    "\n",
    "        was_training = model.training\n",
    "        model.eval()  # æ¨è«–ãƒ¢ãƒ¼ãƒ‰ã«è¨­å®š\n",
    "        with torch.no_grad():\n",
    "            state_tensor = torch.FloatTensor(state).flatten().unsqueeze(0)  # (1, 252)\n",
    "            q_values = model(state_tensor).squeeze(0)  # (36,)\n",
    "        if was_training:\n",
    "            model.train()  # å…ƒã®ãƒ¢ãƒ¼ãƒ‰ã«æˆ»ã™\n",
    "\n",
    "        # æœ‰åŠ¹ãªæ‰‹ã®ä¸­ã‹ã‚‰æœ€å¤§Qå€¤ã‚’é¸æŠ\n",
    "        best_score = float('-inf')\n",
    "        best_move = valid_moves[0]\n",
    "\n",
    "        for move in valid_moves:\n",
    "            move_index, direction, from_pos, to_pos = move\n",
    "            if move_index < len(q_values):\n",
    "                score = q_values[move_index].item()\n",
    "                if score > best_score:\n",
    "                    best_score = score\n",
    "                    best_move = move\n",
    "        return best_move\n",
    "\n",
    "    def play_game(self, episode):\n",
    "        \"\"\"1ã‚²ãƒ¼ãƒ ã‚’å®Ÿè¡Œ\"\"\"\n",
    "        # ï¼ˆå°†æ¥: å½¹å‰²ãƒŸãƒ©ãƒ¼ã‚’ã™ã‚‹ãªã‚‰ã“ã“ã§åˆ†å²ï¼‰\n",
    "        self.env.forced_p2_setup_id = self.choose_p2_setup_id()\n",
    "\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        \n",
    "        game_states_1 = []\n",
    "        game_states_2 = []\n",
    "        \n",
    "        epsilon = max(EPSILON_MIN, EPSILON_START * (EPSILON_DECAY ** episode))\n",
    "        \n",
    "        while not done:\n",
    "            valid_moves = self.env.get_valid_moves()\n",
    "            \n",
    "            if not valid_moves:\n",
    "                # æœ‰åŠ¹æ‰‹ãªã—ï¼ˆç¨€ãªã‚±ãƒ¼ã‚¹ï¼‰\n",
    "                self.env.game_over = True\n",
    "                self.env.winner = None\n",
    "                break\n",
    "            \n",
    "            current_state = state.copy()\n",
    "            current_player = self.env.current_player\n",
    "            \n",
    "            if current_player == 1:\n",
    "                chosen_move = self.select_action(self.cqcnn_1, current_state, valid_moves, epsilon)\n",
    "            else:\n",
    "                chosen_move = self.select_action(self.cqcnn_2, current_state, valid_moves, epsilon)\n",
    "            \n",
    "            next_state, reward, done, info = self.env.make_move(chosen_move)\n",
    "            \n",
    "            # çµŒé¨“ã‚’è¨˜éŒ²ï¼ˆmove_indexã¯ã‚¿ãƒ—ãƒ«å…ˆé ­ã‚’ä½¿ç”¨ï¼‰\n",
    "            if isinstance(chosen_move, (tuple, list)) and len(chosen_move) >= 1:\n",
    "                move_index = int(chosen_move[0])  # 0..35\n",
    "            else:\n",
    "                move_index = int(chosen_move)\n",
    "            experience = (current_state, move_index, reward, next_state, done)\n",
    "            \n",
    "            if current_player == 1:\n",
    "                game_states_1.append(experience)\n",
    "            else:\n",
    "                game_states_2.append(experience)\n",
    "            \n",
    "            state = next_state\n",
    "        \n",
    "        # ã‚²ãƒ¼ãƒ çµæœã‚’è¨˜éŒ²\n",
    "        result = {\n",
    "            'episode': episode,\n",
    "            'winner': self.env.winner,\n",
    "            'turns': self.env.turn,\n",
    "            'player_1_moves': len(game_states_1),\n",
    "            'player_2_moves': len(game_states_2)\n",
    "        }\n",
    "        \n",
    "        # æœ€çµ‚å ±é…¬ã®é…å¸ƒï¼ˆå‹è€…ã«+1ã€æ•—è€…ã«-1ã€å¼•ãåˆ†ã‘ã¯0ï¼‰\n",
    "        final_reward_1 = 1.0 if self.env.winner == 1 else (-1.0 if self.env.winner == 2 else 0.0)\n",
    "        final_reward_2 = 1.0 if self.env.winner == 2 else (-1.0 if self.env.winner == 1 else 0.0)\n",
    "        \n",
    "        # çµŒé¨“ã‚’ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ã«è¿½åŠ ï¼ˆsidä»˜ã6ã‚¿ãƒ—ãƒ«ï¼‰\n",
    "        sid = getattr(self.env, 'p2_setup_id', None)\n",
    "        if sid is not None:\n",
    "            # P1è¦–ç‚¹ã§ w/d/l ã‚’æ›´æ–°ï¼ˆwinner: 1=å…ˆæ‰‹å‹ã¡, 2=å¾Œæ‰‹å‹ã¡, None=å¼•åˆ†ï¼‰\n",
    "            self._record_setup_result(sid, self.env.winner)\n",
    "\n",
    "        for state, action, reward, next_state, done in game_states_1:\n",
    "            final_exp = (state, action, reward + final_reward_1, next_state, done, sid)\n",
    "            self.replay_buffer_1.append(final_exp)\n",
    "\n",
    "        for state, action, reward, next_state, done in game_states_2:\n",
    "            final_exp = (state, action, reward + final_reward_2, next_state, done, sid)\n",
    "            self.replay_buffer_2.append(final_exp)\n",
    "        \n",
    "        return result, epsilon\n",
    "\n",
    "    def train_model(self, model, optimizer, replay_buffer, losses_list):\n",
    "        \"\"\"ãƒ¢ãƒ‡ãƒ«å­¦ç¿’\"\"\"\n",
    "        if len(replay_buffer) < BATCH_SIZE:\n",
    "            return\n",
    "        \n",
    "        # ãƒãƒƒãƒã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ï¼ˆâ€» sidä»˜ã6ã‚¿ãƒ—ãƒ«/æ—§5ã‚¿ãƒ—ãƒ«ä¸¡å¯¾å¿œï¼‰\n",
    "        batch = random.sample(replay_buffer, BATCH_SIZE)\n",
    "\n",
    "        # å¾Œæ–¹äº’æ›ã§å±•é–‹\n",
    "        states = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        dones = []\n",
    "        # sids ã¯ä»Šã¯æœªä½¿ç”¨ã ãŒå°†æ¥ã®åˆ†å±¤å­¦ç¿’ã§åˆ©ç”¨äºˆå®š\n",
    "        for exp in batch:\n",
    "            if len(exp) == 6:\n",
    "                s, a, r, ns, d, sid = exp\n",
    "            else:\n",
    "                s, a, r, ns, d = exp\n",
    "                sid = None\n",
    "            states.append(s)\n",
    "            actions.append(a)\n",
    "            rewards.append(r)\n",
    "            next_states.append(ns)\n",
    "            dones.append(d)\n",
    "\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatFloatTensor(next_states) if hasattr(torch, \"FloatFloatTensor\") else torch.FloatTensor(next_states)\n",
    "        dones = torch.BoolTensor(dones)\n",
    "        \n",
    "        # ç¾åœ¨ã®Qå€¤\n",
    "        current_q_values = model(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "        \n",
    "        # ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤ï¼ˆç°¡å˜ãªTDå­¦ç¿’ï¼‰\n",
    "        with torch.no_grad():\n",
    "            next_q_values = model(next_states).max(1)[0]\n",
    "            target_q_values = rewards + (GAMMA * next_q_values * ~dones)\n",
    "        \n",
    "        # æå¤±è¨ˆç®—ã¨æ›´æ–°\n",
    "        loss = self.criterion(current_q_values, target_q_values)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        losses_list.append(loss.item())\n",
    "\n",
    "    def run_experiment(self):\n",
    "        \"\"\"ãƒ¡ã‚¤ãƒ³å®Ÿé¨“å®Ÿè¡Œ\"\"\"\n",
    "        print(f\"\\n=== å®Ÿé¨“é–‹å§‹: åæŸã¾ã§æœ€å¤§{MAX_EPISODES:,}ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ ===\")\n",
    "        print(f\"åæŸæ¡ä»¶: Balance â‰¥ 0.95, {self.convergence_detector.patience}å›é€£ç¶š\")\n",
    "        \n",
    "        for episode in range(MAX_EPISODES):\n",
    "            episode_start = time.time()\n",
    "            \n",
    "            # ã‚²ãƒ¼ãƒ å®Ÿè¡Œ\n",
    "            result, epsilon = self.play_game(episode)\n",
    "            self.game_results.append(result)\n",
    "            self.epsilon_history.append(epsilon)\n",
    "            \n",
    "            # å­¦ç¿’å®Ÿè¡Œ\n",
    "            if episode % 5 == 0:  # 5ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«å­¦ç¿’\n",
    "                self.train_model(self.cqcnn_1, self.optimizer_1, self.replay_buffer_1, self.losses_1)\n",
    "                self.train_model(self.cqcnn_2, self.optimizer_2, self.replay_buffer_2, self.losses_2)\n",
    "            \n",
    "            episode_time = time.time() - episode_start\n",
    "            self.episode_times.append(episode_time)\n",
    "            \n",
    "            # é€²æ—å ±å‘Š\n",
    "            if (episode + 1) % 100 == 0:\n",
    "                self._print_progress(episode)\n",
    "                \n",
    "                # åæŸãƒã‚§ãƒƒã‚¯\n",
    "                converged, balance, metrics = self.convergence_detector.check_convergence(\n",
    "                    self.game_results, episode\n",
    "                )\n",
    "                \n",
    "                if converged:\n",
    "                    print(f\"\\nğŸ‰ åæŸé”æˆ! Episode {episode+1}\")\n",
    "                    print(f\"Final Balance: {balance:.4f}\")\n",
    "                    print(f\"Consecutive Good: {metrics['consecutive_good']}\")\n",
    "                    break\n",
    "\n",
    "            # ===== è¿½åŠ : é…ç½®ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å®šæœŸãƒ¢ãƒ‹ã‚¿ =====\n",
    "            if (episode + 1) % 500 == 0:\n",
    "                m = self._compute_setup_metrics()\n",
    "                print(f\"[monitor] ep={episode+1} mean={m['mean_wr']:.3f} \"\n",
    "                      f\"worst10%={m['worst_decile']:.3f} var={m['variance']:.4f} \"\n",
    "                      f\"cov_min={m['coverage_min']} cov_ok={m['coverage_ok']}\")\n",
    "                with open(self.metrics_csv_path, \"a\", encoding=\"utf-8\") as f:\n",
    "                    f.write(f\"{episode+1},{m['mean_wr']:.6f},{m['worst_decile']:.6f},\"\n",
    "                            f\"{m['variance']:.6f},{m['coverage_min']},{int(m['coverage_ok'])}\\n\")\n",
    "            # =======================================\n",
    "\n",
    "            # æ—©æœŸåæŸãƒã‚§ãƒƒã‚¯ï¼ˆ1000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ï¼‰\n",
    "            if (episode + 1) % 1000 == 0:\n",
    "                converged, balance, metrics = self.convergence_detector.check_convergence(\n",
    "                    self.game_results, episode\n",
    "                )\n",
    "                if converged:\n",
    "                    print(f\"\\nğŸ‰ åæŸé”æˆ! Episode {episode+1}\")\n",
    "                    break\n",
    "        \n",
    "        total_time = time.time() - self.start_time\n",
    "        \n",
    "        print(f\"\\n=== å®Ÿé¨“å®Œäº† ===\")\n",
    "        print(f\"Total Episodes: {len(self.game_results):,}\")\n",
    "        print(f\"Total Time: {total_time:.1f}s ({total_time/3600:.1f}h)\")\n",
    "        print(f\"Average Episode Time: {np.mean(self.episode_times):.3f}s\")\n",
    "        \n",
    "        # æœ€çµ‚åæŸãƒã‚§ãƒƒã‚¯\n",
    "        final_converged, final_balance, final_metrics = self.convergence_detector.check_convergence(\n",
    "            self.game_results, len(self.game_results)-1\n",
    "        )\n",
    "        \n",
    "        return len(self.game_results), total_time, {\n",
    "            'converged': final_converged,\n",
    "            'balance': final_balance,\n",
    "            'metrics': final_metrics,\n",
    "            'total_games': len(self.game_results)\n",
    "        }\n",
    "\n",
    "    def _print_progress(self, episode):\n",
    "        \"\"\"é€²æ—è¡¨ç¤º\"\"\"\n",
    "        recent_window = min(100, len(self.game_results))\n",
    "        recent_results = self.game_results[-recent_window:]\n",
    "        \n",
    "        wins_1 = sum(1 for r in recent_results if r['winner'] == 1)\n",
    "        wins_2 = sum(1 for r in recent_results if r['winner'] == 2)\n",
    "        draws = sum(1 for r in recent_results if r['winner'] is None)\n",
    "        \n",
    "        win_rate_1 = wins_1 / recent_window\n",
    "        win_rate_2 = wins_2 / recent_window\n",
    "        draw_rate = draws / recent_window\n",
    "        \n",
    "        balance = min(wins_1, wins_2) / max(wins_1, wins_2) if max(wins_1, wins_2) > 0 else 1.0\n",
    "        avg_turns = np.mean([r['turns'] for r in recent_results])\n",
    "        \n",
    "        current_epsilon = self.epsilon_history[-1] if self.epsilon_history else EPSILON_START\n",
    "        avg_loss_1 = np.mean(self.losses_1[-50:]) if len(self.losses_1) >= 50 else 0\n",
    "        avg_loss_2 = np.mean(self.losses_2[-50:]) if len(self.losses_2) >= 50 else 0\n",
    "        \n",
    "        elapsed_time = time.time() - self.start_time\n",
    "        \n",
    "        print(f\"Episode {episode+1:5d} | \"\n",
    "              f\"P1={win_rate_1:.3f} P2={win_rate_2:.3f} D={draw_rate:.3f} | \"\n",
    "              f\"Balance={balance:.4f} | \"\n",
    "              f\"Turns={avg_turns:.1f} | \"\n",
    "              f\"Îµ={current_epsilon:.4f} | \"\n",
    "              f\"Loss={avg_loss_1:.4f}/{avg_loss_2:.4f} | \"\n",
    "              f\"Time={elapsed_time:.0f}s\")\n",
    "\n",
    "print(\"å®Œå…¨ç‰ˆCQCNNå®Ÿé¨“ã‚¯ãƒ©ã‚¹å®šç¾©å®Œäº†\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ CQCNNé‡å­å¼·åŒ–å­¦ç¿’å®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™\n",
      "è¨­å®š: quantum_recovery_stable_config_2025-09-25.json\n",
      "æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: 50,000\n",
      "ç›®æ¨™: Balance â‰¥ 0.95, 50å›é€£ç¶šé”æˆã§åæŸ\n",
      "ã‚¬ã‚¤ã‚¹ã‚¿ãƒ¼ç’°å¢ƒåˆæœŸåŒ–å®Œäº† - å ±é…¬æˆ¦ç•¥: adaptive_anti_collapse\n",
      "CQCNNåˆæœŸåŒ–: 252D â†’ 4Q2L â†’ 36D\n",
      "Frontend: 4 Linear layers\n",
      "Quantum: 4 qubits, 2 layers, angle embedding\n",
      "Backend: 5 Linear layers\n",
      "CQCNNåˆæœŸåŒ–: 252D â†’ 4Q2L â†’ 36D\n",
      "Frontend: 4 Linear layers\n",
      "Quantum: 4 qubits, 2 layers, angle embedding\n",
      "Backend: 5 Linear layers\n",
      "åæŸæ¤œå‡ºå™¨: é–¾å€¤=0.95, patience=50, æœ€å°ã‚²ãƒ¼ãƒ æ•°=1000\n",
      "\n",
      "=== CQCNNå®Ÿé¨“åˆæœŸåŒ–å®Œäº† ===\n",
      "ãƒ¢ãƒ‡ãƒ«1ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 73,144\n",
      "ãƒ¢ãƒ‡ãƒ«2ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿: 73,144\n",
      "ãƒªãƒ—ãƒ¬ã‚¤ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: 6,000\n",
      "ãƒãƒƒãƒã‚µã‚¤ã‚º: 96\n",
      "æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: 50,000\n",
      "\n",
      "=== å®Ÿé¨“é–‹å§‹: åæŸã¾ã§æœ€å¤§50,000ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ ===\n",
      "åæŸæ¡ä»¶: Balance â‰¥ 0.95, 50å›é€£ç¶š\n",
      "Episode   100 | P1=0.880 P2=0.110 D=0.010 | Balance=0.1250 | Turns=61.0 | Îµ=0.4902 | Loss=0.0000/0.0000 | Time=19s\n",
      "Episode   200 | P1=0.810 P2=0.090 D=0.100 | Balance=0.1111 | Turns=80.0 | Îµ=0.4805 | Loss=0.0000/0.0000 | Time=43s\n",
      "Episode   300 | P1=0.840 P2=0.130 D=0.030 | Balance=0.1548 | Turns=67.1 | Îµ=0.4710 | Loss=35.0755/12.1163 | Time=67s\n",
      "Episode   400 | P1=0.820 P2=0.130 D=0.050 | Balance=0.1585 | Turns=68.8 | Îµ=0.4616 | Loss=55.9043/14.2098 | Time=88s\n",
      "Episode   500 | P1=0.840 P2=0.080 D=0.080 | Balance=0.0952 | Turns=65.7 | Îµ=0.4525 | Loss=102.3389/21.2446 | Time=109s\n",
      "Episode   600 | P1=0.700 P2=0.120 D=0.180 | Balance=0.1714 | Turns=92.7 | Îµ=0.4435 | Loss=205.5905/35.1771 | Time=133s\n",
      "Episode   700 | P1=0.680 P2=0.210 D=0.110 | Balance=0.3088 | Turns=80.0 | Îµ=0.4348 | Loss=311.4930/66.3092 | Time=157s\n",
      "Episode   800 | P1=0.800 P2=0.120 D=0.080 | Balance=0.1500 | Turns=74.5 | Îµ=0.4262 | Loss=357.9278/113.3702 | Time=182s\n",
      "Episode   900 | P1=0.800 P2=0.090 D=0.110 | Balance=0.1125 | Turns=92.2 | Îµ=0.4177 | Loss=350.7546/183.2004 | Time=210s\n",
      "Episode  1000 | P1=0.690 P2=0.200 D=0.110 | Balance=0.2899 | Turns=95.8 | Îµ=0.4094 | Loss=356.0003/257.2716 | Time=239s\n",
      "Episode  1100 | P1=0.930 P2=0.040 D=0.030 | Balance=0.0430 | Turns=67.0 | Îµ=0.4013 | Loss=339.2891/349.9188 | Time=263s\n",
      "Episode  1200 | P1=0.720 P2=0.170 D=0.110 | Balance=0.2361 | Turns=99.4 | Îµ=0.3934 | Loss=305.3844/407.0797 | Time=288s\n",
      "Episode  1300 | P1=0.580 P2=0.230 D=0.190 | Balance=0.3966 | Turns=108.0 | Îµ=0.3856 | Loss=260.1883/425.1022 | Time=314s\n",
      "Episode  1400 | P1=0.710 P2=0.110 D=0.180 | Balance=0.1549 | Turns=100.8 | Îµ=0.3780 | Loss=299.4309/372.6582 | Time=344s\n",
      "Episode  1500 | P1=0.320 P2=0.300 D=0.380 | Balance=0.9375 | Turns=145.4 | Îµ=0.3705 | Loss=311.3550/331.3585 | Time=380s\n",
      "Episode  1600 | P1=0.700 P2=0.140 D=0.160 | Balance=0.2000 | Turns=85.6 | Îµ=0.3631 | Loss=293.7011/318.9203 | Time=408s\n",
      "Episode  1700 | P1=0.550 P2=0.050 D=0.400 | Balance=0.0909 | Turns=117.6 | Îµ=0.3559 | Loss=247.0874/325.6772 | Time=440s\n",
      "Episode  1800 | P1=0.420 P2=0.240 D=0.340 | Balance=0.5714 | Turns=122.7 | Îµ=0.3489 | Loss=284.4727/332.9238 | Time=474s\n",
      "Episode  1900 | P1=0.340 P2=0.330 D=0.330 | Balance=0.9706 | Turns=132.3 | Îµ=0.3420 | Loss=367.2160/365.1043 | Time=503s\n",
      "Episode  2000 | P1=0.600 P2=0.160 D=0.240 | Balance=0.2667 | Turns=142.2 | Îµ=0.3352 | Loss=375.1163/339.7856 | Time=534s\n",
      "Episode  2100 | P1=0.440 P2=0.160 D=0.400 | Balance=0.3636 | Turns=139.2 | Îµ=0.3286 | Loss=338.4631/304.0280 | Time=565s\n",
      "Episode  2200 | P1=0.630 P2=0.060 D=0.310 | Balance=0.0952 | Turns=116.0 | Îµ=0.3221 | Loss=294.2337/297.8775 | Time=598s\n",
      "Episode  2300 | P1=0.310 P2=0.150 D=0.540 | Balance=0.4839 | Turns=152.4 | Îµ=0.3157 | Loss=285.9225/339.3861 | Time=631s\n",
      "Episode  2400 | P1=0.300 P2=0.170 D=0.530 | Balance=0.5667 | Turns=156.9 | Îµ=0.3094 | Loss=301.2171/351.1950 | Time=664s\n",
      "Episode  2500 | P1=0.700 P2=0.170 D=0.130 | Balance=0.2429 | Turns=96.2 | Îµ=0.3033 | Loss=321.8808/325.5001 | Time=690s\n",
      "Episode  2600 | P1=0.530 P2=0.120 D=0.350 | Balance=0.2264 | Turns=131.1 | Îµ=0.2973 | Loss=377.5986/324.6408 | Time=720s\n",
      "Episode  2700 | P1=0.460 P2=0.140 D=0.400 | Balance=0.3043 | Turns=136.9 | Îµ=0.2914 | Loss=426.9856/381.2336 | Time=751s\n",
      "Episode  2800 | P1=0.310 P2=0.150 D=0.540 | Balance=0.4839 | Turns=150.1 | Îµ=0.2856 | Loss=420.5974/617.3885 | Time=783s\n",
      "Episode  2900 | P1=0.560 P2=0.230 D=0.210 | Balance=0.4107 | Turns=97.4 | Îµ=0.2800 | Loss=450.3555/892.2150 | Time=810s\n",
      "Episode  3000 | P1=0.790 P2=0.130 D=0.080 | Balance=0.1646 | Turns=73.1 | Îµ=0.2744 | Loss=618.3347/1059.5914 | Time=834s\n",
      "Episode  3100 | P1=0.820 P2=0.120 D=0.060 | Balance=0.1463 | Turns=78.1 | Îµ=0.2690 | Loss=784.9222/899.8251 | Time=863s\n",
      "Episode  3200 | P1=0.810 P2=0.090 D=0.100 | Balance=0.1111 | Turns=95.1 | Îµ=0.2637 | Loss=880.0163/764.7620 | Time=896s\n",
      "Episode  3300 | P1=0.180 P2=0.130 D=0.690 | Balance=0.7222 | Turns=170.9 | Îµ=0.2585 | Loss=850.8854/676.0537 | Time=943s\n",
      "Episode  3400 | P1=0.280 P2=0.440 D=0.280 | Balance=0.6364 | Turns=122.7 | Îµ=0.2533 | Loss=747.9949/770.3604 | Time=978s\n",
      "Episode  3500 | P1=0.210 P2=0.470 D=0.320 | Balance=0.4468 | Turns=131.9 | Îµ=0.2483 | Loss=617.0609/790.7516 | Time=1011s\n",
      "Episode  3600 | P1=0.450 P2=0.440 D=0.110 | Balance=0.9778 | Turns=98.0 | Îµ=0.2434 | Loss=599.5679/798.7782 | Time=1043s\n",
      "Episode  3700 | P1=0.310 P2=0.200 D=0.490 | Balance=0.6452 | Turns=159.8 | Îµ=0.2386 | Loss=533.4281/756.0515 | Time=1084s\n",
      "Episode  3800 | P1=0.670 P2=0.190 D=0.140 | Balance=0.2836 | Turns=115.0 | Îµ=0.2339 | Loss=549.4805/762.3786 | Time=1113s\n",
      "Episode  3900 | P1=0.540 P2=0.330 D=0.130 | Balance=0.6111 | Turns=111.4 | Îµ=0.2292 | Loss=567.6911/707.8130 | Time=1150s\n",
      "Episode  4000 | P1=0.360 P2=0.440 D=0.200 | Balance=0.8182 | Turns=124.1 | Îµ=0.2247 | Loss=610.5861/775.5736 | Time=1191s\n",
      "Episode  4100 | P1=0.400 P2=0.350 D=0.250 | Balance=0.8750 | Turns=132.3 | Îµ=0.2202 | Loss=564.4658/794.8652 | Time=1232s\n",
      "Episode  4200 | P1=0.290 P2=0.250 D=0.460 | Balance=0.8621 | Turns=151.9 | Îµ=0.2159 | Loss=599.9228/692.1823 | Time=1279s\n",
      "Episode  4300 | P1=0.670 P2=0.150 D=0.180 | Balance=0.2239 | Turns=111.5 | Îµ=0.2116 | Loss=593.1891/587.3912 | Time=1318s\n",
      "Episode  4400 | P1=0.700 P2=0.210 D=0.090 | Balance=0.3000 | Turns=88.0 | Îµ=0.2074 | Loss=589.5590/478.9067 | Time=1347s\n",
      "Episode  4500 | P1=0.830 P2=0.130 D=0.040 | Balance=0.1566 | Turns=86.7 | Îµ=0.2033 | Loss=658.7451/445.3758 | Time=1373s\n",
      "Episode  4600 | P1=0.800 P2=0.110 D=0.090 | Balance=0.1375 | Turns=87.5 | Îµ=0.1993 | Loss=629.3536/395.3331 | Time=1400s\n",
      "Episode  4700 | P1=0.830 P2=0.110 D=0.060 | Balance=0.1325 | Turns=90.5 | Îµ=0.1953 | Loss=603.9421/360.3720 | Time=1429s\n",
      "Episode  4800 | P1=0.840 P2=0.070 D=0.090 | Balance=0.0833 | Turns=85.3 | Îµ=0.1915 | Loss=525.7109/312.1880 | Time=1457s\n",
      "Episode  4900 | P1=0.830 P2=0.050 D=0.120 | Balance=0.0602 | Turns=91.3 | Îµ=0.1877 | Loss=516.2178/282.7973 | Time=1486s\n",
      "Episode  5000 | P1=0.750 P2=0.150 D=0.100 | Balance=0.2000 | Turns=96.6 | Îµ=0.1840 | Loss=568.5063/274.2003 | Time=1515s\n",
      "Episode  5100 | P1=0.800 P2=0.070 D=0.130 | Balance=0.0875 | Turns=106.6 | Îµ=0.1803 | Loss=632.1545/278.9406 | Time=1546s\n",
      "Episode  5200 | P1=0.630 P2=0.150 D=0.220 | Balance=0.2381 | Turns=111.0 | Îµ=0.1767 | Loss=605.2455/287.2635 | Time=1577s\n",
      "Episode  5300 | P1=0.670 P2=0.100 D=0.230 | Balance=0.1493 | Turns=113.7 | Îµ=0.1732 | Loss=512.5884/278.7499 | Time=1608s\n",
      "Episode  5400 | P1=0.710 P2=0.100 D=0.190 | Balance=0.1408 | Turns=110.9 | Îµ=0.1698 | Loss=463.9224/279.9076 | Time=1640s\n",
      "Episode  5500 | P1=0.770 P2=0.100 D=0.130 | Balance=0.1299 | Turns=106.8 | Îµ=0.1665 | Loss=448.4272/272.8262 | Time=1670s\n",
      "Episode  5600 | P1=0.730 P2=0.150 D=0.120 | Balance=0.2055 | Turns=105.3 | Îµ=0.1632 | Loss=476.4226/267.3409 | Time=1699s\n",
      "Episode  5700 | P1=0.600 P2=0.260 D=0.140 | Balance=0.4333 | Turns=101.8 | Îµ=0.1599 | Loss=448.7426/307.3555 | Time=1726s\n",
      "Episode  5800 | P1=0.900 P2=0.030 D=0.070 | Balance=0.0333 | Turns=92.6 | Îµ=0.1568 | Loss=418.4909/314.1517 | Time=1755s\n",
      "Episode  5900 | P1=0.700 P2=0.180 D=0.120 | Balance=0.2571 | Turns=100.2 | Îµ=0.1537 | Loss=374.5375/308.9412 | Time=1784s\n",
      "Episode  6000 | P1=0.600 P2=0.250 D=0.150 | Balance=0.4167 | Turns=92.8 | Îµ=0.1506 | Loss=398.4830/305.8276 | Time=1813s\n",
      "Episode  6100 | P1=0.900 P2=0.030 D=0.070 | Balance=0.0333 | Turns=93.6 | Îµ=0.1476 | Loss=399.3799/332.2400 | Time=1842s\n",
      "Episode  6200 | P1=0.830 P2=0.110 D=0.060 | Balance=0.1325 | Turns=85.3 | Îµ=0.1447 | Loss=428.2790/301.0291 | Time=1870s\n",
      "Episode  6300 | P1=0.720 P2=0.120 D=0.160 | Balance=0.1667 | Turns=92.9 | Îµ=0.1418 | Loss=473.5592/232.9936 | Time=1897s\n",
      "Episode  6400 | P1=0.760 P2=0.130 D=0.110 | Balance=0.1711 | Turns=96.9 | Îµ=0.1390 | Loss=485.7662/241.8851 | Time=1925s\n",
      "Episode  6500 | P1=0.880 P2=0.060 D=0.060 | Balance=0.0682 | Turns=66.0 | Îµ=0.1363 | Loss=470.0366/254.9864 | Time=1951s\n",
      "Episode  6600 | P1=0.760 P2=0.090 D=0.150 | Balance=0.1184 | Turns=74.6 | Îµ=0.1336 | Loss=438.7070/224.4858 | Time=1977s\n",
      "Episode  6700 | P1=0.760 P2=0.140 D=0.100 | Balance=0.1842 | Turns=85.2 | Îµ=0.1309 | Loss=499.5823/218.0954 | Time=2002s\n",
      "Episode  6800 | P1=0.740 P2=0.140 D=0.120 | Balance=0.1892 | Turns=86.1 | Îµ=0.1283 | Loss=586.2629/227.8721 | Time=2027s\n",
      "Episode  6900 | P1=0.550 P2=0.170 D=0.280 | Balance=0.3091 | Turns=110.5 | Îµ=0.1258 | Loss=630.7920/243.9481 | Time=2055s\n",
      "Episode  7000 | P1=0.680 P2=0.120 D=0.200 | Balance=0.1765 | Turns=112.7 | Îµ=0.1233 | Loss=541.0789/273.2455 | Time=2084s\n",
      "Episode  7100 | P1=0.620 P2=0.150 D=0.230 | Balance=0.2419 | Turns=108.2 | Îµ=0.1209 | Loss=459.0595/296.0592 | Time=2112s\n",
      "Episode  7200 | P1=0.440 P2=0.320 D=0.240 | Balance=0.7273 | Turns=106.0 | Îµ=0.1185 | Loss=430.0110/325.7952 | Time=2140s\n",
      "Episode  7300 | P1=0.620 P2=0.190 D=0.190 | Balance=0.3065 | Turns=98.2 | Îµ=0.1161 | Loss=406.5311/362.5242 | Time=2167s\n",
      "Episode  7400 | P1=0.570 P2=0.240 D=0.190 | Balance=0.4211 | Turns=101.9 | Îµ=0.1138 | Loss=390.6642/329.5376 | Time=2194s\n",
      "Episode  7500 | P1=0.380 P2=0.350 D=0.270 | Balance=0.9211 | Turns=112.8 | Îµ=0.1116 | Loss=370.6828/338.6355 | Time=2223s\n",
      "Episode  7600 | P1=0.490 P2=0.320 D=0.190 | Balance=0.6531 | Turns=113.2 | Îµ=0.1094 | Loss=374.0803/398.4359 | Time=2252s\n",
      "Episode  7700 | P1=0.660 P2=0.130 D=0.210 | Balance=0.1970 | Turns=111.2 | Îµ=0.1072 | Loss=371.5349/422.1767 | Time=2281s\n",
      "Episode  7800 | P1=0.480 P2=0.250 D=0.270 | Balance=0.5208 | Turns=121.4 | Îµ=0.1051 | Loss=360.1029/442.7446 | Time=2312s\n",
      "Episode  7900 | P1=0.700 P2=0.160 D=0.140 | Balance=0.2286 | Turns=95.3 | Îµ=0.1030 | Loss=334.5951/430.7141 | Time=2338s\n",
      "Episode  8000 | P1=0.450 P2=0.100 D=0.450 | Balance=0.2222 | Turns=135.4 | Îµ=0.1010 | Loss=330.3007/470.4318 | Time=2371s\n",
      "Episode  8100 | P1=0.630 P2=0.100 D=0.270 | Balance=0.1587 | Turns=106.8 | Îµ=0.0990 | Loss=367.5489/445.7218 | Time=2400s\n",
      "Episode  8200 | P1=0.770 P2=0.020 D=0.210 | Balance=0.0260 | Turns=85.0 | Îµ=0.0970 | Loss=405.6118/412.4987 | Time=2426s\n",
      "Episode  8300 | P1=0.750 P2=0.090 D=0.160 | Balance=0.1200 | Turns=90.5 | Îµ=0.0951 | Loss=412.0347/383.8191 | Time=2453s\n",
      "Episode  8400 | P1=0.650 P2=0.050 D=0.300 | Balance=0.0769 | Turns=111.9 | Îµ=0.0932 | Loss=415.9913/394.2260 | Time=2482s\n",
      "Episode  8500 | P1=0.390 P2=0.220 D=0.390 | Balance=0.5641 | Turns=127.6 | Îµ=0.0913 | Loss=394.9622/386.2768 | Time=2514s\n",
      "Episode  8600 | P1=0.580 P2=0.140 D=0.280 | Balance=0.2414 | Turns=113.9 | Îµ=0.0895 | Loss=408.0481/363.1026 | Time=2544s\n",
      "Episode  8700 | P1=0.560 P2=0.180 D=0.260 | Balance=0.3214 | Turns=127.4 | Îµ=0.0878 | Loss=366.0834/332.3520 | Time=2579s\n",
      "Episode  8800 | P1=0.560 P2=0.090 D=0.350 | Balance=0.1607 | Turns=120.2 | Îµ=0.0860 | Loss=358.1009/296.8960 | Time=2616s\n",
      "Episode  8900 | P1=0.700 P2=0.030 D=0.270 | Balance=0.0429 | Turns=103.2 | Îµ=0.0843 | Loss=342.7088/300.2738 | Time=2650s\n",
      "Episode  9000 | P1=0.550 P2=0.060 D=0.390 | Balance=0.1091 | Turns=124.9 | Îµ=0.0827 | Loss=359.7725/304.5362 | Time=2683s\n",
      "Episode  9100 | P1=0.190 P2=0.140 D=0.670 | Balance=0.7368 | Turns=158.8 | Îµ=0.0810 | Loss=319.0729/304.4303 | Time=2720s\n",
      "Episode  9200 | P1=0.580 P2=0.120 D=0.300 | Balance=0.2069 | Turns=81.3 | Îµ=0.0800 | Loss=333.9392/301.5027 | Time=2746s\n",
      "Episode  9300 | P1=0.430 P2=0.280 D=0.290 | Balance=0.6512 | Turns=106.7 | Îµ=0.0800 | Loss=356.0906/309.6422 | Time=2775s\n",
      "Episode  9400 | P1=0.580 P2=0.160 D=0.260 | Balance=0.2759 | Turns=109.0 | Îµ=0.0800 | Loss=375.1143/324.9014 | Time=2804s\n",
      "Episode  9500 | P1=0.520 P2=0.100 D=0.380 | Balance=0.1923 | Turns=125.5 | Îµ=0.0800 | Loss=302.6699/304.3823 | Time=2836s\n",
      "Episode  9600 | P1=0.470 P2=0.110 D=0.420 | Balance=0.2340 | Turns=116.1 | Îµ=0.0800 | Loss=285.9058/293.4227 | Time=2866s\n",
      "Episode  9700 | P1=0.680 P2=0.040 D=0.280 | Balance=0.0588 | Turns=99.7 | Îµ=0.0800 | Loss=314.2758/268.2908 | Time=2893s\n",
      "Episode  9800 | P1=0.610 P2=0.070 D=0.320 | Balance=0.1148 | Turns=109.0 | Îµ=0.0800 | Loss=292.8883/251.6879 | Time=2922s\n",
      "Episode  9900 | P1=0.530 P2=0.120 D=0.350 | Balance=0.2264 | Turns=109.5 | Îµ=0.0800 | Loss=302.4572/238.4948 | Time=2952s\n",
      "Episode 10000 | P1=0.620 P2=0.100 D=0.280 | Balance=0.1613 | Turns=109.7 | Îµ=0.0800 | Loss=298.9504/221.4625 | Time=2981s\n",
      "Episode 10100 | P1=0.490 P2=0.180 D=0.330 | Balance=0.3673 | Turns=113.4 | Îµ=0.0800 | Loss=294.5871/242.0606 | Time=3010s\n",
      "Episode 10200 | P1=0.620 P2=0.130 D=0.250 | Balance=0.2097 | Turns=102.4 | Îµ=0.0800 | Loss=288.2414/266.4143 | Time=3038s\n",
      "Episode 10300 | P1=0.620 P2=0.070 D=0.310 | Balance=0.1129 | Turns=113.7 | Îµ=0.0800 | Loss=273.9738/278.1195 | Time=3068s\n",
      "Episode 10400 | P1=0.580 P2=0.200 D=0.220 | Balance=0.3448 | Turns=110.5 | Îµ=0.0800 | Loss=260.5242/281.5225 | Time=3097s\n",
      "Episode 10500 | P1=0.540 P2=0.160 D=0.300 | Balance=0.2963 | Turns=111.4 | Îµ=0.0800 | Loss=227.8916/257.5013 | Time=3127s\n",
      "Episode 10600 | P1=0.550 P2=0.210 D=0.240 | Balance=0.3818 | Turns=99.8 | Îµ=0.0800 | Loss=226.4915/253.7593 | Time=3155s\n",
      "Episode 10700 | P1=0.630 P2=0.050 D=0.320 | Balance=0.0794 | Turns=98.0 | Îµ=0.0800 | Loss=278.4111/242.0193 | Time=3183s\n",
      "Episode 10800 | P1=0.350 P2=0.100 D=0.550 | Balance=0.2857 | Turns=134.6 | Îµ=0.0800 | Loss=294.1348/271.7439 | Time=3216s\n",
      "Episode 10900 | P1=0.170 P2=0.100 D=0.730 | Balance=0.5882 | Turns=177.3 | Îµ=0.0800 | Loss=263.6453/256.0552 | Time=3255s\n",
      "Episode 11000 | P1=0.280 P2=0.180 D=0.540 | Balance=0.6429 | Turns=141.3 | Îµ=0.0800 | Loss=224.5654/261.3697 | Time=3288s\n",
      "Episode 11100 | P1=0.500 P2=0.200 D=0.300 | Balance=0.4000 | Turns=115.9 | Îµ=0.0800 | Loss=222.2037/250.6971 | Time=3318s\n",
      "Episode 11200 | P1=0.620 P2=0.170 D=0.210 | Balance=0.2742 | Turns=128.7 | Îµ=0.0800 | Loss=235.1091/227.3287 | Time=3350s\n",
      "Episode 11300 | P1=0.470 P2=0.260 D=0.270 | Balance=0.5532 | Turns=126.3 | Îµ=0.0800 | Loss=223.7979/204.4275 | Time=3382s\n",
      "Episode 11400 | P1=0.450 P2=0.240 D=0.310 | Balance=0.5333 | Turns=132.4 | Îµ=0.0800 | Loss=199.1101/189.3758 | Time=3414s\n",
      "Episode 11500 | P1=0.370 P2=0.450 D=0.180 | Balance=0.8222 | Turns=106.8 | Îµ=0.0800 | Loss=187.7260/208.6077 | Time=3443s\n",
      "Episode 11600 | P1=0.060 P2=0.270 D=0.670 | Balance=0.2222 | Turns=153.9 | Îµ=0.0800 | Loss=175.9785/259.2891 | Time=3478s\n",
      "Episode 11700 | P1=0.280 P2=0.300 D=0.420 | Balance=0.9333 | Turns=141.5 | Îµ=0.0800 | Loss=171.5336/283.7819 | Time=3512s\n",
      "Episode 11800 | P1=0.550 P2=0.050 D=0.400 | Balance=0.0909 | Turns=133.4 | Îµ=0.0800 | Loss=160.7366/314.6775 | Time=3544s\n",
      "Episode 11900 | P1=0.330 P2=0.200 D=0.470 | Balance=0.6061 | Turns=135.4 | Îµ=0.0800 | Loss=186.5730/278.9390 | Time=3577s\n",
      "Episode 12000 | P1=0.340 P2=0.270 D=0.390 | Balance=0.7941 | Turns=126.6 | Îµ=0.0800 | Loss=208.0267/281.6587 | Time=3609s\n",
      "Episode 12100 | P1=0.420 P2=0.220 D=0.360 | Balance=0.5238 | Turns=121.4 | Îµ=0.0800 | Loss=219.0570/293.2469 | Time=3640s\n",
      "Episode 12200 | P1=0.410 P2=0.230 D=0.360 | Balance=0.5610 | Turns=128.6 | Îµ=0.0800 | Loss=208.1052/348.8121 | Time=3672s\n",
      "Episode 12300 | P1=0.350 P2=0.150 D=0.500 | Balance=0.4286 | Turns=156.5 | Îµ=0.0800 | Loss=186.2964/315.0135 | Time=3708s\n",
      "Episode 12400 | P1=0.430 P2=0.410 D=0.160 | Balance=0.9535 | Turns=116.1 | Îµ=0.0800 | Loss=180.8279/296.1389 | Time=3739s\n",
      "Episode 12500 | P1=0.180 P2=0.260 D=0.560 | Balance=0.6923 | Turns=160.3 | Îµ=0.0800 | Loss=185.7224/256.1189 | Time=3775s\n",
      "Episode 12600 | P1=0.580 P2=0.040 D=0.380 | Balance=0.0690 | Turns=119.1 | Îµ=0.0800 | Loss=210.5045/275.3876 | Time=3806s\n",
      "Episode 12700 | P1=0.410 P2=0.220 D=0.370 | Balance=0.5366 | Turns=124.6 | Îµ=0.0800 | Loss=230.6759/263.3101 | Time=3838s\n",
      "Episode 12800 | P1=0.150 P2=0.560 D=0.290 | Balance=0.2679 | Turns=113.4 | Îµ=0.0800 | Loss=247.7779/300.4092 | Time=3867s\n",
      "Episode 12900 | P1=0.600 P2=0.050 D=0.350 | Balance=0.0833 | Turns=114.2 | Îµ=0.0800 | Loss=250.7367/327.5404 | Time=3897s\n",
      "Episode 13000 | P1=0.600 P2=0.040 D=0.360 | Balance=0.0667 | Turns=115.5 | Îµ=0.0800 | Loss=246.1744/292.9104 | Time=3927s\n",
      "Episode 13100 | P1=0.690 P2=0.060 D=0.250 | Balance=0.0870 | Turns=99.0 | Îµ=0.0800 | Loss=271.9418/242.7885 | Time=3955s\n",
      "Episode 13200 | P1=0.390 P2=0.170 D=0.440 | Balance=0.4359 | Turns=129.2 | Îµ=0.0800 | Loss=320.0707/210.2983 | Time=3987s\n",
      "Episode 13300 | P1=0.600 P2=0.060 D=0.340 | Balance=0.1000 | Turns=115.2 | Îµ=0.0800 | Loss=332.6754/238.8662 | Time=4018s\n",
      "Episode 13400 | P1=0.240 P2=0.130 D=0.630 | Balance=0.5417 | Turns=142.9 | Îµ=0.0800 | Loss=292.1227/263.4283 | Time=4052s\n",
      "Episode 13500 | P1=0.260 P2=0.030 D=0.710 | Balance=0.1154 | Turns=163.9 | Îµ=0.0800 | Loss=238.8031/269.2437 | Time=4090s\n",
      "Episode 13600 | P1=0.530 P2=0.140 D=0.330 | Balance=0.2642 | Turns=115.4 | Îµ=0.0800 | Loss=223.4304/254.5167 | Time=4120s\n",
      "Episode 13700 | P1=0.220 P2=0.180 D=0.600 | Balance=0.8182 | Turns=150.5 | Îµ=0.0800 | Loss=256.1941/241.8096 | Time=4156s\n",
      "Episode 13800 | P1=0.310 P2=0.120 D=0.570 | Balance=0.3871 | Turns=151.7 | Îµ=0.0800 | Loss=238.4272/238.4680 | Time=4191s\n",
      "Episode 13900 | P1=0.610 P2=0.220 D=0.170 | Balance=0.3607 | Turns=92.2 | Îµ=0.0800 | Loss=194.6979/245.5611 | Time=4217s\n",
      "Episode 14000 | P1=0.300 P2=0.350 D=0.350 | Balance=0.8571 | Turns=117.5 | Îµ=0.0800 | Loss=200.9231/249.0570 | Time=4247s\n",
      "Episode 14100 | P1=0.160 P2=0.160 D=0.680 | Balance=1.0000 | Turns=163.5 | Îµ=0.0800 | Loss=205.6352/277.9587 | Time=4284s\n",
      "Episode 14200 | P1=0.730 P2=0.120 D=0.150 | Balance=0.1644 | Turns=96.2 | Îµ=0.0800 | Loss=166.3009/274.5987 | Time=4312s\n",
      "Episode 14300 | P1=0.860 P2=0.040 D=0.100 | Balance=0.0465 | Turns=71.3 | Îµ=0.0800 | Loss=186.6192/223.4864 | Time=4339s\n",
      "Episode 14400 | P1=0.350 P2=0.180 D=0.470 | Balance=0.5143 | Turns=125.6 | Îµ=0.0800 | Loss=231.0745/203.3016 | Time=4375s\n",
      "Episode 14500 | P1=0.260 P2=0.130 D=0.610 | Balance=0.5000 | Turns=151.0 | Îµ=0.0800 | Loss=230.8692/218.4137 | Time=4415s\n",
      "Episode 14600 | P1=0.330 P2=0.050 D=0.620 | Balance=0.1515 | Turns=148.4 | Îµ=0.0800 | Loss=228.6930/239.9400 | Time=4455s\n",
      "Episode 14700 | P1=0.200 P2=0.160 D=0.640 | Balance=0.8000 | Turns=159.0 | Îµ=0.0800 | Loss=201.2874/219.3980 | Time=4494s\n",
      "Episode 14800 | P1=0.460 P2=0.340 D=0.200 | Balance=0.7391 | Turns=105.0 | Îµ=0.0800 | Loss=191.4315/215.2381 | Time=4527s\n",
      "Episode 14900 | P1=0.470 P2=0.230 D=0.300 | Balance=0.4894 | Turns=125.1 | Îµ=0.0800 | Loss=169.6380/231.0699 | Time=4560s\n",
      "Episode 15000 | P1=0.160 P2=0.170 D=0.670 | Balance=0.9412 | Turns=164.8 | Îµ=0.0800 | Loss=169.4978/237.6980 | Time=4601s\n",
      "Episode 15100 | P1=0.540 P2=0.040 D=0.420 | Balance=0.0741 | Turns=129.8 | Îµ=0.0800 | Loss=171.7599/201.4089 | Time=4638s\n",
      "Episode 15200 | P1=0.490 P2=0.050 D=0.460 | Balance=0.1020 | Turns=143.1 | Îµ=0.0800 | Loss=184.0576/186.4913 | Time=4677s\n",
      "Episode 15300 | P1=0.440 P2=0.280 D=0.280 | Balance=0.6364 | Turns=111.1 | Îµ=0.0800 | Loss=218.4528/193.0050 | Time=4711s\n",
      "Episode 15400 | P1=0.180 P2=0.360 D=0.460 | Balance=0.5000 | Turns=139.1 | Îµ=0.0800 | Loss=217.6492/230.9813 | Time=4746s\n",
      "Episode 15500 | P1=0.490 P2=0.170 D=0.340 | Balance=0.3469 | Turns=107.1 | Îµ=0.0800 | Loss=183.8355/227.0125 | Time=4778s\n",
      "Episode 15600 | P1=0.520 P2=0.230 D=0.250 | Balance=0.4423 | Turns=106.4 | Îµ=0.0800 | Loss=227.4981/185.5869 | Time=4808s\n",
      "Episode 15700 | P1=0.320 P2=0.220 D=0.460 | Balance=0.6875 | Turns=125.2 | Îµ=0.0800 | Loss=235.0759/188.0107 | Time=4841s\n",
      "Episode 15800 | P1=0.570 P2=0.220 D=0.210 | Balance=0.3860 | Turns=89.3 | Îµ=0.0800 | Loss=279.6603/199.4941 | Time=4867s\n",
      "Episode 15900 | P1=0.510 P2=0.120 D=0.370 | Balance=0.2353 | Turns=120.4 | Îµ=0.0800 | Loss=251.1490/194.9154 | Time=4898s\n",
      "Episode 16000 | P1=0.410 P2=0.230 D=0.360 | Balance=0.5610 | Turns=109.9 | Îµ=0.0800 | Loss=219.9064/181.6289 | Time=4931s\n",
      "Episode 16100 | P1=0.430 P2=0.190 D=0.380 | Balance=0.4419 | Turns=121.0 | Îµ=0.0800 | Loss=208.3104/227.7351 | Time=4967s\n",
      "Episode 16200 | P1=0.460 P2=0.160 D=0.380 | Balance=0.3478 | Turns=127.7 | Îµ=0.0800 | Loss=240.1742/244.3640 | Time=5004s\n",
      "Episode 16300 | P1=0.410 P2=0.130 D=0.460 | Balance=0.3171 | Turns=127.5 | Îµ=0.0800 | Loss=218.0495/218.0469 | Time=5037s\n",
      "Episode 16400 | P1=0.670 P2=0.050 D=0.280 | Balance=0.0746 | Turns=93.4 | Îµ=0.0800 | Loss=209.1562/191.4666 | Time=5069s\n",
      "Episode 16500 | P1=0.290 P2=0.250 D=0.460 | Balance=0.8621 | Turns=135.5 | Îµ=0.0800 | Loss=246.5312/194.8606 | Time=5107s\n",
      "Episode 16600 | P1=0.430 P2=0.210 D=0.360 | Balance=0.4884 | Turns=124.3 | Îµ=0.0800 | Loss=232.4076/220.6081 | Time=5141s\n",
      "Episode 16700 | P1=0.540 P2=0.030 D=0.430 | Balance=0.0556 | Turns=121.5 | Îµ=0.0800 | Loss=218.3608/210.4603 | Time=5177s\n",
      "Episode 16800 | P1=0.370 P2=0.020 D=0.610 | Balance=0.0541 | Turns=146.7 | Îµ=0.0800 | Loss=200.1621/190.8722 | Time=5217s\n",
      "Episode 16900 | P1=0.430 P2=0.010 D=0.560 | Balance=0.0233 | Turns=139.0 | Îµ=0.0800 | Loss=220.1951/181.3041 | Time=5254s\n",
      "Episode 17000 | P1=0.230 P2=0.120 D=0.650 | Balance=0.5217 | Turns=162.5 | Îµ=0.0800 | Loss=194.6517/172.1057 | Time=5292s\n",
      "Episode 17100 | P1=0.530 P2=0.010 D=0.460 | Balance=0.0189 | Turns=132.1 | Îµ=0.0800 | Loss=192.2499/167.6677 | Time=5326s\n",
      "Episode 17200 | P1=0.310 P2=0.130 D=0.560 | Balance=0.4194 | Turns=139.6 | Îµ=0.0800 | Loss=183.3975/159.6719 | Time=5365s\n",
      "Episode 17300 | P1=0.450 P2=0.200 D=0.350 | Balance=0.4444 | Turns=112.8 | Îµ=0.0800 | Loss=189.7092/196.5381 | Time=5396s\n",
      "Episode 17400 | P1=0.530 P2=0.040 D=0.430 | Balance=0.0755 | Turns=126.6 | Îµ=0.0800 | Loss=218.1190/215.5429 | Time=5427s\n",
      "Episode 17500 | P1=0.480 P2=0.200 D=0.320 | Balance=0.4167 | Turns=128.6 | Îµ=0.0800 | Loss=210.4396/242.7329 | Time=5459s\n",
      "Episode 17600 | P1=0.500 P2=0.120 D=0.380 | Balance=0.2400 | Turns=123.6 | Îµ=0.0800 | Loss=194.7143/212.5739 | Time=5495s\n",
      "Episode 17700 | P1=0.420 P2=0.340 D=0.240 | Balance=0.8095 | Turns=93.5 | Îµ=0.0800 | Loss=208.0517/201.3223 | Time=5521s\n",
      "Episode 17800 | P1=0.330 P2=0.360 D=0.310 | Balance=0.9167 | Turns=97.8 | Îµ=0.0800 | Loss=207.8701/244.0807 | Time=5553s\n",
      "Episode 17900 | P1=0.630 P2=0.170 D=0.200 | Balance=0.2698 | Turns=87.2 | Îµ=0.0800 | Loss=228.2880/253.2752 | Time=5583s\n",
      "Episode 18000 | P1=0.300 P2=0.320 D=0.380 | Balance=0.9375 | Turns=104.9 | Îµ=0.0800 | Loss=233.7724/204.3596 | Time=5612s\n",
      "Episode 18100 | P1=0.700 P2=0.080 D=0.220 | Balance=0.1143 | Turns=82.7 | Îµ=0.0800 | Loss=265.7210/180.8959 | Time=5637s\n",
      "Episode 18200 | P1=0.280 P2=0.430 D=0.290 | Balance=0.6512 | Turns=103.4 | Îµ=0.0800 | Loss=242.2714/251.6451 | Time=5670s\n",
      "Episode 18300 | P1=0.180 P2=0.110 D=0.710 | Balance=0.6111 | Turns=155.1 | Îµ=0.0800 | Loss=209.3012/333.0512 | Time=5711s\n",
      "Episode 18400 | P1=0.670 P2=0.070 D=0.260 | Balance=0.1045 | Turns=93.7 | Îµ=0.0800 | Loss=173.9161/325.4315 | Time=5742s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m experiment = CQCNNExperiment(config)\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# å®Ÿé¨“å®Ÿè¡Œ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m final_episode, training_time, analysis = \u001b[43mexperiment\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… å®Ÿé¨“å®Œäº†!\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 189\u001b[39m, in \u001b[36mCQCNNExperiment.run_experiment\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m episode % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m:  \u001b[38;5;66;03m# 5ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã”ã¨ã«å­¦ç¿’\u001b[39;00m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m.train_model(\u001b[38;5;28mself\u001b[39m.cqcnn_1, \u001b[38;5;28mself\u001b[39m.optimizer_1, \u001b[38;5;28mself\u001b[39m.replay_buffer_1, \u001b[38;5;28mself\u001b[39m.losses_1)\n\u001b[32m--> \u001b[39m\u001b[32m189\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcqcnn_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreplay_buffer_2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlosses_2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    191\u001b[39m episode_time = time.time() - episode_start\n\u001b[32m    192\u001b[39m \u001b[38;5;28mself\u001b[39m.episode_times.append(episode_time)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 157\u001b[39m, in \u001b[36mCQCNNExperiment.train_model\u001b[39m\u001b[34m(self, model, optimizer, replay_buffer, losses_list)\u001b[39m\n\u001b[32m    154\u001b[39m dones = torch.BoolTensor([exp[\u001b[32m4\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m exp \u001b[38;5;129;01min\u001b[39;00m batch])\n\u001b[32m    156\u001b[39m \u001b[38;5;66;03m# ç¾åœ¨ã®Qå€¤\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m157\u001b[39m current_q_values = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstates\u001b[49m\u001b[43m)\u001b[49m.gather(\u001b[32m1\u001b[39m, actions.unsqueeze(\u001b[32m1\u001b[39m)).squeeze(\u001b[32m1\u001b[39m)\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# ã‚¿ãƒ¼ã‚²ãƒƒãƒˆQå€¤ï¼ˆç°¡å˜ãªTDå­¦ç¿’ï¼‰\u001b[39;00m\n\u001b[32m    160\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 95\u001b[39m, in \u001b[36mCQCNN.forward\u001b[39m\u001b[34m(self, state)\u001b[39m\n\u001b[32m     93\u001b[39m quantum_outputs = []\n\u001b[32m     94\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(batch_size):\n\u001b[32m---> \u001b[39m\u001b[32m95\u001b[39m     q_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantum_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfrontend_out\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mquantum_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     96\u001b[39m     quantum_outputs.append(torch.stack(q_out))\n\u001b[32m     98\u001b[39m quantum_out = torch.stack(quantum_outputs)  \u001b[38;5;66;03m# (batch, 4)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:922\u001b[39m, in \u001b[36mQNode.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    919\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_capture_qnode\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m capture_qnode  \u001b[38;5;66;03m# pylint: disable=import-outside-toplevel\u001b[39;00m\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m capture_qnode(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m922\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_impl_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\qnode.py:895\u001b[39m, in \u001b[36mQNode._impl_call\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    892\u001b[39m \u001b[38;5;66;03m# Calculate the classical jacobians if necessary\u001b[39;00m\n\u001b[32m    893\u001b[39m \u001b[38;5;28mself\u001b[39m._transform_program.set_classical_component(\u001b[38;5;28mself\u001b[39m, args, kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m895\u001b[39m res = \u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    896\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    897\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdiff_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43minterface\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minterface\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtransform_program\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform_program\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgradient_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    904\u001b[39m res = res[\u001b[32m0\u001b[39m]\n\u001b[32m    906\u001b[39m \u001b[38;5;66;03m# convert result to the interface in case the qfunc has no parameters\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\execution.py:233\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(tapes, device, diff_method, interface, grad_on_execution, cache, cachesize, max_diff, device_vjp, postselect_mode, mcm_method, gradient_kwargs, transform_program, executor_backend)\u001b[39m\n\u001b[32m    229\u001b[39m tapes, outer_post_processing = outer_transform(tapes)\n\u001b[32m    231\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m outer_transform.is_informative, \u001b[33m\"\u001b[39m\u001b[33mshould only contain device preprocessing\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m233\u001b[39m results = \u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minner_transform\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m user_post_processing(outer_post_processing(results))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\run.py:338\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(tapes, device, config, inner_transform_program)\u001b[39m\n\u001b[32m    335\u001b[39m         params = tape.get_parameters(trainable_only=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    336\u001b[39m         tape.trainable_params = qml.math.get_trainable_indices(params)\n\u001b[32m--> \u001b[39m\u001b[32m338\u001b[39m results = \u001b[43mml_execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecute_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjpc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\interfaces\\torch.py:240\u001b[39m, in \u001b[36mexecute\u001b[39m\u001b[34m(tapes, execute_fn, jpc, device)\u001b[39m\n\u001b[32m    232\u001b[39m     parameters.extend(tape.get_parameters())\n\u001b[32m    234\u001b[39m kwargs = {\n\u001b[32m    235\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtapes\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtuple\u001b[39m(tapes),\n\u001b[32m    236\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mexecute_fn\u001b[39m\u001b[33m\"\u001b[39m: execute_fn,\n\u001b[32m    237\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mjpc\u001b[39m\u001b[33m\"\u001b[39m: jpc,\n\u001b[32m    238\u001b[39m }\n\u001b[32m--> \u001b[39m\u001b[32m240\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mExecuteTapes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\interfaces\\torch.py:89\u001b[39m, in \u001b[36mpytreeify.<locals>.new_apply\u001b[39m\u001b[34m(*inp)\u001b[39m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_apply\u001b[39m(*inp):\n\u001b[32m     87\u001b[39m     \u001b[38;5;66;03m# Inputs already flat\u001b[39;00m\n\u001b[32m     88\u001b[39m     out_struct_holder = []\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m     flat_out = \u001b[43morig_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_struct_holder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pytree.tree_unflatten(flat_out, out_struct_holder[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\torch\\autograd\\function.py:576\u001b[39m, in \u001b[36mFunction.apply\u001b[39m\u001b[34m(cls, *args, **kwargs)\u001b[39m\n\u001b[32m    573\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch._C._are_functorch_transforms_active():\n\u001b[32m    574\u001b[39m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[32m    575\u001b[39m     args = _functorch.utils.unwrap_dead_wrappers(args)\n\u001b[32m--> \u001b[39m\u001b[32m576\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_setup_ctx_defined:\n\u001b[32m    579\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    580\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    581\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    582\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mstaticmethod. For more details, please see \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    583\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mhttps://pytorch.org/docs/main/notes/extending.func.html\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    584\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\interfaces\\torch.py:93\u001b[39m, in \u001b[36mpytreeify.<locals>.new_forward\u001b[39m\u001b[34m(ctx, out_struct_holder, *inp)\u001b[39m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnew_forward\u001b[39m(ctx, out_struct_holder, *inp):\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m     out = \u001b[43morig_fw\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43minp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     94\u001b[39m     flat_out, out_struct = pytree.tree_flatten(out)\n\u001b[32m     95\u001b[39m     ctx._out_struct = out_struct\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\interfaces\\torch.py:162\u001b[39m, in \u001b[36mExecuteTapes.forward\u001b[39m\u001b[34m(ctx, kwargs, *parameters)\u001b[39m\n\u001b[32m    159\u001b[39m ctx.tapes = kwargs[\u001b[33m\"\u001b[39m\u001b[33mtapes\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    160\u001b[39m ctx.jpc = kwargs[\u001b[33m\"\u001b[39m\u001b[33mjpc\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m162\u001b[39m res = \u001b[38;5;28mtuple\u001b[39m(\u001b[43mkwargs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexecute_fn\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    164\u001b[39m \u001b[38;5;66;03m# if any input tensor uses the GPU, the output should as well\u001b[39;00m\n\u001b[32m    165\u001b[39m ctx.torch_device = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\jacobian_products.py:487\u001b[39m, in \u001b[36mDeviceDerivatives.execute_and_cache_jacobian\u001b[39m\u001b[34m(self, tapes)\u001b[39m\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logger.isEnabledFor(logging.DEBUG):  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    486\u001b[39m     logger.debug(\u001b[33m\"\u001b[39m\u001b[33mForward pass called with \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, tapes)\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m results, jac = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dev_execute_and_compute_derivatives\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._results_cache[tapes] = results\n\u001b[32m    489\u001b[39m \u001b[38;5;28mself\u001b[39m._jacs_cache[tapes] = jac\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\workflow\\jacobian_products.py:451\u001b[39m, in \u001b[36mDeviceDerivatives._dev_execute_and_compute_derivatives\u001b[39m\u001b[34m(self, tapes)\u001b[39m\n\u001b[32m    445\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_dev_execute_and_compute_derivatives\u001b[39m(\u001b[38;5;28mself\u001b[39m, tapes: QuantumScriptBatch):\n\u001b[32m    446\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    447\u001b[39m \u001b[33;03m    Converts tapes to numpy before computing the the results and derivatives on the device.\u001b[39;00m\n\u001b[32m    448\u001b[39m \n\u001b[32m    449\u001b[39m \u001b[33;03m    Dispatches between the two different device interfaces.\u001b[39;00m\n\u001b[32m    450\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m451\u001b[39m     numpy_tapes, _ = \u001b[43mqml\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransforms\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_numpy_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtapes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    452\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._device.execute_and_compute_derivatives(numpy_tapes, \u001b[38;5;28mself\u001b[39m._execution_config)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\core\\transform_dispatcher.py:281\u001b[39m, in \u001b[36mTransformDispatcher.__call__\u001b[39m\u001b[34m(self, *targs, **tkwargs)\u001b[39m\n\u001b[32m    278\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._qfunc_transform(obj, targs, tkwargs)\n\u001b[32m    280\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj, Sequence) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(q, qml.tape.QuantumScript) \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m obj):\n\u001b[32m--> \u001b[39m\u001b[32m281\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_batch_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    283\u001b[39m \u001b[38;5;66;03m# Input is not a QNode nor a quantum tape nor a device.\u001b[39;00m\n\u001b[32m    284\u001b[39m \u001b[38;5;66;03m# Assume Python decorator syntax:\u001b[39;00m\n\u001b[32m    285\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    286\u001b[39m \u001b[38;5;66;03m# result = some_transform(*transform_args)(qnode)(*qnode_args)\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m TransformError(\n\u001b[32m    289\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mDecorating a QNode with @transform_fn(**transform_kwargs) has been \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    290\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mremoved. Please decorate with @functools.partial(transform_fn, **transform_kwargs) \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    293\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhttps://docs.pennylane.ai/en/stable/development/deprecations.html#completed-deprecation-cycles\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    294\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\core\\transform_dispatcher.py:489\u001b[39m, in \u001b[36mTransformDispatcher._batch_transform\u001b[39m\u001b[34m(self, original_batch, targs, tkwargs)\u001b[39m\n\u001b[32m    483\u001b[39m tape_counts = []\n\u001b[32m    485\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m original_batch:\n\u001b[32m    486\u001b[39m     \u001b[38;5;66;03m# Preprocess the tapes by applying transforms\u001b[39;00m\n\u001b[32m    487\u001b[39m     \u001b[38;5;66;03m# to each tape, and storing corresponding tapes\u001b[39;00m\n\u001b[32m    488\u001b[39m     \u001b[38;5;66;03m# for execution, processing functions, and list of tape lengths.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m489\u001b[39m     new_tapes, fn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    490\u001b[39m     execution_tapes.extend(new_tapes)\n\u001b[32m    491\u001b[39m     batch_fns.append(fn)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\core\\transform_dispatcher.py:253\u001b[39m, in \u001b[36mTransformDispatcher.__call__\u001b[39m\u001b[34m(self, *targs, **tkwargs)\u001b[39m\n\u001b[32m    250\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m expand_processing(processed_results)\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m253\u001b[39m     transformed_tapes, processing_fn = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mtargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mtkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_informative:\n\u001b[32m    256\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m processing_fn(transformed_tapes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\convert_to_numpy_parameters.py:84\u001b[39m, in \u001b[36mconvert_to_numpy_parameters\u001b[39m\u001b[34m(tape)\u001b[39m\n\u001b[32m     82\u001b[39m new_ops = (_convert_op_to_numpy_data(op) \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m tape.operations)\n\u001b[32m     83\u001b[39m new_measurements = (_convert_measurement_to_numpy_data(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m tape.measurements)\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m new_circuit = \u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnew_ops\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_measurements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshots\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshots\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainable_params\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtape\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrainable_params\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnull_postprocessing\u001b[39m(results):\n\u001b[32m     89\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"A postprocesing function returned by a transform that only converts the batch of results\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[33;03m    into a result for a single ``QuantumTape``.\u001b[39;00m\n\u001b[32m     91\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\tape\\qscript.py:194\u001b[39m, in \u001b[36mQuantumScript.__init__\u001b[39m\u001b[34m(self, ops, measurements, shots, trainable_params)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\n\u001b[32m    188\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    189\u001b[39m     ops: Optional[Iterable[Operator]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    192\u001b[39m     trainable_params: Optional[Sequence[\u001b[38;5;28mint\u001b[39m]] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    193\u001b[39m ):\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m     \u001b[38;5;28mself\u001b[39m._ops = [] \u001b[38;5;28;01mif\u001b[39;00m ops \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mops\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m     \u001b[38;5;28mself\u001b[39m._measurements = [] \u001b[38;5;28;01mif\u001b[39;00m measurements \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(measurements)\n\u001b[32m    196\u001b[39m     \u001b[38;5;28mself\u001b[39m._shots = Shots(shots)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\convert_to_numpy_parameters.py:82\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m     47\u001b[39m \u001b[38;5;129m@transform\u001b[39m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_numpy_parameters\u001b[39m(tape: QuantumScript) -> \u001b[38;5;28mtuple\u001b[39m[QuantumScriptBatch, PostprocessingFn]:\n\u001b[32m     49\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Transforms a circuit to one with purely numpy parameters.\u001b[39;00m\n\u001b[32m     50\u001b[39m \n\u001b[32m     51\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     80\u001b[39m \n\u001b[32m     81\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     new_ops = (\u001b[43m_convert_op_to_numpy_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m op \u001b[38;5;129;01min\u001b[39;00m tape.operations)\n\u001b[32m     83\u001b[39m     new_measurements = (_convert_measurement_to_numpy_data(m) \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m tape.measurements)\n\u001b[32m     84\u001b[39m     new_circuit = tape.\u001b[34m__class__\u001b[39m(\n\u001b[32m     85\u001b[39m         new_ops, new_measurements, shots=tape.shots, trainable_params=tape.trainable_params\n\u001b[32m     86\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\transforms\\convert_to_numpy_parameters.py:30\u001b[39m, in \u001b[36m_convert_op_to_numpy_data\u001b[39m\u001b[34m(op)\u001b[39m\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m op\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Use operator method to change parameters when it become available\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m qml.ops.functions.bind_new_parameters(op, \u001b[43mmath\u001b[49m\u001b[43m.\u001b[49m\u001b[43munwrap\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\math\\multi_dispatch.py:814\u001b[39m, in \u001b[36munwrap\u001b[39m\u001b[34m(values, max_depth)\u001b[39m\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_val.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_val, ndarray) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_val.shape \u001b[38;5;28;01melse\u001b[39;00m new_val\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    816\u001b[39m     np.to_numpy(values, max_depth=max_depth)\n\u001b[32m    817\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ArrayBox)\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m np.to_numpy(values)\n\u001b[32m    819\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\math\\multi_dispatch.py:814\u001b[39m, in \u001b[36m<genexpr>\u001b[39m\u001b[34m(.0)\u001b[39m\n\u001b[32m    811\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m new_val.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_val, ndarray) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_val.shape \u001b[38;5;28;01melse\u001b[39;00m new_val\n\u001b[32m    813\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m814\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(values)(\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m val \u001b[38;5;129;01min\u001b[39;00m values)\n\u001b[32m    815\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    816\u001b[39m     np.to_numpy(values, max_depth=max_depth)\n\u001b[32m    817\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(values, ArrayBox)\n\u001b[32m    818\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m np.to_numpy(values)\n\u001b[32m    819\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\math\\multi_dispatch.py:809\u001b[39m, in \u001b[36munwrap.<locals>.convert\u001b[39m\u001b[34m(val)\u001b[39m\n\u001b[32m    806\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, (\u001b[38;5;28mtuple\u001b[39m, \u001b[38;5;28mlist\u001b[39m)):\n\u001b[32m    807\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m unwrap(val)\n\u001b[32m    808\u001b[39m new_val = (\n\u001b[32m--> \u001b[39m\u001b[32m809\u001b[39m     np.to_numpy(val, max_depth=max_depth) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, ArrayBox) \u001b[38;5;28;01melse\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    810\u001b[39m )\n\u001b[32m    811\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m new_val.tolist() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(new_val, ndarray) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m new_val.shape \u001b[38;5;28;01melse\u001b[39;00m new_val\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\autoray\\autoray.py:81\u001b[39m, in \u001b[36mdo\u001b[39m\u001b[34m(fn, like, *args, **kwargs)\u001b[39m\n\u001b[32m     79\u001b[39m backend = _choose_backend(fn, args, kwargs, like=like)\n\u001b[32m     80\u001b[39m func = get_lib_fn(backend, fn)\n\u001b[32m---> \u001b[39m\u001b[32m81\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\KS\\Qugeister_clean\\qugeister_venv\\Lib\\site-packages\\pennylane\\math\\single_dispatch.py:613\u001b[39m, in \u001b[36m_to_numpy_torch\u001b[39m\u001b[34m(x)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_to_numpy_torch\u001b[39m(x):\n\u001b[32m--> \u001b[39m\u001b[32m613\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33mis_conj\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_conj\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[32m    614\u001b[39m         \u001b[38;5;66;03m# The following line is only covered if using Torch <v1.10.0\u001b[39;00m\n\u001b[32m    615\u001b[39m         x = x.resolve_conj()\n\u001b[32m    617\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m x.detach().cpu().numpy()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# === å®Ÿé¨“å®Ÿè¡Œ ===\n",
    "print(\"ğŸš€ CQCNNé‡å­å¼·åŒ–å­¦ç¿’å®Ÿé¨“ã‚’é–‹å§‹ã—ã¾ã™\")\n",
    "print(f\"è¨­å®š: {config_path}\")\n",
    "print(f\"æœ€å¤§ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰: {MAX_EPISODES:,}\")\n",
    "print(f\"ç›®æ¨™: Balance â‰¥ 0.95, 50å›é€£ç¶šé”æˆã§åæŸ\")\n",
    "\n",
    "# å®Ÿé¨“ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ç”Ÿæˆ\n",
    "experiment = CQCNNExperiment(config)\n",
    "\n",
    "# å®Ÿé¨“å®Ÿè¡Œ\n",
    "final_episode, training_time, analysis = experiment.run_experiment()\n",
    "\n",
    "print(\"\\nâœ… å®Ÿé¨“å®Œäº†!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === çµæœåˆ†æã¨å¯è¦–åŒ– ===\n",
    "print(\"ğŸ“Š å®Ÿé¨“çµæœã‚’åˆ†æä¸­...\")\n",
    "\n",
    "# ãƒ‡ãƒ¼ã‚¿æº–å‚™\n",
    "episodes = [r['episode'] for r in experiment.game_results]\n",
    "winners = [r['winner'] for r in experiment.game_results]\n",
    "turns = [r['turns'] for r in experiment.game_results]\n",
    "\n",
    "# å‹ç‡è¨ˆç®—ï¼ˆç§»å‹•å¹³å‡ï¼‰\n",
    "window = 100\n",
    "p1_wins = [1 if w == 1 else 0 for w in winners]\n",
    "p2_wins = [1 if w == 2 else 0 for w in winners]\n",
    "draws = [1 if w is None else 0 for w in winners]\n",
    "\n",
    "p1_rate = np.convolve(p1_wins, np.ones(window)/window, mode='valid')\n",
    "p2_rate = np.convolve(p2_wins, np.ones(window)/window, mode='valid')\n",
    "draw_rate = np.convolve(draws, np.ones(window)/window, mode='valid')\n",
    "episodes_smooth = np.array(episodes[window-1:])\n",
    "\n",
    "# ãƒãƒ©ãƒ³ã‚¹è¨ˆç®—\n",
    "balance_history = []\n",
    "for i in range(window-1, len(winners)):\n",
    "    recent = winners[i-window+1:i+1]\n",
    "    w1 = sum(1 for w in recent if w == 1)\n",
    "    w2 = sum(1 for w in recent if w == 2)\n",
    "    balance = min(w1, w2) / max(w1, w2) if max(w1, w2) > 0 else 1.0\n",
    "    balance_history.append(balance)\n",
    "\n",
    "# åæŸãƒ¡ãƒˆãƒªã‚¯ã‚¹\n",
    "convergence_episodes = [h['episode'] for h in experiment.convergence_detector.convergence_history]\n",
    "convergence_balance = [h['metrics']['balance'] for h in experiment.convergence_detector.convergence_history]\n",
    "consecutive_good = [h['metrics']['consecutive_good'] for h in experiment.convergence_detector.convergence_history]\n",
    "\n",
    "# å¯è¦–åŒ–\n",
    "plt.style.use('default')\n",
    "fig, axes = plt.subplots(3, 3, figsize=(20, 15))\n",
    "fig.suptitle(f'CQCNNé‡å­å¼·åŒ–å­¦ç¿’å®Ÿé¨“çµæœ (Episodes: {final_episode:,})', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. å‹ç‡æ¨ç§»\n",
    "axes[0,0].plot(episodes_smooth, p1_rate, label='Player 1', color='blue', linewidth=2)\n",
    "axes[0,0].plot(episodes_smooth, p2_rate, label='Player 2', color='red', linewidth=2)\n",
    "axes[0,0].plot(episodes_smooth, draw_rate, label='Draws', color='gray', linewidth=2)\n",
    "axes[0,0].axhline(y=0.5, color='black', linestyle='--', alpha=0.5)\n",
    "axes[0,0].set_title('Win Rate Trends')\n",
    "axes[0,0].set_xlabel('Episode')\n",
    "axes[0,0].set_ylabel('Win Rate')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. ãƒãƒ©ãƒ³ã‚¹æ¨ç§»\n",
    "axes[0,1].plot(episodes_smooth, balance_history, color='green', linewidth=2)\n",
    "axes[0,1].axhline(y=0.95, color='red', linestyle='--', label='Target (0.95)')\n",
    "axes[0,1].axhline(y=0.995, color='orange', linestyle='--', label='Ultra-strict (0.995)')\n",
    "axes[0,1].set_title('Balance Evolution')\n",
    "axes[0,1].set_xlabel('Episode')\n",
    "axes[0,1].set_ylabel('Balance')\n",
    "axes[0,1].legend()\n",
    "axes[0,1].grid(True, alpha=0.3)\n",
    "axes[0,1].set_ylim([0, 1.05])\n",
    "\n",
    "# 3. åæŸé€²è¡Œ\n",
    "if convergence_episodes:\n",
    "    axes[0,2].plot(convergence_episodes, consecutive_good, color='purple', linewidth=2, marker='o', markersize=3)\n",
    "    axes[0,2].axhline(y=50, color='red', linestyle='--', label='Target (50)')\n",
    "    axes[0,2].set_title('Convergence Progress')\n",
    "    axes[0,2].set_xlabel('Episode')\n",
    "    axes[0,2].set_ylabel('Consecutive Good Checks')\n",
    "    axes[0,2].legend()\n",
    "    axes[0,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. ã‚²ãƒ¼ãƒ é•·æ¨ç§»\n",
    "turns_smooth = np.convolve(turns, np.ones(window)/window, mode='valid')\n",
    "axes[1,0].plot(episodes_smooth, turns_smooth, color='brown', linewidth=2)\n",
    "axes[1,0].set_title('Average Game Length')\n",
    "axes[1,0].set_xlabel('Episode')\n",
    "axes[1,0].set_ylabel('Turns per Game')\n",
    "axes[1,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 5. Îµæ¸›è¡°\n",
    "if experiment.epsilon_history:\n",
    "    axes[1,1].plot(experiment.epsilon_history, color='orange', linewidth=2)\n",
    "    axes[1,1].set_title('Epsilon Decay')\n",
    "    axes[1,1].set_xlabel('Episode')\n",
    "    axes[1,1].set_ylabel('Epsilon')\n",
    "    axes[1,1].set_yscale('log')\n",
    "    axes[1,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 6. æå¤±æ¨ç§»\n",
    "if experiment.losses_1 and experiment.losses_2:\n",
    "    loss_episodes_1 = np.linspace(0, final_episode, len(experiment.losses_1))\n",
    "    loss_episodes_2 = np.linspace(0, final_episode, len(experiment.losses_2))\n",
    "    axes[1,2].plot(loss_episodes_1, experiment.losses_1, alpha=0.7, color='blue', label='Player 1')\n",
    "    axes[1,2].plot(loss_episodes_2, experiment.losses_2, alpha=0.7, color='red', label='Player 2')\n",
    "    \n",
    "    # ç§»å‹•å¹³å‡\n",
    "    if len(experiment.losses_1) > 50:\n",
    "        loss1_smooth = np.convolve(experiment.losses_1, np.ones(50)/50, mode='valid')\n",
    "        loss2_smooth = np.convolve(experiment.losses_2, np.ones(50)/50, mode='valid')\n",
    "        axes[1,2].plot(loss_episodes_1[49:], loss1_smooth, color='darkblue', linewidth=2)\n",
    "        axes[1,2].plot(loss_episodes_2[49:], loss2_smooth, color='darkred', linewidth=2)\n",
    "    \n",
    "    axes[1,2].set_title('Training Loss')\n",
    "    axes[1,2].set_xlabel('Episode')\n",
    "    axes[1,2].set_ylabel('Loss')\n",
    "    axes[1,2].legend()\n",
    "    axes[1,2].grid(True, alpha=0.3)\n",
    "\n",
    "# 7. æœ€çµ‚çµæœåˆ†å¸ƒ\n",
    "final_1000 = winners[-1000:] if len(winners) >= 1000 else winners\n",
    "w1_final = sum(1 for w in final_1000 if w == 1)\n",
    "w2_final = sum(1 for w in final_1000 if w == 2)\n",
    "d_final = sum(1 for w in final_1000 if w is None)\n",
    "\n",
    "categories = ['Player 1', 'Player 2', 'Draws']\n",
    "values = [w1_final, w2_final, d_final]\n",
    "colors = ['blue', 'red', 'gray']\n",
    "axes[2,0].pie(values, labels=categories, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "axes[2,0].set_title(f'Final Results (Last {len(final_1000)} games)')\n",
    "\n",
    "# 8. é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿åˆ†å¸ƒ\n",
    "params1 = experiment.cqcnn_1.quantum_params.detach().numpy().flatten()\n",
    "params2 = experiment.cqcnn_2.quantum_params.detach().numpy().flatten()\n",
    "axes[2,1].hist(params1, bins=30, alpha=0.7, label='Player 1', color='blue', density=True)\n",
    "axes[2,1].hist(params2, bins=30, alpha=0.7, label='Player 2', color='red', density=True)\n",
    "axes[2,1].set_title('Quantum Parameter Distribution')\n",
    "axes[2,1].set_xlabel('Parameter Value')\n",
    "axes[2,1].set_ylabel('Density')\n",
    "axes[2,1].legend()\n",
    "axes[2,1].grid(True, alpha=0.3)\n",
    "\n",
    "# 9. ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ™‚é–“\n",
    "if experiment.episode_times:\n",
    "    time_smooth = np.convolve(experiment.episode_times, np.ones(min(100, len(experiment.episode_times)))//min(100, len(experiment.episode_times)), mode='valid')\n",
    "    time_episodes = range(len(time_smooth))\n",
    "    axes[2,2].plot(time_episodes, time_smooth, color='green', linewidth=2)\n",
    "    axes[2,2].set_title('Episode Time')\n",
    "    axes[2,2].set_xlabel('Episode')\n",
    "    axes[2,2].set_ylabel('Time (seconds)')\n",
    "    axes[2,2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"å¯è¦–åŒ–å®Œäº†!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === è©³ç´°åˆ†æãƒ¬ãƒãƒ¼ãƒˆ ===\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"          CQCNNé‡å­å¼·åŒ–å­¦ç¿’å®Ÿé¨“ - æœ€çµ‚ãƒ¬ãƒãƒ¼ãƒˆ\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# åŸºæœ¬çµ±è¨ˆ\n",
    "total_games = len(experiment.game_results)\n",
    "total_p1_wins = sum(1 for r in experiment.game_results if r['winner'] == 1)\n",
    "total_p2_wins = sum(1 for r in experiment.game_results if r['winner'] == 2)\n",
    "total_draws = sum(1 for r in experiment.game_results if r['winner'] is None)\n",
    "\n",
    "print(f\"\\nğŸ“ˆ åŸºæœ¬çµ±è¨ˆ\")\n",
    "print(f\"  ç·ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {total_games:,}\")\n",
    "print(f\"  Player 1 å‹åˆ©: {total_p1_wins:,} ({total_p1_wins/total_games*100:.1f}%)\")\n",
    "print(f\"  Player 2 å‹åˆ©: {total_p2_wins:,} ({total_p2_wins/total_games*100:.1f}%)\")\n",
    "print(f\"  å¼•ãåˆ†ã‘: {total_draws:,} ({total_draws/total_games*100:.1f}%)\")\n",
    "print(f\"  å¹³å‡ã‚²ãƒ¼ãƒ é•·: {np.mean(turns):.1f} ã‚¿ãƒ¼ãƒ³\")\n",
    "print(f\"  å®Ÿé¨“æ™‚é–“: {training_time:.1f}ç§’ ({training_time/3600:.2f}æ™‚é–“)\")\n",
    "\n",
    "# æœ€çµ‚æœŸé–“ã®è©³ç´°åˆ†æ\n",
    "final_period = min(1000, total_games)\n",
    "final_results = experiment.game_results[-final_period:]\n",
    "final_p1 = sum(1 for r in final_results if r['winner'] == 1)\n",
    "final_p2 = sum(1 for r in final_results if r['winner'] == 2)\n",
    "final_draws = sum(1 for r in final_results if r['winner'] is None)\n",
    "final_balance = min(final_p1, final_p2) / max(final_p1, final_p2) if max(final_p1, final_p2) > 0 else 1.0\n",
    "\n",
    "print(f\"\\nğŸ¯ æœ€çµ‚æœŸé–“åˆ†æ (ç›´è¿‘{final_period}ã‚²ãƒ¼ãƒ )\")\n",
    "print(f\"  Player 1: {final_p1} ({final_p1/final_period*100:.1f}%)\")\n",
    "print(f\"  Player 2: {final_p2} ({final_p2/final_period*100:.1f}%)\")\n",
    "print(f\"  å¼•ãåˆ†ã‘: {final_draws} ({final_draws/final_period*100:.1f}%)\")\n",
    "print(f\"  ãƒãƒ©ãƒ³ã‚¹: {final_balance:.4f}\")\n",
    "print(f\"  ç›®æ¨™é”æˆ: {'âœ… YES' if final_balance >= 0.95 else 'âŒ NO'} (ç›®æ¨™: â‰¥0.95)\")\n",
    "\n",
    "# åæŸåˆ†æ\n",
    "print(f\"\\nğŸ”„ åæŸåˆ†æ\")\n",
    "if analysis['converged']:\n",
    "    print(f\"  âœ… åæŸé”æˆ!\")\n",
    "    print(f\"  æœ€çµ‚ãƒãƒ©ãƒ³ã‚¹: {analysis['balance']:.4f}\")\n",
    "    print(f\"  é€£ç¶šé”æˆå›æ•°: {analysis['metrics']['consecutive_good']}\")\n",
    "else:\n",
    "    print(f\"  âŒ åæŸæœªé”æˆ\")\n",
    "    print(f\"  ç¾åœ¨ãƒãƒ©ãƒ³ã‚¹: {analysis['balance']:.4f}\")\n",
    "    print(f\"  é€£ç¶šé”æˆå›æ•°: {analysis['metrics']['consecutive_good']}/50\")\n",
    "    print(f\"  ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹: {analysis['metrics']['status']}\")\n",
    "\n",
    "# Ultra-strictå®Ÿé¨“ã¨ã®æ¯”è¼ƒ\n",
    "ultra_strict_balance = 1.000  # Ultra-strictå®Ÿé¨“ã®çµæœ\n",
    "ultra_strict_episodes = 46400\n",
    "ultra_strict_draws = 1.0\n",
    "\n",
    "print(f\"\\nâš–ï¸  Ultra-strictå®Ÿé¨“ã¨ã®æ¯”è¼ƒ\")\n",
    "print(f\"  è¨­å®š      â”‚ Ultra-strict â”‚ ç¾åœ¨ã®å®Ÿé¨“\")\n",
    "print(f\"  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\")\n",
    "print(f\"  é–¾å€¤      â”‚     0.995    â”‚    0.95\")\n",
    "print(f\"  ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰â”‚   {ultra_strict_episodes:,}    â”‚   {total_games:,}\")\n",
    "print(f\"  ãƒãƒ©ãƒ³ã‚¹  â”‚   {ultra_strict_balance:.3f}    â”‚   {final_balance:.3f}\")\n",
    "print(f\"  å¼•ãåˆ†ã‘ç‡â”‚   {ultra_strict_draws*100:.1f}%     â”‚   {final_draws/final_period*100:.1f}%\")\n",
    "print(f\"  åæŸ      â”‚     æœªé”æˆ    â”‚   {'é”æˆ' if analysis['converged'] else 'æœªé”æˆ'}\")\n",
    "\n",
    "# é‡å­åŠ¹æœã®åˆ†æ\n",
    "quantum_std_1 = np.std(params1)\n",
    "quantum_std_2 = np.std(params2)\n",
    "quantum_diff = np.mean(np.abs(params1 - params2))\n",
    "\n",
    "print(f\"\\nğŸŒŒ é‡å­åŠ¹æœåˆ†æ\")\n",
    "print(f\"  Player 1 é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²: [{params1.min():.3f}, {params1.max():.3f}]\")\n",
    "print(f\"  Player 2 é‡å­ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ç¯„å›²: [{params2.min():.3f}, {params2.max():.3f}]\")\n",
    "print(f\"  Player 1 æ¨™æº–åå·®: {quantum_std_1:.3f}\")\n",
    "print(f\"  Player 2 æ¨™æº–åå·®: {quantum_std_2:.3f}\")\n",
    "print(f\"  ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼é–“å·®ç•°: {quantum_diff:.3f}\")\n",
    "\n",
    "# å­¦ç¿’åŠ¹ç‡\n",
    "if experiment.losses_1 and experiment.losses_2:\n",
    "    initial_loss_1 = np.mean(experiment.losses_1[:50]) if len(experiment.losses_1) >= 50 else 0\n",
    "    final_loss_1 = np.mean(experiment.losses_1[-50:]) if len(experiment.losses_1) >= 50 else 0\n",
    "    initial_loss_2 = np.mean(experiment.losses_2[:50]) if len(experiment.losses_2) >= 50 else 0\n",
    "    final_loss_2 = np.mean(experiment.losses_2[-50:]) if len(experiment.losses_2) >= 50 else 0\n",
    "    \n",
    "    print(f\"\\nğŸ“š å­¦ç¿’åŠ¹ç‡\")\n",
    "    print(f\"  Player 1 æå¤±: {initial_loss_1:.4f} â†’ {final_loss_1:.4f} ({((final_loss_1-initial_loss_1)/initial_loss_1*100):+.1f}%)\")\n",
    "    print(f\"  Player 2 æå¤±: {initial_loss_2:.4f} â†’ {final_loss_2:.4f} ({((final_loss_2-initial_loss_2)/initial_loss_2*100):+.1f}%)\")\n",
    "    print(f\"  ç·å­¦ç¿’ã‚¹ãƒ†ãƒƒãƒ—: {len(experiment.losses_1) + len(experiment.losses_2):,}\")\n",
    "\n",
    "# å®Ÿé¨“è¨­å®šã‚µãƒãƒªãƒ¼\n",
    "print(f\"\\nâš™ï¸  å®Ÿé¨“è¨­å®š\")\n",
    "print(f\"  é‡å­æ§‹æˆ: {N_QUBITS}Q{N_LAYERS}L\")\n",
    "print(f\"  çŠ¶æ…‹æ¬¡å…ƒ: {STATE_DIM}\")\n",
    "print(f\"  è¡Œå‹•ç©ºé–“: {ACTION_DIM}\")\n",
    "print(f\"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {BATCH_SIZE}\")\n",
    "print(f\"  å­¦ç¿’ç‡: {LEARNING_RATE}\")\n",
    "print(f\"  Îµæ¸›è¡°: {EPSILON_START} â†’ {EPSILON_MIN} (ä¿‚æ•°: {EPSILON_DECAY})\")\n",
    "print(f\"  ãƒãƒƒãƒ•ã‚¡ã‚µã‚¤ã‚º: {BUFFER_SIZE:,}\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(f\"å®Ÿé¨“å®Œäº†! çµæœã¯ä¸Šè¨˜ã®é€šã‚Šã§ã™ã€‚\")\n",
    "print(f\"ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯ä¿å­˜æ¨å¥¨: ã“ã®ã‚»ãƒ«ã®çµæœã‚’è¨˜éŒ²ã—ã¦ãã ã•ã„ã€‚\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qugeister_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
